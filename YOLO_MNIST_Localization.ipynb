{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThOpaque/Food_Recognition/blob/main/YOLO_MNIST_Localization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAHeh1BRufX4",
        "outputId": "a193c888-45ab-41b6-a85f-d234ad958ced"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchinfo in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (1.7.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: torchmetrics in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (0.9.3)\n",
            "Requirement already satisfied: packaging in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from torchmetrics) (1.23.1)\n",
            "Requirement already satisfied: torch>=1.3.1 in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from torchmetrics) (1.13.0.dev20220730)\n",
            "Requirement already satisfied: typing-extensions in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from torch>=1.3.1->torchmetrics) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "import os, time, datetime\n",
        "from timeit import default_timer as timer\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "%pip install torchinfo;\n",
        "%pip install torchmetrics;\n",
        "from torchmetrics import MeanSquaredError;\n",
        "from torchinfo import summary;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOraK1TX7XZB",
        "outputId": "917d3947-108d-42f7-b499-4b5924b0279a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------------\n",
            "Execute notebook on - mps -\n",
            "------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Choosing device between CPU or GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "elif torch.has_mps:\n",
        "    device=torch.device('mps')\n",
        "else:\n",
        "    device=torch.device('cpu')\n",
        "    \n",
        "print(\"\\n------------------------------------\")\n",
        "print(f\"Execute notebook on - {device} -\")\n",
        "print(\"------------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "M7VztqFE71JZ"
      },
      "outputs": [],
      "source": [
        "class my_mnist_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root:str, split:str=None, download:bool=False, S=6, sizeHW=75):\n",
        "        assert split, \"You have to specify the split.\"\n",
        "        \n",
        "        if split == \"train\":\n",
        "            train = True\n",
        "        elif split == \"test\":\n",
        "            train = False\n",
        "        \n",
        "        self.dataset = torchvision.datasets.MNIST(root=root, train=train, download=download)\n",
        "        \n",
        "        self.cell_size = sizeHW / S\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def _numpy_pad_to_bounding_box(self, image, offset_height=0, offset_width=0, target_height=0, target_width=0):\n",
        "        assert image.shape[:-1][0] <= target_height-offset_height, \"height must be <= target - offset\"\n",
        "        assert image.shape[:-1][1] <= target_width-offset_width, \"width must be <= target - offset\"\n",
        "        \n",
        "        target_array = np.zeros((target_height, target_width, image.shape[-1]))\n",
        "\n",
        "        for k in range(image.shape[0]):\n",
        "            target_array[offset_height+k][offset_width:image.shape[1]+offset_width] = image[k]\n",
        "        \n",
        "        return target_array\n",
        "\n",
        "    def _transform_pasting75(self, image, label):\n",
        "        ### xmin, ymin of digit\n",
        "        xmin = torch.randint(0, 48, (1,))\n",
        "        ymin = torch.randint(0, 48, (1,))\n",
        "        \n",
        "        image = torchvision.transforms.ToTensor()(image)\n",
        "        image = torch.reshape(image, (28,28,1,))\n",
        "        image = torch.from_numpy(self._numpy_pad_to_bounding_box(image, ymin, xmin, 75, 75))\n",
        "        image = image.permute(2, 0, 1) #(C,H,W)\n",
        "        image = image.to(torch.float)\n",
        "        \n",
        "        xmin, ymin = xmin.to(torch.float), ymin.to(torch.float)\n",
        "\n",
        "        xmax_bbox, ymax_bbox = (xmin + 28), (ymin + 28)\n",
        "        xmin_bbox, ymin_bbox = xmin, ymin\n",
        "        w_bbox = xmax_bbox-xmin_bbox\n",
        "        h_bbox = ymax_bbox-ymin_bbox\n",
        "\n",
        "        rw = w_bbox / 75\n",
        "        rh = h_bbox / 75\n",
        "        cx = (xmin + (w_bbox/2))/75\n",
        "        cy = (ymin + (h_bbox/2))/75\n",
        "\n",
        "        cx_rcell = cx % self.cell_size / self.cell_size\n",
        "        cy_rcell = cy % self.cell_size / self.cell_size\n",
        "\n",
        "\n",
        "        label_one_hot = F.one_hot(torch.as_tensor(label, dtype=torch.int64), 10)\n",
        "        bbox_coord = torch.Tensor([cx_rcell, cy_rcell, rw, rh])\n",
        "\n",
        "        return image, label_one_hot, bbox_coord\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        image, one_hot_label, bbox_coord = self._transform_pasting75(self.dataset[idx][0], self.dataset[idx][1])\n",
        "        \n",
        "        return image, one_hot_label.to(torch.float), bbox_coord\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423,
          "referenced_widgets": [
            "9811f18275dd46f397a0dc445bcbd90a",
            "c63e5aa3160c49d28f9a6df08abb2558",
            "201eec9906304c4e95854aa2fca10439",
            "da28120108424e88bdb8599bd0f8d4f7",
            "83fbb691d40b4d02b029bcbdb2f85590",
            "30d2ecca2c46436198f644e8d6dc7f24",
            "6c99c88b52334f53a1a3c8e89f5c3d7f",
            "d5d88a84b1a24e98b3ee1d1b469defff",
            "cc8b573ffafd427495a1ccf3b818e815",
            "46612f5ff5024571917d3275717a5ae6",
            "8c60136a50bd4bf6ad83c4d26f8d8aa7",
            "06ff67cf46e64c3e8ba324f03a84c5ef",
            "b7a121501cfd438895e7bd2d85e56251",
            "c1c21fc51a224d60b314af87cbde69d4",
            "97522edebd964d2aa9671dac5a5f381d",
            "216234dec1484887a68608c520b3f093",
            "74c88f9633d541b0a60149df3d9a606c",
            "a4fef10f26744768b57986d380689412",
            "04f6d46789754cfd9f0a441baa32af45",
            "5f502d93a2fc46398195151da51c0083",
            "7dc6465a02ed4763b527f143ad9e254c",
            "7d20b57ac5b44fbd91e7cee432070247",
            "f85da4cd1a64460bb2580b69d8fa066d",
            "09ed36dea11c480d8e0f38478998b828",
            "e18767ed9f8944b5a8091d5f39e986d9",
            "58c90db1a5df4c0bb4d6c6cc8cd98dce",
            "85daafeb4a1a4b55bc6b63ba3f088bd5",
            "b60a12468347494d9c4fe04cd86af58e",
            "f68b6dd7bde64b71afb10a929c410ed9",
            "c136bc57bec64d38b2505a0c60e6f64b",
            "a66241a6cd464e339bef5fb2fceab412",
            "dd7117704fa347838c6b19067ebca54c",
            "15b3c78cbd51469ca2b870b993b6fca0",
            "809cb638d85e48999892dfd8cfc815a7",
            "9a64e70ecfbe46558b9865815d283596",
            "ae6fd6d93a5647a28ffd9af5e26942e6",
            "aad87ec81bb34aadac8c84ae9b505c68",
            "cdfd8330d4964e9fa41bfe4fd1639def",
            "c9418919bc204bb6af14a33815d1bd5f",
            "24e840cc10c54af0a0d40c6eb27beda1",
            "4ee695bbe8e543a2b6cc48b97ed63b3e",
            "54e7c876fe4d4be482733d96848b5a51",
            "b34a8acab2614688bef300d8be847cf6",
            "0963b6c3e45e4e5dabfd2e0d9b6cca38"
          ]
        },
        "id": "yQNznLfO8jOx",
        "outputId": "6979c79f-9d22-4eb4-aad5-bd90483983b6"
      },
      "outputs": [],
      "source": [
        "def get_training_dataset(BATCH_SIZE=64):\n",
        "    \"\"\"\n",
        "    Loads and maps the training split of the dataset using the custom dataset class. \n",
        "    \"\"\"\n",
        "    dataset = my_mnist_dataset(root=\"data\", split=\"train\", download=True)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    return dataloader, len(dataset)\n",
        "\n",
        "def get_validation_dataset(BATCH_SIZE = None):\n",
        "    \"\"\"\n",
        "    Loads and maps the validation split of the datasetusing the custom dataset class. \n",
        "    \"\"\"\n",
        "    dataset = my_mnist_dataset(root=\"data\", split=\"test\", download=True)\n",
        "    if BATCH_SIZE is None:\n",
        "        BATCH_SIZE = len(dataset)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    return dataloader, len(dataset)\n",
        "\n",
        "# instantiate the datasets\n",
        "training_dataset, len_training_ds = get_training_dataset()\n",
        "validation_dataset, len_validation_ds = get_validation_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wfgUx1srt90c"
      },
      "outputs": [],
      "source": [
        "class CNNBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(CNNBlock, self).__init__()\n",
        "        self.conv = torch.nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "        self.bn = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.l_relu = torch.nn.LeakyReLU(0.1)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        x = self.conv(input)\n",
        "        x = self.bn(x)\n",
        "        return self.l_relu(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3VjCkTEce-CT"
      },
      "outputs": [],
      "source": [
        "class YoloMNIST(torch.nn.Module):\n",
        "    def __init__(self, sizeHW, S, C, B):\n",
        "        super(YoloMNIST, self).__init__()\n",
        "        self.S, self.C, self.B = S, C, B\n",
        "        self.sizeHW = sizeHW\n",
        "        self.cell_size = self.sizeHW / self.S\n",
        "\n",
        "        self.seq = torch.nn.Sequential()        \n",
        "        self.seq.add_module(f\"conv_1\", CNNBlock(1, 32, stride=2, kernel_size=7, padding=2))\n",
        "        self.seq.add_module(f\"maxpool_1\", torch.nn.MaxPool2d(2))\n",
        "        self.seq.add_module(f\"conv_3\", CNNBlock(32, 128, stride=1, kernel_size=3, padding=0))\n",
        "        self.seq.add_module(f\"maxpool_2\", torch.nn.MaxPool2d(2))\n",
        "        self.seq.add_module(f\"conv_5\", CNNBlock(128, 64, stride=1, kernel_size=1, padding=0))\n",
        "        self.seq.add_module(f\"conv_4\", CNNBlock(64, 128, stride=1, kernel_size=3, padding=0))\n",
        "        self.seq.add_module(f\"conv_6\", CNNBlock(128, 128, stride=1, kernel_size=3, padding=1))\n",
        "        \n",
        "        self.fcs = self._create_fcs()\n",
        "\n",
        "    def _size_output(self, sizeHW:int, kernel:int, stride:int, padding:int=0, isMaxPool:bool=False)->int:\n",
        "        \"\"\"\n",
        "        Output size (width/height) of convolutional or maxpool layers.\n",
        "\n",
        "        Args:\n",
        "            sizeHW : int\n",
        "                Image size (we suppose this is a square image)\n",
        "            kernel : int\n",
        "                Size of a square kernel\n",
        "            stride : int\n",
        "                Stride of convolution layer\n",
        "            padding : int\n",
        "                Padding of convolution layer\n",
        "            isMaxPool : Bool, default is False.\n",
        "                Specify if it is a Maxpool layer (True) or not (False). \n",
        "\n",
        "        Return:\n",
        "            output_size : int\n",
        "                Image output size after a convolutional or MaxPool layer.\n",
        "        \"\"\" \n",
        "        if isMaxPool == True:\n",
        "            output_size = int(sizeHW/2)\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "        if padding == 'same':\n",
        "            output_size = sizeHW\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "        else:\n",
        "            output_size = (sizeHW + 2 * padding - (kernel-1)-1)/stride\n",
        "            output_size = int(output_size + 1)\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "\n",
        "    def _create_fcs(self):\n",
        "        output = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(128 * self.S * self.S, 4096),\n",
        "            torch.nn.LeakyReLU(0.1),\n",
        "            torch.nn.Linear(4096, self.S * self.S * (self.C + self.B*5))\n",
        "        )\n",
        "        return output\n",
        "    \n",
        "\n",
        "    def forward(self, input:torch.Tensor)->tuple:\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            input : torch.Tensor of shape (N, C, H, W)\n",
        "                Batch of images.\n",
        "\n",
        "        Return:\n",
        "            box_coord : torch.Tensor of shape (N, 6, 6, 5)\n",
        "                Contains xc_rcell, yc_rcell, rw, rh and the confidence number c\n",
        "                over 6x6 grid cells.\n",
        "            classifier : torch.Tensor of shape (N, 6, 6, 10)\n",
        "                Contains the one-hot encoding of each digit number over\n",
        "                6x6 grid cells.\n",
        "        \"\"\"     \n",
        "        x = self.seq(input)\n",
        "        x = self.fcs(x)\n",
        "        x = x.view(x.size(0), self.S, self.S, self.B * 5 + self.C)\n",
        "        box_coord = x[:,:,:,0:5]\n",
        "        classifier = x[:,:,:,5:]\n",
        "        return box_coord, classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "T_jOg_i_p2_c"
      },
      "outputs": [],
      "source": [
        "class YoloLoss(torch.nn.Module):\n",
        "    def __init__(self, lambd_coord:int, lambd_noobj:float, device:torch.device, S:int=6):\n",
        "        super(YoloLoss, self).__init__()\n",
        "        self.LAMBD_COORD = lambd_coord\n",
        "        self.LAMBD_NOOBJ = lambd_noobj\n",
        "        self.S = S\n",
        "        self.device = device\n",
        "\n",
        "    def _coordloss(self, pred_coord_rcell, true_coord_rcell):\n",
        "        \"\"\"\n",
        "        Args : \n",
        "            pred_coord_rcell : torch.Tensor of shape (N, 2)\n",
        "            true_coord_rcell : torch.Tensor of shape (N, 2)\n",
        "        Returns :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        xc_hat, yc_hat = pred_coord_rcell.permute(1,0)\n",
        "        xc, yc = true_coord_rcell.permute(1,0)\n",
        "\n",
        "        squared_error = torch.pow(xc - xc_hat,2) + torch.pow(yc - yc_hat,2)\n",
        "        return squared_error\n",
        "\n",
        "    def _sizeloss(self, pred_size, true_size):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_size : torch.Tensor of shape (N, 2)\n",
        "            true_size : torch.Tensor of shape (N, 2)\n",
        "        Returns : \n",
        "            root_squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        rw_hat, rh_hat = pred_size.permute(1,0)\n",
        "        rw, rh = true_size.permute(1,0)\n",
        "\n",
        "        #sizes can't be negative\n",
        "        rw_hat = rw_hat.clip(min=0)\n",
        "        rh_hat = rh_hat.clip(min=0)\n",
        "\n",
        "        root_squared_error_w = torch.pow(torch.sqrt(rw) - torch.sqrt(rw_hat),2)\n",
        "        root_squared_error_h = torch.pow(torch.sqrt(rh) - torch.sqrt(rh_hat),2)\n",
        "        root_squared_error = root_squared_error_w + root_squared_error_h\n",
        "        return root_squared_error\n",
        "\n",
        "    def _confidenceloss(self, pred_c, true_c):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_c : torch.Tensor of shape (N)\n",
        "            true_c : torch.Tensor of shape (N)\n",
        "        Return :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        squared_error = torch.pow(true_c - pred_c, 2)\n",
        "        return squared_error\n",
        "\n",
        "    def _classloss(self, pred_class, true_class):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_class : torch.Tensor of shape (N, 10)\n",
        "            true_class : torch.Tensor of shape (N, 10)\n",
        "        Returns :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        squared_error = torch.pow(true_class - pred_class, 2)\n",
        "        return torch.sum(squared_error, dim=1)\n",
        "\n",
        "    def forward(self, pred_box:torch.Tensor, true_box:torch.Tensor, pred_class:torch.Tensor, true_class:torch.Tensor):\n",
        "        \"\"\"\n",
        "        Grid forward pass.\n",
        "\n",
        "        Args:\n",
        "            pred_box : torch.Tensor of shape (N, S, S, 5)\n",
        "                Batch predicted outputs containing xc_rcell, yc_rcell, rw, rh,\n",
        "                and confident number c for each grid cell.\n",
        "            true_box : torch.Tensor of shape (N, S, S, 5)\n",
        "                Groundtrue batch containing bbox values for each cell and\n",
        "                c indicate if there is an object to detect or not (1/0).\n",
        "            pred_class : torch.Tensor of shape (N, S, S, 10)\n",
        "                Probability of each digit class in each grid cell\n",
        "            true_class : torch.Tensor of shape (N, 10)\n",
        "                one-hot vect of each digit\n",
        "\n",
        "        Return:\n",
        "            loss : float\n",
        "                The batch loss value of the grid\n",
        "        \"\"\"\n",
        "        BATCH_SIZE = len(pred_box)\n",
        "\n",
        "        ### Initialization of the losses\n",
        "        losses_list = ['loss_xy', 'loss_wh', 'loss_conf_obj', 'loss_conf_noobj', 'loss_class','isObject']\n",
        "        losses = {key : torch.zeros(BATCH_SIZE).to(self.device) for key in losses_list}\n",
        "        check_loss = []\n",
        "        ### Compute the losses for all images in the batch\n",
        "        for i in range(self.S):\n",
        "            for j in range(self.S):\n",
        "                ### Intersection over Union\n",
        "                #IoU = self._intersection_over_union(pred_box[:,i,j], true_box[:,i,j])\n",
        "\n",
        "                ### bbox coordinates\n",
        "                xy_hat = pred_box[:,i,j,:2]\n",
        "                xy = true_box[:,i,j,:2]\n",
        "                wh_hat = pred_box[:,i,j,2:4]\n",
        "                wh = true_box[:,i,j,2:4]\n",
        "                \n",
        "                ### confidence numbers\n",
        "                pred_c = pred_box[:,i,j,4]# * IoU\n",
        "                true_c = true_box[:,i,j,4]\n",
        "\n",
        "                ### objects to detect\n",
        "                isObject = true_c.to(torch.bool)\n",
        "                isNoObject = torch.logical_not(true_c) #(~bool) doesn't work on MPS device\n",
        "\n",
        "                ### sum the losses over the grid\n",
        "                losses['isObject'] += isObject\n",
        "                losses['loss_xy'] += isObject * self._coordloss(xy_hat, xy)\n",
        "                check_loss.append(losses['loss_xy'])\n",
        "                losses['loss_wh'] += isObject * self._sizeloss(wh_hat, wh)\n",
        "                losses['loss_conf_obj'] += isObject * self._confidenceloss(pred_c, true_c)\n",
        "                losses['loss_conf_noobj'] += isNoObject * self._confidenceloss(pred_c, true_c)\n",
        "                losses['loss_class'] += isObject * self._classloss(pred_class[:,i,j], true_class)\n",
        "        \n",
        "\n",
        "\n",
        "        ### Yolo_v1 loss over the batch, shape : (BATCH_SIZE)\n",
        "        loss = self.LAMBD_COORD * losses['loss_xy'] \\\n",
        "                + self.LAMBD_COORD * losses['loss_wh'] \\\n",
        "                + losses['loss_conf_obj'] \\\n",
        "                + self.LAMBD_NOOBJ * losses['loss_conf_noobj'] \\\n",
        "                + losses['loss_class']\n",
        "\n",
        "\n",
        "        assert torch.isnan(torch.sum(losses['loss_conf_obj']))==False, \"La loss {} est devenu nan\".format('loss_conf_obj')\n",
        "        assert torch.isnan(torch.sum(losses['loss_conf_noobj']))==False, \"La loss {} est devenu nan\".format('loss_conf_noobj')\n",
        "        assert torch.isnan(torch.sum(losses['loss_class']))==False, \"La loss {} est devenu nan\".format('loss_class')\n",
        "        assert torch.isnan(torch.sum(losses['isObject']))==False, \"La loss {} est devenu nan\".format('isObject')\n",
        "        assert torch.isnan(torch.sum(losses['loss_wh']))==False, \"La loss {} est devenu nan\".format('loss_wh')\n",
        "        assert torch.isnan(torch.sum(losses['loss_xy']))==False, \"La loss {} est devenu nan\".format('loss_xy')\n",
        "\n",
        "\n",
        "        loss = torch.sum(loss) / BATCH_SIZE\n",
        "\n",
        "        return check_loss, losses, loss\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OdIlkN61mXJj"
      },
      "outputs": [],
      "source": [
        "def bbox2Tensor(bbox:torch.Tensor, S:int=6, sizeHW:int=75, device=torch.device('cpu'))->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Constructs en Tensor and puts bbox values in the corresponding i,j grid cell.\n",
        "\n",
        "    Args :\n",
        "        bbox : torch.Tensor of shape (N,4)\n",
        "            Contains bbox values xc_rcell, yc_rcell, rw and rh.\n",
        "        S : int, default is 6\n",
        "            Size of the grid.\n",
        "        sizeHW : int, default is 75\n",
        "            Size of the image.\n",
        "\n",
        "    Return :\n",
        "        bbox_t : torch.Tensor of shape (N, S, S, 5)\n",
        "            Tensor containing all 4 bbox values in the corresponding i,j grid\n",
        "            cell position i.e. in the i,j position where an object should be\n",
        "            detected.\n",
        "    \"\"\"\n",
        "    N = len(bbox)\n",
        "    bbox_t = torch.zeros(N,S,S,5).to(device)\n",
        "    cell_size = sizeHW/S\n",
        "\n",
        "    xc_rcell, yc_rcell, rw, rh = bbox.permute(1,0).to(device)\n",
        "    xc = xc_rcell * cell_size - (1/cell_size) * (xc_rcell/cell_size).to(torch.int32)\n",
        "    yc = yc_rcell * cell_size - (1/cell_size) * (yc_rcell/cell_size).to(torch.int32)\n",
        "\n",
        "    N_range = torch.arange(N)\n",
        "    lines = (yc * S).to(torch.long)\n",
        "    columns = (xc * S).to(torch.long)\n",
        "    bbox_t[N_range, lines, columns] = torch.stack((xc_rcell, yc_rcell, rw, rh, torch.ones(N))).permute(1,0)\n",
        "    \n",
        "    return bbox_t.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "V9QFEV2oweFN"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cpu')\n",
        "loss_yolo = YoloLoss(lambd_coord=5, lambd_noobj=0.5, S=6, device=device)\n",
        "learning_rate = 0.00001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mHUOkfWPe-CW"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "model_MNIST = YoloMNIST(sizeHW=75, S=6, C=10, B=1)\n",
        "model_MNIST = model_MNIST.to(device)\n",
        "optimizer = torch.optim.Adam(params=model_MNIST.parameters(), lr=learning_rate, weight_decay=0.0005)\n",
        "loss_yolo = YoloLoss(lambd_coord=5, lambd_noobj=0.5, S=6, device=device)\n",
        "\n",
        "# print(optimizer)\n",
        "#summary(model_MNIST, input_size = (BATCH_SIZE,1,75,75))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9tSO0Qo7e-CX",
        "outputId": "30e9cb02-0115-4295-8901-373725937086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[START] : 2022-08-26 17:58:48 :\n",
            "[Training on] : CPU\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 1/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.09439\n",
            "xy_coord training loss for this batch : 0.00019\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.03031\n",
            "confidence without object training loss for this batch : 0.04939\n",
            "class proba training loss for this batch : 0.03779\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.08519\n",
            "xy_coord training loss for this batch : 0.00099\n",
            "wh_sizes training loss for this batch : 0.00031\n",
            "confidence with object training loss for this batch : 0.02470\n",
            "confidence without object training loss for this batch : 0.05698\n",
            "class proba training loss for this batch : 0.02549\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.10395\n",
            "xy_coord training loss for this batch : 0.00023\n",
            "wh_sizes training loss for this batch : 0.00018\n",
            "confidence with object training loss for this batch : 0.03329\n",
            "confidence without object training loss for this batch : 0.05603\n",
            "class proba training loss for this batch : 0.04058\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.10638\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.02419\n",
            "confidence without object training loss for this batch : 0.05187\n",
            "class proba training loss for this batch : 0.05551\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.10645\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.02743\n",
            "confidence without object training loss for this batch : 0.05348\n",
            "class proba training loss for this batch : 0.05138\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.13770\n",
            "xy_coord training loss for this batch : 0.00003\n",
            "wh_sizes training loss for this batch : 0.00020\n",
            "confidence with object training loss for this batch : 0.02019\n",
            "confidence without object training loss for this batch : 0.04441\n",
            "class proba training loss for this batch : 0.09417\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.07867\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.01098\n",
            "confidence without object training loss for this batch : 0.01979\n",
            "class proba training loss for this batch : 0.05674\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.12112\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.02806\n",
            "confidence without object training loss for this batch : 0.04989\n",
            "class proba training loss for this batch : 0.06707\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.08081\n",
            "xy_coord training loss for this batch : 0.00034\n",
            "wh_sizes training loss for this batch : 0.00050\n",
            "confidence with object training loss for this batch : 0.01680\n",
            "confidence without object training loss for this batch : 0.02968\n",
            "class proba training loss for this batch : 0.04498\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.05703\n",
            "xy_coord training loss for this batch : 0.00013\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.01417\n",
            "confidence without object training loss for this batch : 0.03282\n",
            "class proba training loss for this batch : 0.02498\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.08396\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.03066\n",
            "confidence without object training loss for this batch : 0.06265\n",
            "class proba training loss for this batch : 0.02106\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:57.650195\n",
            "Mean training loss for this epoch : 0.09038\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 2/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.09173\n",
            "xy_coord training loss for this batch : 0.00012\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.01858\n",
            "confidence without object training loss for this batch : 0.03295\n",
            "class proba training loss for this batch : 0.05538\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.06509\n",
            "xy_coord training loss for this batch : 0.00012\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.01541\n",
            "confidence without object training loss for this batch : 0.03136\n",
            "class proba training loss for this batch : 0.03270\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.07973\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.01547\n",
            "confidence without object training loss for this batch : 0.03445\n",
            "class proba training loss for this batch : 0.04607\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.07496\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.02685\n",
            "confidence without object training loss for this batch : 0.05310\n",
            "class proba training loss for this batch : 0.02053\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.08368\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00019\n",
            "confidence with object training loss for this batch : 0.01975\n",
            "confidence without object training loss for this batch : 0.03551\n",
            "class proba training loss for this batch : 0.04471\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.06606\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.01489\n",
            "confidence without object training loss for this batch : 0.03546\n",
            "class proba training loss for this batch : 0.03223\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.11111\n",
            "xy_coord training loss for this batch : 0.00018\n",
            "wh_sizes training loss for this batch : 0.00018\n",
            "confidence with object training loss for this batch : 0.02116\n",
            "confidence without object training loss for this batch : 0.03725\n",
            "class proba training loss for this batch : 0.06955\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.14527\n",
            "xy_coord training loss for this batch : 0.00013\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.05380\n",
            "confidence without object training loss for this batch : 0.08763\n",
            "class proba training loss for this batch : 0.04650\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.07322\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.02709\n",
            "confidence without object training loss for this batch : 0.04310\n",
            "class proba training loss for this batch : 0.02347\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.11145\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.04289\n",
            "confidence without object training loss for this batch : 0.06041\n",
            "class proba training loss for this batch : 0.03730\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.11547\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00016\n",
            "confidence with object training loss for this batch : 0.05234\n",
            "confidence without object training loss for this batch : 0.07041\n",
            "class proba training loss for this batch : 0.02666\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:56.459185\n",
            "Mean training loss for this epoch : 0.08923\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 3/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.08336\n",
            "xy_coord training loss for this batch : 0.00036\n",
            "wh_sizes training loss for this batch : 0.00025\n",
            "confidence with object training loss for this batch : 0.02421\n",
            "confidence without object training loss for this batch : 0.04690\n",
            "class proba training loss for this batch : 0.03268\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.06516\n",
            "xy_coord training loss for this batch : 0.00012\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.01404\n",
            "confidence without object training loss for this batch : 0.03006\n",
            "class proba training loss for this batch : 0.03482\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.08599\n",
            "xy_coord training loss for this batch : 0.00003\n",
            "wh_sizes training loss for this batch : 0.00020\n",
            "confidence with object training loss for this batch : 0.02775\n",
            "confidence without object training loss for this batch : 0.04570\n",
            "class proba training loss for this batch : 0.03421\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.06789\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.01728\n",
            "confidence without object training loss for this batch : 0.03030\n",
            "class proba training loss for this batch : 0.03451\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.09584\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00018\n",
            "confidence with object training loss for this batch : 0.02434\n",
            "confidence without object training loss for this batch : 0.03720\n",
            "class proba training loss for this batch : 0.05159\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.06190\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.01276\n",
            "confidence without object training loss for this batch : 0.02326\n",
            "class proba training loss for this batch : 0.03652\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.07418\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02107\n",
            "confidence without object training loss for this batch : 0.03494\n",
            "class proba training loss for this batch : 0.03481\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.06980\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.02522\n",
            "confidence without object training loss for this batch : 0.04467\n",
            "class proba training loss for this batch : 0.02124\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.09082\n",
            "xy_coord training loss for this batch : 0.00019\n",
            "wh_sizes training loss for this batch : 0.00020\n",
            "confidence with object training loss for this batch : 0.02565\n",
            "confidence without object training loss for this batch : 0.06633\n",
            "class proba training loss for this batch : 0.03006\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.06620\n",
            "xy_coord training loss for this batch : 0.00014\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.01700\n",
            "confidence without object training loss for this batch : 0.02717\n",
            "class proba training loss for this batch : 0.03439\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.09271\n",
            "xy_coord training loss for this batch : 0.00017\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.02175\n",
            "confidence without object training loss for this batch : 0.04452\n",
            "class proba training loss for this batch : 0.04726\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:56.631517\n",
            "Mean training loss for this epoch : 0.08772\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 4/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.08024\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00019\n",
            "confidence with object training loss for this batch : 0.02737\n",
            "confidence without object training loss for this batch : 0.04552\n",
            "class proba training loss for this batch : 0.02866\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.09471\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00018\n",
            "confidence with object training loss for this batch : 0.03293\n",
            "confidence without object training loss for this batch : 0.06195\n",
            "class proba training loss for this batch : 0.02950\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.10564\n",
            "xy_coord training loss for this batch : 0.00014\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.03292\n",
            "confidence without object training loss for this batch : 0.05261\n",
            "class proba training loss for this batch : 0.04511\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.07517\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.02474\n",
            "confidence without object training loss for this batch : 0.04318\n",
            "class proba training loss for this batch : 0.02789\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.10294\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.02586\n",
            "confidence without object training loss for this batch : 0.04466\n",
            "class proba training loss for this batch : 0.05376\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.08961\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02195\n",
            "confidence without object training loss for this batch : 0.03785\n",
            "class proba training loss for this batch : 0.04774\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.06810\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.01643\n",
            "confidence without object training loss for this batch : 0.02890\n",
            "class proba training loss for this batch : 0.03642\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.10339\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.03173\n",
            "confidence without object training loss for this batch : 0.04481\n",
            "class proba training loss for this batch : 0.04821\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.08301\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.02701\n",
            "confidence without object training loss for this batch : 0.04607\n",
            "class proba training loss for this batch : 0.03189\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.06550\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.01781\n",
            "confidence without object training loss for this batch : 0.04599\n",
            "class proba training loss for this batch : 0.02370\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.10133\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.02585\n",
            "confidence without object training loss for this batch : 0.04989\n",
            "class proba training loss for this batch : 0.04933\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:55.753316\n",
            "Mean training loss for this epoch : 0.08465\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 5/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.11983\n",
            "xy_coord training loss for this batch : 0.00015\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.03035\n",
            "confidence without object training loss for this batch : 0.04148\n",
            "class proba training loss for this batch : 0.06724\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.11224\n",
            "xy_coord training loss for this batch : 0.00027\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.03120\n",
            "confidence without object training loss for this batch : 0.05016\n",
            "class proba training loss for this batch : 0.05388\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.07145\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.02110\n",
            "confidence without object training loss for this batch : 0.04308\n",
            "class proba training loss for this batch : 0.02811\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.08510\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.01724\n",
            "confidence without object training loss for this batch : 0.03003\n",
            "class proba training loss for this batch : 0.05171\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.08496\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.02557\n",
            "confidence without object training loss for this batch : 0.04429\n",
            "class proba training loss for this batch : 0.03622\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.12778\n",
            "xy_coord training loss for this batch : 0.00004\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.02765\n",
            "confidence without object training loss for this batch : 0.05068\n",
            "class proba training loss for this batch : 0.07411\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.09183\n",
            "xy_coord training loss for this batch : 0.00016\n",
            "wh_sizes training loss for this batch : 0.00046\n",
            "confidence with object training loss for this batch : 0.02159\n",
            "confidence without object training loss for this batch : 0.03889\n",
            "class proba training loss for this batch : 0.04772\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.09356\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.02913\n",
            "confidence without object training loss for this batch : 0.05331\n",
            "class proba training loss for this batch : 0.03642\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.07143\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.02738\n",
            "confidence without object training loss for this batch : 0.04739\n",
            "class proba training loss for this batch : 0.01931\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.09531\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02921\n",
            "confidence without object training loss for this batch : 0.04613\n",
            "class proba training loss for this batch : 0.04205\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.09217\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.03564\n",
            "confidence without object training loss for this batch : 0.05594\n",
            "class proba training loss for this batch : 0.02745\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:55.334478\n",
            "Mean training loss for this epoch : 0.08431\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 6/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.07006\n",
            "xy_coord training loss for this batch : 0.00015\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.01923\n",
            "confidence without object training loss for this batch : 0.03845\n",
            "class proba training loss for this batch : 0.03015\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.09544\n",
            "xy_coord training loss for this batch : 0.00017\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.02448\n",
            "confidence without object training loss for this batch : 0.04319\n",
            "class proba training loss for this batch : 0.04785\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.08287\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.02044\n",
            "confidence without object training loss for this batch : 0.03612\n",
            "class proba training loss for this batch : 0.04363\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.07609\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00016\n",
            "confidence with object training loss for this batch : 0.02348\n",
            "confidence without object training loss for this batch : 0.03572\n",
            "class proba training loss for this batch : 0.03365\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.07497\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.03330\n",
            "confidence without object training loss for this batch : 0.04344\n",
            "class proba training loss for this batch : 0.01904\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.12991\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.04061\n",
            "confidence without object training loss for this batch : 0.06286\n",
            "class proba training loss for this batch : 0.05702\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.06424\n",
            "xy_coord training loss for this batch : 0.00015\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.01527\n",
            "confidence without object training loss for this batch : 0.03205\n",
            "class proba training loss for this batch : 0.03135\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.10622\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00023\n",
            "confidence with object training loss for this batch : 0.03240\n",
            "confidence without object training loss for this batch : 0.05534\n",
            "class proba training loss for this batch : 0.04450\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.05259\n",
            "xy_coord training loss for this batch : 0.00018\n",
            "wh_sizes training loss for this batch : 0.00023\n",
            "confidence with object training loss for this batch : 0.01570\n",
            "confidence without object training loss for this batch : 0.03024\n",
            "class proba training loss for this batch : 0.01975\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.08334\n",
            "xy_coord training loss for this batch : 0.00054\n",
            "wh_sizes training loss for this batch : 0.00021\n",
            "confidence with object training loss for this batch : 0.02755\n",
            "confidence without object training loss for this batch : 0.03871\n",
            "class proba training loss for this batch : 0.03268\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.12842\n",
            "xy_coord training loss for this batch : 0.00024\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.04882\n",
            "confidence without object training loss for this batch : 0.07335\n",
            "class proba training loss for this batch : 0.04102\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:56.721960\n",
            "Mean training loss for this epoch : 0.08369\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 7/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.05451\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00019\n",
            "confidence with object training loss for this batch : 0.01478\n",
            "confidence without object training loss for this batch : 0.02802\n",
            "class proba training loss for this batch : 0.02422\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.06715\n",
            "xy_coord training loss for this batch : 0.00014\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.02101\n",
            "confidence without object training loss for this batch : 0.03877\n",
            "class proba training loss for this batch : 0.02538\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.06385\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.01487\n",
            "confidence without object training loss for this batch : 0.03632\n",
            "class proba training loss for this batch : 0.02952\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.10680\n",
            "xy_coord training loss for this batch : 0.00013\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.03402\n",
            "confidence without object training loss for this batch : 0.05434\n",
            "class proba training loss for this batch : 0.04432\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.10260\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.04385\n",
            "confidence without object training loss for this batch : 0.05872\n",
            "class proba training loss for this batch : 0.02817\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.09076\n",
            "xy_coord training loss for this batch : 0.00018\n",
            "wh_sizes training loss for this batch : 0.00018\n",
            "confidence with object training loss for this batch : 0.02743\n",
            "confidence without object training loss for this batch : 0.05180\n",
            "class proba training loss for this batch : 0.03565\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.05871\n",
            "xy_coord training loss for this batch : 0.00004\n",
            "wh_sizes training loss for this batch : 0.00007\n",
            "confidence with object training loss for this batch : 0.01105\n",
            "confidence without object training loss for this batch : 0.02359\n",
            "class proba training loss for this batch : 0.03532\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.04066\n",
            "xy_coord training loss for this batch : 0.00003\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.01180\n",
            "confidence without object training loss for this batch : 0.02655\n",
            "class proba training loss for this batch : 0.01506\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.07510\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.02218\n",
            "confidence without object training loss for this batch : 0.03829\n",
            "class proba training loss for this batch : 0.03287\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.05745\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.01998\n",
            "confidence without object training loss for this batch : 0.03895\n",
            "class proba training loss for this batch : 0.01697\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.19073\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.02661\n",
            "confidence without object training loss for this batch : 0.04557\n",
            "class proba training loss for this batch : 0.14070\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:12.574158\n",
            "Mean training loss for this epoch : 0.08056\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 8/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.07927\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00007\n",
            "confidence with object training loss for this batch : 0.02852\n",
            "confidence without object training loss for this batch : 0.04449\n",
            "class proba training loss for this batch : 0.02766\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.10151\n",
            "xy_coord training loss for this batch : 0.00024\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.03401\n",
            "confidence without object training loss for this batch : 0.05782\n",
            "class proba training loss for this batch : 0.03673\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.07040\n",
            "xy_coord training loss for this batch : 0.00013\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.02990\n",
            "confidence without object training loss for this batch : 0.05196\n",
            "class proba training loss for this batch : 0.01314\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.07875\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.02615\n",
            "confidence without object training loss for this batch : 0.04449\n",
            "class proba training loss for this batch : 0.02938\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.09732\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02366\n",
            "confidence without object training loss for this batch : 0.04658\n",
            "class proba training loss for this batch : 0.04968\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.07584\n",
            "xy_coord training loss for this batch : 0.00020\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.02877\n",
            "confidence without object training loss for this batch : 0.04067\n",
            "class proba training loss for this batch : 0.02504\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.10806\n",
            "xy_coord training loss for this batch : 0.00024\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.04151\n",
            "confidence without object training loss for this batch : 0.06483\n",
            "class proba training loss for this batch : 0.03224\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.09691\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.03430\n",
            "confidence without object training loss for this batch : 0.05421\n",
            "class proba training loss for this batch : 0.03440\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.07912\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.03432\n",
            "confidence without object training loss for this batch : 0.04667\n",
            "class proba training loss for this batch : 0.02029\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.07137\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00005\n",
            "confidence with object training loss for this batch : 0.02058\n",
            "confidence without object training loss for this batch : 0.03914\n",
            "class proba training loss for this batch : 0.03063\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.05552\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.01215\n",
            "confidence without object training loss for this batch : 0.01576\n",
            "class proba training loss for this batch : 0.03446\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:06.819568\n",
            "Mean training loss for this epoch : 0.08130\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 9/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.06489\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.00946\n",
            "confidence without object training loss for this batch : 0.02048\n",
            "class proba training loss for this batch : 0.04447\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.05294\n",
            "xy_coord training loss for this batch : 0.00004\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.01566\n",
            "confidence without object training loss for this batch : 0.02475\n",
            "class proba training loss for this batch : 0.02422\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.07293\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02097\n",
            "confidence without object training loss for this batch : 0.03436\n",
            "class proba training loss for this batch : 0.03407\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.10113\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.02283\n",
            "confidence without object training loss for this batch : 0.03617\n",
            "class proba training loss for this batch : 0.05917\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.06490\n",
            "xy_coord training loss for this batch : 0.00026\n",
            "wh_sizes training loss for this batch : 0.00020\n",
            "confidence with object training loss for this batch : 0.02060\n",
            "confidence without object training loss for this batch : 0.04534\n",
            "class proba training loss for this batch : 0.01933\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.11825\n",
            "xy_coord training loss for this batch : 0.00014\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.02599\n",
            "confidence without object training loss for this batch : 0.05779\n",
            "class proba training loss for this batch : 0.06195\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.06326\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.01985\n",
            "confidence without object training loss for this batch : 0.02962\n",
            "class proba training loss for this batch : 0.02780\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.06217\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00016\n",
            "confidence with object training loss for this batch : 0.02094\n",
            "confidence without object training loss for this batch : 0.03773\n",
            "class proba training loss for this batch : 0.02132\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.13603\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.05199\n",
            "confidence without object training loss for this batch : 0.07714\n",
            "class proba training loss for this batch : 0.04419\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.11675\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.03062\n",
            "confidence without object training loss for this batch : 0.04582\n",
            "class proba training loss for this batch : 0.06231\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.08727\n",
            "xy_coord training loss for this batch : 0.00004\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.03553\n",
            "confidence without object training loss for this batch : 0.06989\n",
            "class proba training loss for this batch : 0.01582\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:59.223043\n",
            "Mean training loss for this epoch : 0.08019\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 10/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.10570\n",
            "xy_coord training loss for this batch : 0.00021\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.03178\n",
            "confidence without object training loss for this batch : 0.06125\n",
            "class proba training loss for this batch : 0.04160\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.06053\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.01647\n",
            "confidence without object training loss for this batch : 0.03675\n",
            "class proba training loss for this batch : 0.02472\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.05142\n",
            "xy_coord training loss for this batch : 0.00015\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.01232\n",
            "confidence without object training loss for this batch : 0.02203\n",
            "class proba training loss for this batch : 0.02656\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.14592\n",
            "xy_coord training loss for this batch : 0.00020\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.04142\n",
            "confidence without object training loss for this batch : 0.07727\n",
            "class proba training loss for this batch : 0.06407\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.07642\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02089\n",
            "confidence without object training loss for this batch : 0.03983\n",
            "class proba training loss for this batch : 0.03486\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.07278\n",
            "xy_coord training loss for this batch : 0.00012\n",
            "wh_sizes training loss for this batch : 0.00020\n",
            "confidence with object training loss for this batch : 0.02589\n",
            "confidence without object training loss for this batch : 0.04409\n",
            "class proba training loss for this batch : 0.02326\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.06789\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.02204\n",
            "confidence without object training loss for this batch : 0.03870\n",
            "class proba training loss for this batch : 0.02569\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.07532\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.02212\n",
            "confidence without object training loss for this batch : 0.03580\n",
            "class proba training loss for this batch : 0.03447\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.05659\n",
            "xy_coord training loss for this batch : 0.00014\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.01872\n",
            "confidence without object training loss for this batch : 0.02611\n",
            "class proba training loss for this batch : 0.02362\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.08370\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02106\n",
            "confidence without object training loss for this batch : 0.03717\n",
            "class proba training loss for this batch : 0.04328\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.08118\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.00537\n",
            "confidence without object training loss for this batch : 0.00688\n",
            "class proba training loss for this batch : 0.07170\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:01.382402\n",
            "Mean training loss for this epoch : 0.07840\n"
          ]
        }
      ],
      "source": [
        "delta_time = datetime.timedelta(hours=1)\n",
        "timezone = datetime.timezone(offset=delta_time)\n",
        "\n",
        "t = datetime.datetime.now(tz=timezone)\n",
        "str_t = '{:%Y-%m-%d %H:%M:%S}'.format(t)\n",
        "print(f\"[START] : {str_t} :\")\n",
        "print(f\"[Training on] : {str(device).upper()}\")\n",
        "\n",
        "EPOCHS = 10\n",
        "size_grid = 6\n",
        "batch_loss_list = []\n",
        "model_MNIST = model_MNIST.to(device)\n",
        "check = []\n",
        "\n",
        "for epoch in range(EPOCHS) : \n",
        "    begin_time = timer()\n",
        "    epochs_loss = 0.\n",
        "    \n",
        "    print(\"-\"*20)\n",
        "    str_t = '{:%Y-%m-%d %H:%M:%S}'.format(t)\n",
        "    print(\" \"*5 + f\"{str_t} : EPOCH {epoch+1}/{EPOCHS}\")\n",
        "    print(\"-\"*20)\n",
        "\n",
        "    model_MNIST.train()\n",
        "    for batch, (img, labels, bbox_true) in enumerate(training_dataset):\n",
        "        loss = 0\n",
        "        begin_batch_time = timer()\n",
        "        img, labels, bbox_true = img.to(device), labels.to(device), bbox_true.to(device)\n",
        "        \n",
        "        ### turn bbox into NxSxSx5 tensor\n",
        "        bbox_true_6x6 = bbox2Tensor(bbox_true).to(device)\n",
        "        \n",
        "        ### clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        ### compute predictions\n",
        "        bbox_preds, label_preds = model_MNIST(img)\n",
        "        \n",
        "        ### compute losses over each grid cell for each image in the batch\n",
        "        check_xy, losses, loss = loss_yolo(bbox_preds, bbox_true_6x6, label_preds, labels)\n",
        "        check.append(check_xy)\n",
        "    \n",
        "        ### compute gradients\n",
        "        loss.backward()\n",
        "        \n",
        "        ### Weight updates\n",
        "        optimizer.step()\n",
        "        \n",
        "        ######### print part #######################\n",
        "        current_loss = loss.item()\n",
        "        batch_loss_list.append(current_loss)\n",
        "        epochs_loss = epochs_loss + current_loss\n",
        "\n",
        "        if batch+1 <= len_training_ds//BATCH_SIZE:\n",
        "            current_training_sample = (batch+1)*BATCH_SIZE\n",
        "        else:\n",
        "            current_training_sample = (batch)*BATCH_SIZE + len_training_ds%BATCH_SIZE\n",
        "        \n",
        "        if (batch) == 0 or (batch+1)%100 == 0 or batch == len_training_ds//BATCH_SIZE:\n",
        "            print(f\" --- Image : {current_training_sample}/{len_training_ds}\",\\\n",
        "                    f\" : loss = {current_loss:.5f}\")\n",
        "            print(f\"xy_coord training loss for this batch : {torch.sum(losses['loss_xy']) / len(img):.5f}\")\n",
        "            print(f\"wh_sizes training loss for this batch : {torch.sum(losses['loss_wh']) / len(img):.5f}\")\n",
        "            print(f\"confidence with object training loss for this batch : {torch.sum(losses['loss_conf_obj']) / len(img):.5f}\")\n",
        "            print(f\"confidence without object training loss for this batch : {torch.sum(losses['loss_conf_noobj']) / len(img):.5f}\")\n",
        "            print(f\"class proba training loss for this batch : {torch.sum(losses['loss_class']) / len(img):.5f}\")\n",
        "            print('\\n')\n",
        "            if batch == (len_training_ds//BATCH_SIZE):\n",
        "                print(f\"Total elapsed time for training : {datetime.timedelta(seconds=timer()-begin_time)}\")\n",
        "                print(f\"Mean training loss for this epoch : {epochs_loss / len(training_dataset):.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "pS0A-3zLweFR"
      },
      "outputs": [],
      "source": [
        "torch.save(model_MNIST.state_dict(), \"yolo_mnist_model_Xepochs.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "7O8u0S3PweFR"
      },
      "outputs": [],
      "source": [
        "def relative2absolute(bbox_relative:torch.Tensor, SIZEHW=75, S=6)->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Turns bounding box relative to cell coordinates into absolute coordinates \n",
        "    (pixels). Used to calculate IoU. \n",
        "\n",
        "    Args:\n",
        "        bbox_relative : torch.Tensor of shape (N, 4)\n",
        "            Bounding box coordinates to convert.\n",
        "    Return:\n",
        "        bbox_absolute : torch.Tensor of shape (N, 4)\n",
        "    \"\"\"\n",
        "    CELL_SIZE = SIZEHW/S\n",
        "\n",
        "    cx_rcell, cy_rcell, rw, rh = bbox_relative[:,:4].permute(1,0)\n",
        "    \n",
        "    ### xc,yc centers relative to the frame coordinates\n",
        "    cx = cx_rcell * CELL_SIZE - (1/CELL_SIZE) * (cx_rcell/CELL_SIZE).to(torch.int32)\n",
        "    cy = cy_rcell * CELL_SIZE - (1/CELL_SIZE) * (cy_rcell/CELL_SIZE).to(torch.int32)\n",
        "\n",
        "    ### xc,yc centers absolute coordinates\n",
        "    cx_abs = SIZEHW * cx\n",
        "    cy_abs = SIZEHW * cy\n",
        "\n",
        "    ### x,y absolute positions \n",
        "    x_min = cx_abs - (SIZEHW * (rw/2))\n",
        "    y_min = cy_abs - (SIZEHW * (rh/2))\n",
        "    x_max = cx_abs + (SIZEHW * (rw/2))\n",
        "    y_max = cy_abs + (SIZEHW * (rh/2))\n",
        "\n",
        "    bbox_absolute = torch.stack((x_min, y_min, x_max, y_max), dim=-1)\n",
        "    return bbox_absolute\n",
        "\n",
        "def intersection_over_union(pred_box:torch.Tensor, true_box:torch.Tensor)->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Intersection over Union method.\n",
        "\n",
        "    Args:\n",
        "        pred_box : torch.Tensor of shape (N, 5)\n",
        "            Predicted bounding boxes of a batch, in a given cell.\n",
        "        true_box : torch.Tensor of shape (N, 5)\n",
        "            Ground truth bounding boxes of a batch, in a given cell.\n",
        "\n",
        "    Return:\n",
        "        iou : float\n",
        "            Number between 0 and 1 where 1 is a perfect overlap.\n",
        "    \"\"\"\n",
        "    ### Convert cell reltative coordinates to absolute coordinates\n",
        "    pred_box = relative2absolute(pred_box)\n",
        "    true_box = relative2absolute(true_box)   \n",
        "    xmin_pred, ymin_pred, xmax_pred, ymax_pred = pred_box.permute(1,0)\n",
        "    xmin_true, ymin_true, xmax_true, ymax_true = true_box.permute(1,0)\n",
        "\n",
        "    ### There is no object if all coordinates are zero\n",
        "    isObject = xmin_true + ymin_true + xmax_true + ymax_true\n",
        "    isObject = isObject.to(torch.bool)\n",
        "\n",
        "    smoothing_factor = 1e-10\n",
        "\n",
        "    ### x, y overlaps btw pred and groundtrue\n",
        "    xmin_overlap = torch.maximum(xmin_pred, xmin_true)\n",
        "    xmax_overlap = torch.minimum(xmax_pred, xmax_true)\n",
        "    ymin_overlap = torch.maximum(ymin_pred, ymin_true)\n",
        "    ymax_overlap = torch.minimum(ymax_pred, ymax_true)\n",
        "    \n",
        "    ### Pred and groundtrue areas\n",
        "    pred_box_area = (xmax_pred - xmin_pred) * (ymax_pred - ymin_pred)\n",
        "    true_box_area = (xmax_true - xmin_true) * (ymax_true - ymin_true)\n",
        "\n",
        "    ### Compute intersection area, union area and IoU\n",
        "    overlap_area = torch.maximum((xmax_overlap - xmin_overlap), torch.Tensor([0]).to(device)) * torch.maximum((ymax_overlap - ymin_overlap), torch.Tensor([0]).to(device))\n",
        "    union_area = (pred_box_area + true_box_area) - overlap_area\n",
        "    iou = (overlap_area + smoothing_factor) / (union_area + smoothing_factor)\n",
        "    \n",
        "    ### Set IoU to zero when there is no coordinates (i.e. no object)\n",
        "    iou = iou * isObject\n",
        "\n",
        "    return iou   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "OWc1Cr8yrSH8",
        "outputId": "bd79409e-b55c-42a0-fa11-f609b48225cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE BOX : 0.00018\n",
            "MSE confidence score : 0.21276\n",
            "class acc : 99.04%\n"
          ]
        }
      ],
      "source": [
        "S=6\n",
        "for (img, labels, bbox_true) in validation_dataset:\n",
        "    img, labels, bbox_true = img.to(device), labels.to(device), bbox_true.to(device)\n",
        "    model_MNIST.eval()\n",
        "    with torch.no_grad():\n",
        "        ### prediction\n",
        "        bbox_preds, label_preds = model_MNIST(img)\n",
        "\n",
        "        ### (N,4) -> (N, S, S, 5)\n",
        "        bbox_true_6x6 = bbox2Tensor(bbox_true).to(device)\n",
        "\n",
        "        ### keeping only cells (i,j) with an object \n",
        "        cells_with_obj = bbox_true_6x6.nonzero()[::5]\n",
        "        N, cells_i, cells_j, _ = cells_with_obj.permute(1,0)\n",
        "\n",
        "        ### MSE along bbox coordinates and sizes in the cells containing an object\n",
        "        mse_box = (1/len(img)) * torch.sum(torch.pow(bbox_true - bbox_preds[N, cells_i, cells_j,:4],2))\n",
        "        \n",
        "        ### confidence score accuracy : sum of the all grid confidence scores\n",
        "        ### pred confidence score is confidence score times IoU.\n",
        "        mse_confidence_score = torch.zeros(len(img))\n",
        "        for i in range(S):\n",
        "            for j in range(S):\n",
        "                iou = intersection_over_union(bbox_true_6x6[:,i,j], bbox_preds[:,i,j])\n",
        "                mse_confidence_score += torch.pow(bbox_true_6x6[:,i,j,-1] - bbox_preds[:,i,j,-1] * iou,2)\n",
        "        \n",
        "        mse_confidence_score = (1/(len(img))) * torch.sum(mse_confidence_score)\n",
        "\n",
        "        ### applied softmax to class predictions and compute accuracy\n",
        "        softmax_pred_classes = torch.softmax(label_preds[N, cells_i, cells_j], dim=1)\n",
        "        classes_acc = (1/len(img)) * torch.sum(torch.argmax(labels, dim=1) == torch.argmax(softmax_pred_classes, dim=1))\n",
        "\n",
        "print(f\"MSE BOX : {mse_box.item():.5f}\")\n",
        "print(f\"MSE confidence score : {mse_confidence_score.item():.5f}\")\n",
        "print(f\"class acc : {classes_acc.item()*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "iAUtKFv24Rhq"
      },
      "outputs": [],
      "source": [
        "############### Matplotlib config\n",
        "plt.rc('image', cmap='gray')\n",
        "plt.rc('grid', linewidth=0)\n",
        "plt.rc('xtick', top=False, bottom=False, labelsize='large')\n",
        "plt.rc('ytick', left=False, right=False, labelsize='large')\n",
        "plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')\n",
        "plt.rc('text', color='a8151a')\n",
        "plt.rc('figure', facecolor='F0F0F0')# Matplotlib fonts\n",
        "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), \"mpl-data/fonts/ttf\")\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {},
      "outputs": [],
      "source": [
        "def draw_ONE_bounding_box_on_image(image, ymin:int, xmin:int, ymax:int, xmax:int, \n",
        "                               color:str='red', thickness:int=1, display_str:bool=None):\n",
        "  \"\"\"Adds a bounding box to an image.\n",
        "  Bounding box coordinates can be specified in either absolute (pixel) or\n",
        "  normalized coordinates by setting the use_normalized_coordinates argument.\n",
        "  \n",
        "  Args:\n",
        "    image: a PIL.Image object.\n",
        "    ymin: ymin of bounding box.\n",
        "    xmin: xmin of bounding box.\n",
        "    ymax: ymax of bounding box.\n",
        "    xmax: xmax of bounding box.\n",
        "    color: color to draw bounding box. Default is red.\n",
        "    thickness: line thickness. Default value is 4.\n",
        "    display_str_list: string to display in box\n",
        "    use_normalized_coordinates: If True (default), treat coordinates\n",
        "      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n",
        "      coordinates as absolute.\n",
        "  \"\"\"\n",
        "  draw = PIL.ImageDraw.Draw(image)\n",
        "  im_width, im_height = image.size\n",
        "  \n",
        "  left, right, top, bottom = xmin, xmax, ymin, ymax\n",
        "  \n",
        "  draw.line([(left, top), (left, bottom), (right, bottom), (right, top), (left, top)], width=thickness, fill=color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 334,
      "metadata": {},
      "outputs": [],
      "source": [
        "def draw_bounding_boxes_on_image(image, boxes_dict:dict, color_list:list=[], \n",
        "                                 thickness:int=1, display_str_list:tuple=()):\n",
        "  \"\"\"Draws bounding boxes on image.\n",
        "\n",
        "  Args:\n",
        "    image: PIL.Image.\n",
        "    boxes: numpy array of shape (N,4)\n",
        "      Contains (ymin, xmin, ymax, xmax). The coordinates are absolute.\n",
        "    color: list, default is empty\n",
        "      Color to draw bounding box.\n",
        "    thickness: int, default value is 4\n",
        "      Line thickness.\n",
        "    display_str_list: tuple\n",
        "      A list of strings for each bounding box.\n",
        "                           \n",
        "  Raises:\n",
        "    ValueError: if boxes is not a [N, 4] array\n",
        "  \"\"\"\n",
        "  for key in boxes_dict.keys():\n",
        "    if key == \"true_bbox\":\n",
        "      color = color_list[0]\n",
        "      thickness = 4\n",
        "    else : \n",
        "      color = color_list[1]\n",
        "      thickness = 1\n",
        "\n",
        "    boxes = np.asarray(boxes_dict[key])\n",
        "    boxes_shape = boxes.shape\n",
        "    if not boxes_shape:\n",
        "      return\n",
        "    if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n",
        "      raise ValueError('Input must be of size [N, 4]')\n",
        "    \n",
        "    for i in range(boxes_shape[0]):\n",
        "      draw_ONE_bounding_box_on_image(image, \n",
        "                                    boxes[i, 1], boxes[i, 0], \n",
        "                                    boxes[i, 3], boxes[i, 2], \n",
        "                                    color=color, thickness=thickness)\n",
        "                                    #, thickness, display_str_list[i])\n",
        "    \n",
        "                              "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 335,
      "metadata": {},
      "outputs": [],
      "source": [
        "def draw_bounding_boxes_on_image_array(image:np.ndarray, boxes:dict, color:list=[], \n",
        "                                       thickness:int=1, display_str_list:tuple=()):\n",
        "  \"\"\"Draws bounding boxes on image (numpy array).\n",
        "\n",
        "  Args:\n",
        "    image: a numpy array object.\n",
        "    ####boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
        "           The coordinates are in normalized format between [0, 1].######\n",
        "    color: color to draw bounding box. Default is red.\n",
        "    thickness: line thickness. Default value is 4.\n",
        "    display_str_list_list: a list of strings for each bounding box.\n",
        "  Raises:\n",
        "    ValueError: if boxes is not a [N, 4] array\n",
        "  \"\"\"\n",
        "  image_pil = PIL.Image.fromarray(image)\n",
        "  rgbimg = PIL.Image.new(\"RGBA\", image_pil.size)\n",
        "  rgbimg.paste(image_pil)\n",
        "  draw_bounding_boxes_on_image(rgbimg, boxes, color, thickness, display_str_list)\n",
        "  return np.array(rgbimg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 336,
      "metadata": {},
      "outputs": [],
      "source": [
        "################################################################################\n",
        "def display_digits_with_boxes(digits, predictions, labels, pred_bboxes, bboxes, title, nb_sample=10):\n",
        "  \"\"\"Utility to display a row of digits with their predictions.\n",
        "\n",
        "  Args:\n",
        "    digits : np.ndarray of shape (N,75,75,1)\n",
        "        Raw image with normalized pixel values (from 0 to 1)\n",
        "    predictions : np.ndarray of shape (N,)\n",
        "        Predicted label with the same shape as labels\n",
        "    labels : np.ndarray of shape (N,)\n",
        "        Labels of the digits (from 0 to 9)\n",
        "    pred_bboxes : np.ndarray of shape (N, 4) ??\n",
        "        Predicted bboxes locations\n",
        "    bboxes : np.ndarray of shape (N, 4)\n",
        "        Ground true bboxe locations\n",
        "    iou : list of shape (N,)\n",
        "        IoU of each bboxes\n",
        "    title : str\n",
        "        Figure's title\n",
        "  \"\"\"\n",
        "  iou_threshold = 0.6\n",
        "  nb_sample = 10\n",
        "  indexes = np.random.choice(len(predictions), size=nb_sample)\n",
        "  \n",
        "  n_digits = digits[indexes].numpy()\n",
        "  # Rescale pixel values to un-normed values (from 0 -black- to 255 -white-)\n",
        "  n_digits = n_digits * 255.0\n",
        "  n_digits = n_digits.reshape(nb_sample, 75, 75)\n",
        "  \n",
        "  n_predictions = predictions[indexes]\n",
        "  # Argmax of one-hot vectors. Shape : (N,S,S,10) -> (N,S,S)\n",
        "  n_predictions = torch.argmax(torch.softmax(n_predictions, dim=-1), dim=-1).numpy()\n",
        "  \n",
        "  ### shape : (N, S, S, 5)\n",
        "  n_pred_bboxes = pred_bboxes[indexes]\n",
        "\n",
        "  ### shape : (N, 4)\n",
        "  n_bboxes_rel = bboxes[indexes]\n",
        "  n_bboxes = relative2absolute(torch.as_tensor(n_bboxes_rel)).numpy()\n",
        "  # n_bboxes = n_bboxes_rel/75\n",
        "\n",
        "  # Set plot config\n",
        "  fig = plt.figure(figsize=(20, 4))\n",
        "  plt.title(title)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  \n",
        "  bboxes_to_plot = {\"true_bbox\":[], \"pred_bbox\":[]}\n",
        "  for i in range(nb_sample):\n",
        "    bboxes_to_plot[\"pred_bbox\"] = []\n",
        "    bboxes_to_plot[\"true_bbox\"] = []\n",
        "    \n",
        "    for cell_i in range(6):\n",
        "      for cell_j in range(6):\n",
        "        n_pred_bboxes_ij = n_pred_bboxes[:, cell_i, cell_j, :4]\n",
        "        \n",
        "        # n_iou = intersection_over_union(n_pred_bboxes_ij, n_bboxes_rel)\n",
        "        \n",
        "        # n_predictions_ij = n_predictions[:, cell_i, cell_j]\n",
        "\n",
        "        n_pred_bboxes_ij = relative2absolute(n_pred_bboxes_ij).numpy()\n",
        "        \n",
        "        bboxes_to_plot[\"pred_bbox\"].append(n_pred_bboxes_ij[i])\n",
        "    \n",
        "    bboxes_to_plot[\"true_bbox\"].append(n_bboxes[i])\n",
        "    \n",
        "    ax = fig.add_subplot(1, nb_sample, i+1)\n",
        "    img_to_draw = draw_bounding_boxes_on_image_array(image=n_digits[i], boxes = bboxes_to_plot, color=[\"white\", \"red\"])#, display_str_list=[\"true\", \"pred\"])\n",
        "    # img_to_draw = draw_bounding_boxes_on_image_array(image=n_digits[i], boxes = np.asarray(bboxes_to_plot[\"pred_bbox\"]), color=\"red\")#, display_str_list=[\"true\", \"pred\"])\n",
        "# plt.xlabel(n_predictions[i])\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "# if n_predictions[i] != n_labels[i]:\n",
        "#   ax.xaxis.label.set_color('red')\n",
        "\n",
        "    plt.imshow(img_to_draw)\n",
        " \n",
        "# if len(iou) > i :\n",
        "# color = \"black\"\n",
        "# if (n_iou[i] < iou_threshold):\n",
        "#   color = \"red\"\n",
        "# ax.text(0.2, -0.3, \"iou: %s\" %(n_iou[i]), color=color, transform=ax.transAxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1440x288 with 11 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display_digits_with_boxes(img, label_preds, labels, bbox_preds, bbox_true, \"Un titre\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "YOLO_MNIST_Localization.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "bfc2e043f4a41bbce066f7921047bdcfe8769546c7198e4df6482fd90de274f1"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04f6d46789754cfd9f0a441baa32af45": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06ff67cf46e64c3e8ba324f03a84c5ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7a121501cfd438895e7bd2d85e56251",
              "IPY_MODEL_c1c21fc51a224d60b314af87cbde69d4",
              "IPY_MODEL_97522edebd964d2aa9671dac5a5f381d"
            ],
            "layout": "IPY_MODEL_216234dec1484887a68608c520b3f093"
          }
        },
        "0963b6c3e45e4e5dabfd2e0d9b6cca38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09ed36dea11c480d8e0f38478998b828": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b60a12468347494d9c4fe04cd86af58e",
            "placeholder": "​",
            "style": "IPY_MODEL_f68b6dd7bde64b71afb10a929c410ed9",
            "value": "100%"
          }
        },
        "15b3c78cbd51469ca2b870b993b6fca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "201eec9906304c4e95854aa2fca10439": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5d88a84b1a24e98b3ee1d1b469defff",
            "max": 9912422,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc8b573ffafd427495a1ccf3b818e815",
            "value": 9912422
          }
        },
        "216234dec1484887a68608c520b3f093": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24e840cc10c54af0a0d40c6eb27beda1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30d2ecca2c46436198f644e8d6dc7f24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46612f5ff5024571917d3275717a5ae6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ee695bbe8e543a2b6cc48b97ed63b3e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54e7c876fe4d4be482733d96848b5a51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58c90db1a5df4c0bb4d6c6cc8cd98dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd7117704fa347838c6b19067ebca54c",
            "placeholder": "​",
            "style": "IPY_MODEL_15b3c78cbd51469ca2b870b993b6fca0",
            "value": " 1648877/1648877 [00:00&lt;00:00, 5302356.85it/s]"
          }
        },
        "5f502d93a2fc46398195151da51c0083": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c99c88b52334f53a1a3c8e89f5c3d7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74c88f9633d541b0a60149df3d9a606c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d20b57ac5b44fbd91e7cee432070247": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dc6465a02ed4763b527f143ad9e254c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "809cb638d85e48999892dfd8cfc815a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a64e70ecfbe46558b9865815d283596",
              "IPY_MODEL_ae6fd6d93a5647a28ffd9af5e26942e6",
              "IPY_MODEL_aad87ec81bb34aadac8c84ae9b505c68"
            ],
            "layout": "IPY_MODEL_cdfd8330d4964e9fa41bfe4fd1639def"
          }
        },
        "83fbb691d40b4d02b029bcbdb2f85590": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85daafeb4a1a4b55bc6b63ba3f088bd5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c60136a50bd4bf6ad83c4d26f8d8aa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97522edebd964d2aa9671dac5a5f381d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dc6465a02ed4763b527f143ad9e254c",
            "placeholder": "​",
            "style": "IPY_MODEL_7d20b57ac5b44fbd91e7cee432070247",
            "value": " 28881/28881 [00:00&lt;00:00, 810071.71it/s]"
          }
        },
        "9811f18275dd46f397a0dc445bcbd90a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c63e5aa3160c49d28f9a6df08abb2558",
              "IPY_MODEL_201eec9906304c4e95854aa2fca10439",
              "IPY_MODEL_da28120108424e88bdb8599bd0f8d4f7"
            ],
            "layout": "IPY_MODEL_83fbb691d40b4d02b029bcbdb2f85590"
          }
        },
        "9a64e70ecfbe46558b9865815d283596": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9418919bc204bb6af14a33815d1bd5f",
            "placeholder": "​",
            "style": "IPY_MODEL_24e840cc10c54af0a0d40c6eb27beda1",
            "value": "100%"
          }
        },
        "a4fef10f26744768b57986d380689412": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a66241a6cd464e339bef5fb2fceab412": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aad87ec81bb34aadac8c84ae9b505c68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b34a8acab2614688bef300d8be847cf6",
            "placeholder": "​",
            "style": "IPY_MODEL_0963b6c3e45e4e5dabfd2e0d9b6cca38",
            "value": " 4542/4542 [00:00&lt;00:00, 136321.56it/s]"
          }
        },
        "ae6fd6d93a5647a28ffd9af5e26942e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ee695bbe8e543a2b6cc48b97ed63b3e",
            "max": 4542,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54e7c876fe4d4be482733d96848b5a51",
            "value": 4542
          }
        },
        "b34a8acab2614688bef300d8be847cf6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b60a12468347494d9c4fe04cd86af58e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7a121501cfd438895e7bd2d85e56251": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74c88f9633d541b0a60149df3d9a606c",
            "placeholder": "​",
            "style": "IPY_MODEL_a4fef10f26744768b57986d380689412",
            "value": "100%"
          }
        },
        "c136bc57bec64d38b2505a0c60e6f64b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1c21fc51a224d60b314af87cbde69d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04f6d46789754cfd9f0a441baa32af45",
            "max": 28881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f502d93a2fc46398195151da51c0083",
            "value": 28881
          }
        },
        "c63e5aa3160c49d28f9a6df08abb2558": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30d2ecca2c46436198f644e8d6dc7f24",
            "placeholder": "​",
            "style": "IPY_MODEL_6c99c88b52334f53a1a3c8e89f5c3d7f",
            "value": "100%"
          }
        },
        "c9418919bc204bb6af14a33815d1bd5f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc8b573ffafd427495a1ccf3b818e815": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cdfd8330d4964e9fa41bfe4fd1639def": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5d88a84b1a24e98b3ee1d1b469defff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da28120108424e88bdb8599bd0f8d4f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46612f5ff5024571917d3275717a5ae6",
            "placeholder": "​",
            "style": "IPY_MODEL_8c60136a50bd4bf6ad83c4d26f8d8aa7",
            "value": " 9912422/9912422 [00:00&lt;00:00, 20048947.36it/s]"
          }
        },
        "dd7117704fa347838c6b19067ebca54c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e18767ed9f8944b5a8091d5f39e986d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c136bc57bec64d38b2505a0c60e6f64b",
            "max": 1648877,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a66241a6cd464e339bef5fb2fceab412",
            "value": 1648877
          }
        },
        "f68b6dd7bde64b71afb10a929c410ed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f85da4cd1a64460bb2580b69d8fa066d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09ed36dea11c480d8e0f38478998b828",
              "IPY_MODEL_e18767ed9f8944b5a8091d5f39e986d9",
              "IPY_MODEL_58c90db1a5df4c0bb4d6c6cc8cd98dce"
            ],
            "layout": "IPY_MODEL_85daafeb4a1a4b55bc6b63ba3f088bd5"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
