{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThOpaque/Food_Recognition/blob/main/YOLO_MNIST_Localization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAHeh1BRufX4",
        "outputId": "dc6de471-93f4-4d41-d781-9574240fd3fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchinfo in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (1.7.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: torchmetrics in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (0.9.3)\n",
            "Requirement already satisfied: packaging in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from torchmetrics) (1.23.1)\n",
            "Requirement already satisfied: torch>=1.3.1 in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from torchmetrics) (1.13.0.dev20220730)\n",
            "Requirement already satisfied: typing-extensions in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from torch>=1.3.1->torchmetrics) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "import os, time, datetime\n",
        "from timeit import default_timer as timer\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "%pip install torchinfo;\n",
        "%pip install torchmetrics;\n",
        "from torchmetrics import MeanSquaredError;\n",
        "from torchinfo import summary;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOraK1TX7XZB",
        "outputId": "3001e719-ddf8-47a6-fcf4-149184181740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------------\n",
            "Execute notebook on - mps -\n",
            "------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Choosing device between CPU or GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "elif torch.has_mps:\n",
        "    device=torch.device('mps')\n",
        "else:\n",
        "    device=torch.device('cpu')\n",
        "    \n",
        "print(\"\\n------------------------------------\")\n",
        "print(f\"Execute notebook on - {device} -\")\n",
        "print(\"------------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "M7VztqFE71JZ"
      },
      "outputs": [],
      "source": [
        "class my_mnist_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root:str, split:str=None, download:bool=False, S=6, sizeHW=75):\n",
        "        assert split, \"You have to specify the split.\"\n",
        "        \n",
        "        if split == \"train\":\n",
        "            train = True\n",
        "        elif split == \"test\":\n",
        "            train = False\n",
        "        \n",
        "        self.dataset = torchvision.datasets.MNIST(root=root, train=train, download=download)\n",
        "        \n",
        "        self.cell_size = sizeHW / S\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def _numpy_pad_to_bounding_box(self, image, offset_height=0, offset_width=0, target_height=0, target_width=0):\n",
        "        assert image.shape[:-1][0] <= target_height-offset_height, \"height must be <= target - offset\"\n",
        "        assert image.shape[:-1][1] <= target_width-offset_width, \"width must be <= target - offset\"\n",
        "        \n",
        "        target_array = np.zeros((target_height, target_width, image.shape[-1]))\n",
        "\n",
        "        for k in range(image.shape[0]):\n",
        "            target_array[offset_height+k][offset_width:image.shape[1]+offset_width] = image[k]\n",
        "        \n",
        "        return target_array\n",
        "\n",
        "    def _transform_pasting75(self, image, label):\n",
        "        ### xmin, ymin of digit\n",
        "        xmin = torch.randint(0, 48, (1,))\n",
        "        ymin = torch.randint(0, 48, (1,))\n",
        "        \n",
        "        image = torchvision.transforms.ToTensor()(image)\n",
        "        image = torch.reshape(image, (28,28,1,))\n",
        "        image = torch.from_numpy(self._numpy_pad_to_bounding_box(image, ymin, xmin, 75, 75))\n",
        "        image = image.permute(2, 0, 1) #(C,H,W)\n",
        "        image = image.to(torch.float)\n",
        "        \n",
        "        xmin, ymin = xmin.to(torch.float), ymin.to(torch.float)\n",
        "\n",
        "        xmax_bbox, ymax_bbox = (xmin + 28), (ymin + 28)\n",
        "        xmin_bbox, ymin_bbox = xmin, ymin\n",
        "        w_bbox = xmax_bbox-xmin_bbox\n",
        "        h_bbox = ymax_bbox-ymin_bbox\n",
        "\n",
        "        rw = w_bbox / 75\n",
        "        rh = h_bbox / 75\n",
        "        cx = (xmin + (w_bbox/2))/75\n",
        "        cy = (ymin + (h_bbox/2))/75\n",
        "\n",
        "        cx_rcell = cx % self.cell_size / self.cell_size\n",
        "        cy_rcell = cy % self.cell_size / self.cell_size\n",
        "\n",
        "\n",
        "        label_one_hot = F.one_hot(torch.as_tensor(label, dtype=torch.int64), 10)\n",
        "        bbox_coord = torch.Tensor([cx_rcell, cy_rcell, rw, rh])\n",
        "\n",
        "        return image, label_one_hot, bbox_coord\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        image, one_hot_label, bbox_coord = self._transform_pasting75(self.dataset[idx][0], self.dataset[idx][1])\n",
        "        \n",
        "        return image, one_hot_label.to(torch.float), bbox_coord\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "yQNznLfO8jOx"
      },
      "outputs": [],
      "source": [
        "def get_training_dataset(BATCH_SIZE=64):\n",
        "    \"\"\"\n",
        "    Loads and maps the training split of the dataset using the custom dataset class. \n",
        "    \"\"\"\n",
        "    dataset = my_mnist_dataset(root=\"data\", split=\"train\", download=True)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    return dataloader, len(dataset)\n",
        "\n",
        "def get_validation_dataset(BATCH_SIZE = None):\n",
        "    \"\"\"\n",
        "    Loads and maps the validation split of the datasetusing the custom dataset class. \n",
        "    \"\"\"\n",
        "    dataset = my_mnist_dataset(root=\"data\", split=\"test\", download=True)\n",
        "    if BATCH_SIZE is None:\n",
        "        BATCH_SIZE = len(dataset)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    return dataloader, len(dataset)\n",
        "\n",
        "# instantiate the datasets\n",
        "training_dataset, len_training_ds = get_training_dataset()\n",
        "validation_dataset, len_validation_ds = get_validation_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wfgUx1srt90c"
      },
      "outputs": [],
      "source": [
        "class CNNBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(CNNBlock, self).__init__()\n",
        "        self.conv = torch.nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "        self.bn = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.l_relu = torch.nn.LeakyReLU(0.1)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        x = self.conv(input)\n",
        "        x = self.bn(x)\n",
        "        return self.l_relu(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3VjCkTEce-CT"
      },
      "outputs": [],
      "source": [
        "class YoloMNIST(torch.nn.Module):\n",
        "    def __init__(self, sizeHW, S, C, B):\n",
        "        super(YoloMNIST, self).__init__()\n",
        "        self.S, self.C, self.B = S, C, B\n",
        "        self.sizeHW = sizeHW\n",
        "        self.cell_size = self.sizeHW / self.S\n",
        "\n",
        "        self.seq = torch.nn.Sequential()        \n",
        "        self.seq.add_module(f\"conv_1\", CNNBlock(1, 32, stride=2, kernel_size=7, padding=2))\n",
        "        self.seq.add_module(f\"maxpool_1\", torch.nn.MaxPool2d(2))\n",
        "        self.seq.add_module(f\"conv_3\", CNNBlock(32, 128, stride=1, kernel_size=3, padding=0))\n",
        "        self.seq.add_module(f\"maxpool_2\", torch.nn.MaxPool2d(2))\n",
        "        self.seq.add_module(f\"conv_5\", CNNBlock(128, 64, stride=1, kernel_size=1, padding=0))\n",
        "        self.seq.add_module(f\"conv_4\", CNNBlock(64, 128, stride=1, kernel_size=3, padding=0))\n",
        "        self.seq.add_module(f\"conv_6\", CNNBlock(128, 128, stride=1, kernel_size=3, padding=1))\n",
        "        \n",
        "        self.fcs = self._create_fcs()\n",
        "\n",
        "    def _size_output(self, sizeHW:int, kernel:int, stride:int, padding:int=0, isMaxPool:bool=False)->int:\n",
        "        \"\"\"\n",
        "        Output size (width/height) of convolutional or maxpool layers.\n",
        "\n",
        "        Args:\n",
        "            sizeHW : int\n",
        "                Image size (we suppose this is a square image)\n",
        "            kernel : int\n",
        "                Size of a square kernel\n",
        "            stride : int\n",
        "                Stride of convolution layer\n",
        "            padding : int\n",
        "                Padding of convolution layer\n",
        "            isMaxPool : Bool, default is False.\n",
        "                Specify if it is a Maxpool layer (True) or not (False). \n",
        "\n",
        "        Return:\n",
        "            output_size : int\n",
        "                Image output size after a convolutional or MaxPool layer.\n",
        "        \"\"\" \n",
        "        if isMaxPool == True:\n",
        "            output_size = int(sizeHW/2)\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "        if padding == 'same':\n",
        "            output_size = sizeHW\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "        else:\n",
        "            output_size = (sizeHW + 2 * padding - (kernel-1)-1)/stride\n",
        "            output_size = int(output_size + 1)\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "\n",
        "    def _create_fcs(self):\n",
        "        output = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(128 * self.S * self.S, 4096),\n",
        "            torch.nn.LeakyReLU(0.1),\n",
        "            torch.nn.Linear(4096, self.S * self.S * (self.C + self.B*5))\n",
        "        )\n",
        "        return output\n",
        "    \n",
        "\n",
        "    def forward(self, input:torch.Tensor)->tuple:\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            input : torch.Tensor of shape (N, C, H, W)\n",
        "                Batch of images.\n",
        "\n",
        "        Return:\n",
        "            box_coord : torch.Tensor of shape (N, 6, 6, 5)\n",
        "                Contains xc_rcell, yc_rcell, rw, rh and the confidence number c\n",
        "                over 6x6 grid cells.\n",
        "            classifier : torch.Tensor of shape (N, 6, 6, 10)\n",
        "                Contains the one-hot encoding of each digit number over\n",
        "                6x6 grid cells.\n",
        "        \"\"\"     \n",
        "        x = self.seq(input)\n",
        "        x = self.fcs(x)\n",
        "        x = x.view(x.size(0), self.S, self.S, self.B * 5 + self.C)\n",
        "        box_coord = x[:,:,:,0:5]\n",
        "        classifier = x[:,:,:,5:]\n",
        "        return box_coord, classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "T_jOg_i_p2_c"
      },
      "outputs": [],
      "source": [
        "class YoloLoss(torch.nn.Module):\n",
        "    def __init__(self, lambd_coord:int, lambd_noobj:float, device:torch.device, S:int=6, sizeHW:int=75):\n",
        "        super(YoloLoss, self).__init__()\n",
        "        self.LAMBD_COORD = lambd_coord\n",
        "        self.LAMBD_NOOBJ = lambd_noobj\n",
        "        self.S = S\n",
        "        self.SIZEHW = sizeHW\n",
        "        self.CELL_SIZE = sizeHW/S\n",
        "        self.device = device\n",
        "\n",
        "    def _coordloss(self, pred_coord_rcell, true_coord_rcell):\n",
        "        \"\"\"\n",
        "        Args : \n",
        "            pred_coord_rcell : torch.Tensor of shape (N, 2)\n",
        "            true_coord_rcell : torch.Tensor of shape (N, 2)\n",
        "        Returns :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        xc_hat, yc_hat = pred_coord_rcell.permute(1,0)\n",
        "        xc, yc = true_coord_rcell.permute(1,0)\n",
        "        \n",
        "        xc_hat = xc_hat.clip(min=0)\n",
        "        yc_hat = yc_hat.clip(min=0)\n",
        "\n",
        "        squared_error = torch.pow(xc - xc_hat,2) + torch.pow(yc - yc_hat,2)\n",
        "        return squared_error\n",
        "\n",
        "    def _sizeloss(self, pred_size, true_size):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_size : torch.Tensor of shape (N, 2)\n",
        "            true_size : torch.Tensor of shape (N, 2)\n",
        "        Returns : \n",
        "            root_squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        rw_hat, rh_hat = pred_size.permute(1,0)\n",
        "        rw, rh = true_size.permute(1,0)\n",
        "\n",
        "        #sizes can't be negative\n",
        "        rw_hat = rw_hat.clip(min=0)\n",
        "        rh_hat = rh_hat.clip(min=0)\n",
        "\n",
        "        root_squared_error_w = torch.pow(torch.sqrt(rw) - torch.sqrt(rw_hat),2)\n",
        "        root_squared_error_h = torch.pow(torch.sqrt(rh) - torch.sqrt(rh_hat),2)\n",
        "        root_squared_error = root_squared_error_w + root_squared_error_h\n",
        "        return root_squared_error\n",
        "\n",
        "    def _confidenceloss(self, pred_c, true_c):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_c : torch.Tensor of shape (N)\n",
        "            true_c : torch.Tensor of shape (N)\n",
        "        Return :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        squared_error = torch.pow(true_c - pred_c, 2)\n",
        "        return squared_error\n",
        "\n",
        "    def _classloss(self, pred_class, true_class):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_class : torch.Tensor of shape (N, 10)\n",
        "            true_class : torch.Tensor of shape (N, 10)\n",
        "        Returns :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        squared_error = torch.pow(true_class - pred_class, 2)\n",
        "        return torch.sum(squared_error, dim=1)\n",
        "    \n",
        "    def _relative2absolute(self, bbox_relative:torch.Tensor)->torch.Tensor:\n",
        "        \"\"\"\n",
        "        Turns bounding box relative to cell coordinates into absolute coordinates \n",
        "        (pixels). Used to calculate IoU. \n",
        "\n",
        "        Args:\n",
        "            bbox_relative : torch.Tensor of shape (N, 4)\n",
        "                Bounding box coordinates to convert.\n",
        "        Return:\n",
        "            bbox_absolute : torch.Tensor of shape (N, 4)\n",
        "        \"\"\"\n",
        "        cx_rcell, cy_rcell, rw, rh = bbox_relative[:,:4].permute(1,0)\n",
        "        \n",
        "        ### xc,yc centers relative to the frame coordinates\n",
        "        cx = cx_rcell * self.CELL_SIZE - (1/self.CELL_SIZE) * (cx_rcell/self.CELL_SIZE).to(torch.int32)\n",
        "        cy = cy_rcell * self.CELL_SIZE - (1/self.CELL_SIZE) * (cy_rcell/self.CELL_SIZE).to(torch.int32)\n",
        "\n",
        "        ### xc,yc centers absolute coordinates\n",
        "        cx_abs = self.SIZEHW * cx\n",
        "        cy_abs = self.SIZEHW * cy\n",
        "\n",
        "        ### x,y absolute positions \n",
        "        x_min = cx_abs - (self.SIZEHW * (rw/2))\n",
        "        y_min = cy_abs - (self.SIZEHW * (rh/2))\n",
        "        x_max = cx_abs + (self.SIZEHW * (rw/2))\n",
        "        y_max = cy_abs + (self.SIZEHW * (rh/2))\n",
        "\n",
        "        bbox_absolute = torch.stack((x_min, y_min, x_max, y_max), dim=-1)\n",
        "        return bbox_absolute\n",
        "\n",
        "    def _intersection_over_union(self, pred_box:torch.Tensor, true_box:torch.Tensor)->torch.Tensor:\n",
        "        \"\"\"\n",
        "        Intersection over Union method.\n",
        "\n",
        "        Args:\n",
        "            pred_box : torch.Tensor of shape (N, 5)\n",
        "                Predicted bounding boxes of a batch, in a given cell.\n",
        "            true_box : torch.Tensor of shape (N, 4)\n",
        "                Ground truth bounding boxes of a batch, in a given cell.\n",
        "\n",
        "        Return:\n",
        "            iou : float\n",
        "                Number between 0 and 1 where 1 is a perfect overlap.\n",
        "        \"\"\"\n",
        "        ### Convert cell reltative coordinates to absolute coordinates\n",
        "        pred_box = self._relative2absolute(pred_box)\n",
        "        true_box = self._relative2absolute(true_box)   \n",
        "        xmin_pred, ymin_pred, xmax_pred, ymax_pred = pred_box.permute(1,0)\n",
        "        xmin_true, ymin_true, xmax_true, ymax_true = true_box.permute(1,0)\n",
        "\n",
        "        ### There is no object if all coordinates are zero\n",
        "        isObject = xmin_true + ymin_true + xmax_true + ymax_true\n",
        "        isObject = isObject.to(torch.bool)\n",
        "\n",
        "        smoothing_factor = 1e-10\n",
        "\n",
        "        ### x, y overlaps btw pred and groundtrue\n",
        "        xmin_overlap = torch.maximum(xmin_pred, xmin_true)\n",
        "        xmax_overlap = torch.minimum(xmax_pred, xmax_true)\n",
        "        ymin_overlap = torch.maximum(ymin_pred, ymin_true)\n",
        "        ymax_overlap = torch.minimum(ymax_pred, ymax_true)\n",
        "        \n",
        "        ### Pred and groundtrue areas\n",
        "        pred_box_area = (xmax_pred - xmin_pred) * (ymax_pred - ymin_pred)\n",
        "        true_box_area = (xmax_true - xmin_true) * (ymax_true - ymin_true)\n",
        "\n",
        "        ### Compute intersection area, union area and IoU\n",
        "        overlap_area = torch.maximum((xmax_overlap - xmin_overlap), torch.Tensor([0]).to(self.device)) * torch.maximum((ymax_overlap - ymin_overlap), torch.Tensor([0]).to(self.device))\n",
        "        union_area = (pred_box_area + true_box_area) - overlap_area\n",
        "        iou = (overlap_area + smoothing_factor) / (union_area + smoothing_factor)\n",
        "        \n",
        "        ### Set IoU to zero when there is no coordinates (i.e. no object)\n",
        "        iou = iou * isObject\n",
        "\n",
        "        return iou   \n",
        "\n",
        "\n",
        "    def forward(self, pred_box:torch.Tensor, true_box:torch.Tensor, pred_class:torch.Tensor, true_class:torch.Tensor):\n",
        "        \"\"\"\n",
        "        Grid forward pass.\n",
        "\n",
        "        Args:\n",
        "            pred_box : torch.Tensor of shape (N, S, S, 5)\n",
        "                Batch predicted outputs containing xc_rcell, yc_rcell, rw, rh,\n",
        "                and confident number c for each grid cell.\n",
        "            true_box : torch.Tensor of shape (N, S, S, 5)\n",
        "                Groundtrue batch containing bbox values for each cell and\n",
        "                c indicate if there is an object to detect or not (1/0).\n",
        "            pred_class : torch.Tensor of shape (N, S, S, 10)\n",
        "                Probability of each digit class in each grid cell\n",
        "            true_class : torch.Tensor of shape (N, 10)\n",
        "                one-hot vect of each digit\n",
        "\n",
        "        Return:\n",
        "            loss : float\n",
        "                The batch loss value of the grid\n",
        "        \"\"\"\n",
        "        BATCH_SIZE = len(pred_box)\n",
        "\n",
        "        ### Initialization of the losses\n",
        "        losses_list = ['loss_xy', 'loss_wh', 'loss_conf_obj', 'loss_conf_noobj', 'loss_class']\n",
        "        losses = {key : torch.zeros(BATCH_SIZE).to(self.device) for key in losses_list}\n",
        "        \n",
        "        ### Compute the losses for all images in the batch\n",
        "        for i in range(self.S):\n",
        "            for j in range(self.S):\n",
        "                ### Intersection over Union\n",
        "                IoU = self._intersection_over_union(pred_box[:,i,j], true_box[:,i,j])\n",
        "\n",
        "                ### bbox coordinates\n",
        "                xy_hat = pred_box[:,i,j,:2]\n",
        "                xy = true_box[:,i,j,:2]\n",
        "                wh_hat = pred_box[:,i,j,2:4]\n",
        "                wh = true_box[:,i,j,2:4]\n",
        "                \n",
        "                ### confidence numbers\n",
        "                pred_c = pred_box[:,i,j,4]# * IoU\n",
        "                true_c = true_box[:,i,j,4]\n",
        "\n",
        "                ### objects to detect\n",
        "                isObject = true_c.to(torch.bool)\n",
        "                isNoObject = torch.logical_not(true_c) #(~bool) doesn't work on MPS device\n",
        "\n",
        "                ### sum the losses over the grid\n",
        "                losses['loss_xy'] += isObject * self._coordloss(xy_hat, xy)\n",
        "                losses['loss_wh'] += isObject * self._sizeloss(wh_hat, wh)\n",
        "                losses['loss_conf_obj'] += isObject * self._confidenceloss(pred_c, true_c)\n",
        "                losses['loss_conf_noobj'] += isNoObject * self._confidenceloss(pred_c, true_c)\n",
        "                losses['loss_class'] += isObject * self._classloss(pred_class[:,i,j], true_class)\n",
        "\n",
        "        ### Yolo_v1 loss over the batch, shape : (BATCH_SIZE)\n",
        "        loss = self.LAMBD_COORD * losses['loss_xy'] \\\n",
        "                + self.LAMBD_COORD * losses['loss_wh'] \\\n",
        "                + losses['loss_conf_obj'] \\\n",
        "                + self.LAMBD_NOOBJ * losses['loss_conf_noobj'] \\\n",
        "                + losses['loss_class']\n",
        "        \n",
        "        loss = torch.sum(loss) / BATCH_SIZE\n",
        "\n",
        "        return losses, loss\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OdIlkN61mXJj"
      },
      "outputs": [],
      "source": [
        "def bbox2Tensor(bbox:torch.Tensor, S:int=6, sizeHW:int=75, device=torch.device('cpu'))->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Constructs en Tensor and puts bbox values in the corresponding i,j grid cell.\n",
        "\n",
        "    Args :\n",
        "        bbox : torch.Tensor of shape (N,4)\n",
        "            Contains bbox values xc_rcell, yc_rcell, rw and rh.\n",
        "        S : int, default is 6\n",
        "            Size of the grid.\n",
        "        sizeHW : int, default is 75\n",
        "            Size of the image.\n",
        "\n",
        "    Return :\n",
        "        bbox_t : torch.Tensor of shape (N, S, S, 4)\n",
        "            Tensor containing all 4 bbox values in the corresponding i,j grid\n",
        "            cell position i.e. in the i,j position where an object should be\n",
        "            detected.\n",
        "    \"\"\"\n",
        "    N = len(bbox)\n",
        "    bbox_t = torch.zeros(N,S,S,5).to(device)\n",
        "    cell_size = sizeHW/S\n",
        "\n",
        "    xc_rcell, yc_rcell, rw, rh = bbox.permute(1,0).to(device)\n",
        "    xc = xc_rcell * cell_size - (1/cell_size) * (xc_rcell/cell_size).to(torch.int32)\n",
        "    yc = yc_rcell * cell_size - (1/cell_size) * (yc_rcell/cell_size).to(torch.int32)\n",
        "\n",
        "    N_range = torch.arange(N)\n",
        "    lines = (yc * S).to(torch.long)\n",
        "    columns = (xc * S).to(torch.long)\n",
        "    bbox_t[N_range, lines, columns] = torch.stack((xc_rcell, yc_rcell, rw, rh, torch.ones(N))).permute(1,0)\n",
        "    \n",
        "    return bbox_t.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mHUOkfWPe-CW"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.0001\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "model_MNIST = YoloMNIST(sizeHW=75, S=6, C=10, B=1)\n",
        "model_MNIST = model_MNIST.to(device)\n",
        "optimizer = torch.optim.Adam(params=model_MNIST.parameters(), lr=learning_rate, weight_decay=0.0005)\n",
        "loss_yolo = YoloLoss(lambd_coord=50, lambd_noobj=0.5, S=6, sizeHW=75, device=device)\n",
        "\n",
        "# print(optimizer)\n",
        "#summary(model_MNIST, input_size = (BATCH_SIZE,1,75,75))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9tSO0Qo7e-CX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[START] : 2022-08-25 17:22:16 :\n",
            "[Training on] : CPU\n",
            "--------------------\n",
            "     2022-08-25 17:22:16 : EPOCH 1/3\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 30.31519\n",
            "xy_coord training loss for this batch : 0.02377\n",
            "wh_sizes training loss for this batch : 0.52171\n",
            "confidence with object training loss for this batch : 1.19763\n",
            "confidence without object training loss for this batch : 1.11863\n",
            "class proba training loss for this batch : 1.28424\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 1.95145\n",
            "xy_coord training loss for this batch : 0.00317\n",
            "wh_sizes training loss for this batch : 0.01059\n",
            "confidence with object training loss for this batch : 0.18185\n",
            "confidence without object training loss for this batch : 0.30622\n",
            "class proba training loss for this batch : 0.92844\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 1.33716\n",
            "xy_coord training loss for this batch : 0.00251\n",
            "wh_sizes training loss for this batch : 0.00184\n",
            "confidence with object training loss for this batch : 0.10853\n",
            "confidence without object training loss for this batch : 0.27892\n",
            "class proba training loss for this batch : 0.87168\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 1.29882\n",
            "xy_coord training loss for this batch : 0.00292\n",
            "wh_sizes training loss for this batch : 0.00184\n",
            "confidence with object training loss for this batch : 0.06777\n",
            "confidence without object training loss for this batch : 0.21929\n",
            "class proba training loss for this batch : 0.88348\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 1.23523\n",
            "xy_coord training loss for this batch : 0.00272\n",
            "wh_sizes training loss for this batch : 0.00121\n",
            "confidence with object training loss for this batch : 0.07468\n",
            "confidence without object training loss for this batch : 0.22033\n",
            "class proba training loss for this batch : 0.85374\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 1.11693\n",
            "xy_coord training loss for this batch : 0.00308\n",
            "wh_sizes training loss for this batch : 0.00066\n",
            "confidence with object training loss for this batch : 0.04909\n",
            "confidence without object training loss for this batch : 0.15718\n",
            "class proba training loss for this batch : 0.80236\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 1.07244\n",
            "xy_coord training loss for this batch : 0.00236\n",
            "wh_sizes training loss for this batch : 0.00089\n",
            "confidence with object training loss for this batch : 0.06653\n",
            "confidence without object training loss for this batch : 0.15744\n",
            "class proba training loss for this batch : 0.76463\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 1.17107\n",
            "xy_coord training loss for this batch : 0.00279\n",
            "wh_sizes training loss for this batch : 0.00079\n",
            "confidence with object training loss for this batch : 0.07842\n",
            "confidence without object training loss for this batch : 0.17769\n",
            "class proba training loss for this batch : 0.82482\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 1.08140\n",
            "xy_coord training loss for this batch : 0.00313\n",
            "wh_sizes training loss for this batch : 0.00095\n",
            "confidence with object training loss for this batch : 0.06848\n",
            "confidence without object training loss for this batch : 0.13676\n",
            "class proba training loss for this batch : 0.74059\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 1.01751\n",
            "xy_coord training loss for this batch : 0.00264\n",
            "wh_sizes training loss for this batch : 0.00094\n",
            "confidence with object training loss for this batch : 0.06350\n",
            "confidence without object training loss for this batch : 0.13252\n",
            "class proba training loss for this batch : 0.70857\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 1.04375\n",
            "xy_coord training loss for this batch : 0.00323\n",
            "wh_sizes training loss for this batch : 0.00103\n",
            "confidence with object training loss for this batch : 0.06926\n",
            "confidence without object training loss for this batch : 0.14780\n",
            "class proba training loss for this batch : 0.68777\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:15.564586\n",
            "Mean training loss for this epoch : 1.76513\n",
            "--------------------\n",
            "     2022-08-25 17:22:16 : EPOCH 2/3\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 1.04208\n",
            "xy_coord training loss for this batch : 0.00259\n",
            "wh_sizes training loss for this batch : 0.00078\n",
            "confidence with object training loss for this batch : 0.10057\n",
            "confidence without object training loss for this batch : 0.17448\n",
            "class proba training loss for this batch : 0.68566\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.94244\n",
            "xy_coord training loss for this batch : 0.00259\n",
            "wh_sizes training loss for this batch : 0.00092\n",
            "confidence with object training loss for this batch : 0.04699\n",
            "confidence without object training loss for this batch : 0.12268\n",
            "class proba training loss for this batch : 0.65823\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 1.05709\n",
            "xy_coord training loss for this batch : 0.00296\n",
            "wh_sizes training loss for this batch : 0.00074\n",
            "confidence with object training loss for this batch : 0.09038\n",
            "confidence without object training loss for this batch : 0.17772\n",
            "class proba training loss for this batch : 0.69256\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.95970\n",
            "xy_coord training loss for this batch : 0.00314\n",
            "wh_sizes training loss for this batch : 0.00083\n",
            "confidence with object training loss for this batch : 0.06417\n",
            "confidence without object training loss for this batch : 0.12537\n",
            "class proba training loss for this batch : 0.63421\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.96455\n",
            "xy_coord training loss for this batch : 0.00261\n",
            "wh_sizes training loss for this batch : 0.00092\n",
            "confidence with object training loss for this batch : 0.05781\n",
            "confidence without object training loss for this batch : 0.11248\n",
            "class proba training loss for this batch : 0.67405\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.90318\n",
            "xy_coord training loss for this batch : 0.00285\n",
            "wh_sizes training loss for this batch : 0.00104\n",
            "confidence with object training loss for this batch : 0.06744\n",
            "confidence without object training loss for this batch : 0.13016\n",
            "class proba training loss for this batch : 0.57620\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.76151\n",
            "xy_coord training loss for this batch : 0.00270\n",
            "wh_sizes training loss for this batch : 0.00083\n",
            "confidence with object training loss for this batch : 0.04084\n",
            "confidence without object training loss for this batch : 0.10225\n",
            "class proba training loss for this batch : 0.49328\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.76150\n",
            "xy_coord training loss for this batch : 0.00323\n",
            "wh_sizes training loss for this batch : 0.00082\n",
            "confidence with object training loss for this batch : 0.04597\n",
            "confidence without object training loss for this batch : 0.10982\n",
            "class proba training loss for this batch : 0.45810\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.75765\n",
            "xy_coord training loss for this batch : 0.00290\n",
            "wh_sizes training loss for this batch : 0.00088\n",
            "confidence with object training loss for this batch : 0.04861\n",
            "confidence without object training loss for this batch : 0.10479\n",
            "class proba training loss for this batch : 0.46751\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.84155\n",
            "xy_coord training loss for this batch : 0.00283\n",
            "wh_sizes training loss for this batch : 0.00085\n",
            "confidence with object training loss for this batch : 0.07630\n",
            "confidence without object training loss for this batch : 0.15257\n",
            "class proba training loss for this batch : 0.50507\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.75924\n",
            "xy_coord training loss for this batch : 0.00322\n",
            "wh_sizes training loss for this batch : 0.00090\n",
            "confidence with object training loss for this batch : 0.08898\n",
            "confidence without object training loss for this batch : 0.17630\n",
            "class proba training loss for this batch : 0.37644\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:17.135308\n",
            "Mean training loss for this epoch : 0.89089\n",
            "--------------------\n",
            "     2022-08-25 17:22:16 : EPOCH 3/3\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.72316\n",
            "xy_coord training loss for this batch : 0.00287\n",
            "wh_sizes training loss for this batch : 0.00069\n",
            "confidence with object training loss for this batch : 0.06064\n",
            "confidence without object training loss for this batch : 0.14020\n",
            "class proba training loss for this batch : 0.41416\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.69767\n",
            "xy_coord training loss for this batch : 0.00271\n",
            "wh_sizes training loss for this batch : 0.00084\n",
            "confidence with object training loss for this batch : 0.03856\n",
            "confidence without object training loss for this batch : 0.09345\n",
            "class proba training loss for this batch : 0.43497\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.70142\n",
            "xy_coord training loss for this batch : 0.00273\n",
            "wh_sizes training loss for this batch : 0.00080\n",
            "confidence with object training loss for this batch : 0.03967\n",
            "confidence without object training loss for this batch : 0.09882\n",
            "class proba training loss for this batch : 0.43611\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.66304\n",
            "xy_coord training loss for this batch : 0.00324\n",
            "wh_sizes training loss for this batch : 0.00077\n",
            "confidence with object training loss for this batch : 0.04046\n",
            "confidence without object training loss for this batch : 0.10554\n",
            "class proba training loss for this batch : 0.36896\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.67875\n",
            "xy_coord training loss for this batch : 0.00281\n",
            "wh_sizes training loss for this batch : 0.00070\n",
            "confidence with object training loss for this batch : 0.06540\n",
            "confidence without object training loss for this batch : 0.13917\n",
            "class proba training loss for this batch : 0.36870\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.62860\n",
            "xy_coord training loss for this batch : 0.00303\n",
            "wh_sizes training loss for this batch : 0.00105\n",
            "confidence with object training loss for this batch : 0.05116\n",
            "confidence without object training loss for this batch : 0.09345\n",
            "class proba training loss for this batch : 0.32670\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.65724\n",
            "xy_coord training loss for this batch : 0.00285\n",
            "wh_sizes training loss for this batch : 0.00073\n",
            "confidence with object training loss for this batch : 0.06068\n",
            "confidence without object training loss for this batch : 0.12286\n",
            "class proba training loss for this batch : 0.35609\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.62238\n",
            "xy_coord training loss for this batch : 0.00276\n",
            "wh_sizes training loss for this batch : 0.00080\n",
            "confidence with object training loss for this batch : 0.04410\n",
            "confidence without object training loss for this batch : 0.08863\n",
            "class proba training loss for this batch : 0.35580\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.53617\n",
            "xy_coord training loss for this batch : 0.00282\n",
            "wh_sizes training loss for this batch : 0.00077\n",
            "confidence with object training loss for this batch : 0.04185\n",
            "confidence without object training loss for this batch : 0.08701\n",
            "class proba training loss for this batch : 0.27154\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.52978\n",
            "xy_coord training loss for this batch : 0.00283\n",
            "wh_sizes training loss for this batch : 0.00074\n",
            "confidence with object training loss for this batch : 0.04878\n",
            "confidence without object training loss for this batch : 0.10224\n",
            "class proba training loss for this batch : 0.25122\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.50441\n",
            "xy_coord training loss for this batch : 0.00278\n",
            "wh_sizes training loss for this batch : 0.00055\n",
            "confidence with object training loss for this batch : 0.03727\n",
            "confidence without object training loss for this batch : 0.09163\n",
            "class proba training loss for this batch : 0.25473\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:16.743851\n",
            "Mean training loss for this epoch : 0.62585\n"
          ]
        }
      ],
      "source": [
        "delta_time = datetime.timedelta(hours=1)\n",
        "timezone = datetime.timezone(offset=delta_time)\n",
        "\n",
        "t = datetime.datetime.now(tz=timezone)\n",
        "str_t = '{:%Y-%m-%d %H:%M:%S}'.format(t)\n",
        "print(f\"[START] : {str_t} :\")\n",
        "print(f\"[Training on] : {str(device).upper()}\")\n",
        "\n",
        "EPOCHS = 3\n",
        "size_grid = 6\n",
        "batch_loss_list = []\n",
        "model_MNIST = model_MNIST.to(device)\n",
        "\n",
        "for epoch in range(EPOCHS) : \n",
        "    begin_time = timer()\n",
        "    epochs_loss = 0.\n",
        "    \n",
        "    print(\"-\"*20)\n",
        "    str_t = '{:%Y-%m-%d %H:%M:%S}'.format(t)\n",
        "    print(\" \"*5 + f\"{str_t} : EPOCH {epoch+1}/{EPOCHS}\")\n",
        "    print(\"-\"*20)\n",
        "\n",
        "    model_MNIST.train()\n",
        "    for batch, (img, labels, bbox_true) in enumerate(training_dataset):\n",
        "        loss = 0\n",
        "        begin_batch_time = timer()\n",
        "        img, labels, bbox_true = img.to(device), labels.to(device), bbox_true.to(device)\n",
        "        \n",
        "        ### turn bbox into NxSxSx5 tensor\n",
        "        bbox_true_6x6 = bbox2Tensor(bbox_true).to(device)\n",
        "        \n",
        "        ### clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        ### compute predictions\n",
        "        bbox_preds, label_preds = model_MNIST(img)\n",
        "        \n",
        "        ### compute losses over each grid cell for each image in the batch\n",
        "        losses, loss = loss_yolo(bbox_preds, bbox_true_6x6, label_preds, labels)\n",
        "    \n",
        "        ### compute gradients\n",
        "        loss.backward()\n",
        "        \n",
        "        ### Weight updates\n",
        "        optimizer.step()\n",
        "        \n",
        "        ######### print part #######################\n",
        "        current_loss = loss.item()\n",
        "        batch_loss_list.append(current_loss)\n",
        "        epochs_loss = epochs_loss + current_loss\n",
        "\n",
        "        if batch+1 <= len_training_ds//BATCH_SIZE:\n",
        "            current_training_sample = (batch+1)*BATCH_SIZE\n",
        "        else:\n",
        "            current_training_sample = (batch)*BATCH_SIZE + len_training_ds%BATCH_SIZE\n",
        "        \n",
        "        if (batch) == 0 or (batch+1)%100 == 0 or batch == len_training_ds//BATCH_SIZE:\n",
        "            print(f\" --- Image : {current_training_sample}/{len_training_ds}\",\\\n",
        "                    f\" : loss = {current_loss:.5f}\")\n",
        "            print(f\"xy_coord training loss for this batch : {torch.sum(losses['loss_xy']) / len(img):.5f}\")\n",
        "            print(f\"wh_sizes training loss for this batch : {torch.sum(losses['loss_wh']) / len(img):.5f}\")\n",
        "            print(f\"confidence with object training loss for this batch : {torch.sum(losses['loss_conf_obj']) / len(img):.5f}\")\n",
        "            print(f\"confidence without object training loss for this batch : {torch.sum(losses['loss_conf_noobj']) / len(img):.5f}\")\n",
        "            print(f\"class proba training loss for this batch : {torch.sum(losses['loss_class']) / len(img):.5f}\")\n",
        "            print('\\n')\n",
        "            if batch == (len_training_ds//BATCH_SIZE):\n",
        "                print(f\"Total elapsed time for training : {datetime.timedelta(seconds=timer()-begin_time)}\")\n",
        "                print(f\"Mean training loss for this epoch : {epochs_loss / len(training_dataset):.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model_MNIST.state_dict(), \"yolo_mnist_model_3epochs.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "OWc1Cr8yrSH8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE BOX :  tensor(0.2443)\n",
            "class acc :  tensor(93.0700) %\n"
          ]
        }
      ],
      "source": [
        "for (img, labels, bbox_true) in validation_dataset:\n",
        "    img, labels, bbox_true = img.to(device), labels.to(device), bbox_true.to(device)\n",
        "    model_MNIST.eval()\n",
        "    with torch.no_grad():\n",
        "        ### prediction\n",
        "        bbox_preds, label_preds = model_MNIST(img)\n",
        "\n",
        "        ### (N,4) -> (N, S, S, 5)\n",
        "        bbox_true_6x6 = bbox2Tensor(bbox_true).to(device)\n",
        "\n",
        "        ### keeping only cells (i,j) with an object \n",
        "        cells_with_obj = bbox_true_6x6.nonzero()[::5]\n",
        "        N, cells_i, cells_j, _ = cells_with_obj.permute(1,0)\n",
        "\n",
        "        ### MSE along bbox coordinates and sizes in the cells containing an object\n",
        "        mse_box = (1/len(img)) * torch.sum(torch.pow(bbox_true - bbox_preds[N, cells_i, cells_j,:4],2))\n",
        "\n",
        "        ### applied softmax to class predictions and compute accuracy\n",
        "        softmax_pred_classes = torch.softmax(label_preds[N, cells_i, cells_j], dim=1)\n",
        "        classes_acc = (1/len(img)) * torch.sum(torch.argmax(labels, dim=1) == torch.argmax(softmax_pred_classes, dim=1))\n",
        "\n",
        "print(\"MSE BOX : \", mse_box)\n",
        "print(\"class acc : \", classes_acc*100,\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10000, 6, 6, 5])"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bbox_true_6x6.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "YOLO_MNIST_Localization.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "bfc2e043f4a41bbce066f7921047bdcfe8769546c7198e4df6482fd90de274f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
