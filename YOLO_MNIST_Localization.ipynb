{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThOpaque/Food_Recognition/blob/main/YOLO_MNIST_Localization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAHeh1BRufX4",
        "outputId": "82ec3f5c-fa85-469e-fd40-36aa5f2c62f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.9.3\n"
          ]
        }
      ],
      "source": [
        "import os, time, datetime\n",
        "from timeit import default_timer as timer\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "%pip install torchinfo;\n",
        "%pip install torchmetrics;\n",
        "from torchmetrics import MeanSquaredError;\n",
        "from torchinfo import summary;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOraK1TX7XZB",
        "outputId": "32c1b5ed-fdce-43de-ddb0-1f48eb60a3ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------------------------\n",
            "Execute notebook on - cpu -\n",
            "------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Choosing device between CPU or GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "elif torch.has_mps:\n",
        "    device=torch.device('mps')\n",
        "else:\n",
        "    device=torch.device('cpu')\n",
        "    \n",
        "print(\"\\n------------------------------------\")\n",
        "print(f\"Execute notebook on - {device} -\")\n",
        "print(\"------------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "M7VztqFE71JZ"
      },
      "outputs": [],
      "source": [
        "class my_mnist_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root:str, split:str=None, download:bool=False, S=6, sizeHW=75):\n",
        "        assert split, \"You have to specify the split.\"\n",
        "        \n",
        "        if split == \"train\":\n",
        "            train = True\n",
        "        elif split == \"test\":\n",
        "            train = False\n",
        "        \n",
        "        self.dataset = torchvision.datasets.MNIST(root=root, train=train, download=download)\n",
        "        \n",
        "        self.cell_size = sizeHW / S\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def _numpy_pad_to_bounding_box(self, image, offset_height=0, offset_width=0, target_height=0, target_width=0):\n",
        "        assert image.shape[:-1][0] <= target_height-offset_height, \"height must be <= target - offset\"\n",
        "        assert image.shape[:-1][1] <= target_width-offset_width, \"width must be <= target - offset\"\n",
        "        \n",
        "        target_array = np.zeros((target_height, target_width, image.shape[-1]))\n",
        "\n",
        "        for k in range(image.shape[0]):\n",
        "            target_array[offset_height+k][offset_width:image.shape[1]+offset_width] = image[k]\n",
        "        \n",
        "        return target_array\n",
        "\n",
        "    def _transform_pasting75(self, image, label):\n",
        "        ### xmin, ymin of digit\n",
        "        xmin = torch.randint(0, 48, (1,))\n",
        "        ymin = torch.randint(0, 48, (1,))\n",
        "        \n",
        "        image = torchvision.transforms.ToTensor()(image)\n",
        "        image = torch.reshape(image, (28,28,1,))\n",
        "        image = torch.from_numpy(self._numpy_pad_to_bounding_box(image, ymin, xmin, 75, 75))\n",
        "        image = image.permute(2, 0, 1) #(C,H,W)\n",
        "        image = image.to(torch.float)\n",
        "        \n",
        "        xmin, ymin = xmin.to(torch.float), ymin.to(torch.float)\n",
        "\n",
        "        xmax_bbox, ymax_bbox = (xmin + 28), (ymin + 28)\n",
        "        xmin_bbox, ymin_bbox = xmin, ymin\n",
        "        w_bbox = xmax_bbox-xmin_bbox\n",
        "        h_bbox = ymax_bbox-ymin_bbox\n",
        "\n",
        "        rw = w_bbox / 75\n",
        "        rh = h_bbox / 75\n",
        "        cx = (xmin + (w_bbox/2))/75\n",
        "        cy = (ymin + (h_bbox/2))/75\n",
        "\n",
        "        cx_rcell = cx % self.cell_size / self.cell_size\n",
        "        cy_rcell = cy % self.cell_size / self.cell_size\n",
        "\n",
        "\n",
        "        label_one_hot = F.one_hot(torch.as_tensor(label, dtype=torch.int64), 10)\n",
        "        bbox_coord = torch.Tensor([cx_rcell, cy_rcell, rw, rh])\n",
        "\n",
        "        return image, label_one_hot, bbox_coord\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        image, one_hot_label, bbox_coord = self._transform_pasting75(self.dataset[idx][0], self.dataset[idx][1])\n",
        "        \n",
        "        return image, one_hot_label.to(torch.float), bbox_coord\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yQNznLfO8jOx",
        "outputId": "51320c7b-27c9-46ea-9491-6f8a14fc17a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423,
          "referenced_widgets": [
            "a9c9fb5cfe5b401d86845703a998f6db",
            "6aba0ea5bfdf4f14a8c518ff3d9a71f5",
            "2bc7b2045344477786066f7f2608fdb3",
            "307c3ca345474ff7b4f0f9fb3a9d807f",
            "8916b530b9a845b98eb2d9c20caed78e",
            "9208ff95819b4239a4f95946ff74133b",
            "6b4f8fd39d0a4ee4ac61c69782bf188c",
            "a709533f2f9b40738441a55a6c6b7c97",
            "5683d25400ba49568b5fe1d064de61f6",
            "9f505185d1274002a36479074afdba33",
            "00636579a0ec4bf593858001d88d6433",
            "43eab58b65014e8f99743e3418d66142",
            "9c4394133c644462b01f881969ffca90",
            "6d0eb0f2ad6d48f198e010b2131449c5",
            "60ca8581c0784d118955e2243e7bf07d",
            "2612c008838f4d04ac954a9757ee2ff4",
            "5ad1d80c567e40bb84d63b82f80ba3c0",
            "79d28217e64d4437ba797af011bf8876",
            "8ccdcd2be1de4c2cbeca443df198f9f1",
            "4f6945eee0284efcb07275ec0200768c",
            "b9656c9e32c04fc88a22707afe80751d",
            "80295e93550f426499dc157e2a1191e4",
            "3f5e919103fb4395af74f6d45a099609",
            "77298f5e1af84c62bb652e42f54cf9ac",
            "a17a84f17723487fa5a91300a8e7c9c9",
            "4c6d6b6b59324cc4937833ff1d3bc87e",
            "840d28bff63440949877e251461fee19",
            "0fd26fe686e349e9b93ae78429c9134f",
            "d627a7fa5be045079b232b7d28511916",
            "84f46fccb72c4e938867304d1520b697",
            "5d9e90b4e3844f2687538b4e2959ed24",
            "70267f10866f4c618801e0107ed9f63d",
            "58f79da6016e4aa898cf16c8e0199801",
            "18a8f093259a4e0ca40d5774e79daef5",
            "0fed3cb6c89d477699865cf55eff0c90",
            "f991d13ade0a4db0b0d8f5c6a4d06cf3",
            "0ec81182a61d411f830be71719af3272",
            "c12a275fbaff4d90a5740b9d3e21eaf6",
            "6b62c2bd9be5445481ccd8d2c361654a",
            "4c3d9324a7c945ababe6aec06c94386e",
            "2307ed6b07414d7e943c925db0586990",
            "8a798ceb39614164acafab5379d93118",
            "e71976373e374c83a95018bb38eabe91",
            "c19b7809f56a4316bf0445469e18cb2f"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/9912422 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9c9fb5cfe5b401d86845703a998f6db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/28881 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43eab58b65014e8f99743e3418d66142"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1648877 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f5e919103fb4395af74f6d45a099609"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4542 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "18a8f093259a4e0ca40d5774e79daef5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def get_training_dataset(BATCH_SIZE=64):\n",
        "    \"\"\"\n",
        "    Loads and maps the training split of the dataset using the custom dataset class. \n",
        "    \"\"\"\n",
        "    dataset = my_mnist_dataset(root=\"data\", split=\"train\", download=True)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    return dataloader, len(dataset)\n",
        "\n",
        "def get_validation_dataset(BATCH_SIZE = None):\n",
        "    \"\"\"\n",
        "    Loads and maps the validation split of the datasetusing the custom dataset class. \n",
        "    \"\"\"\n",
        "    dataset = my_mnist_dataset(root=\"data\", split=\"test\", download=True)\n",
        "    if BATCH_SIZE is None:\n",
        "        BATCH_SIZE = len(dataset)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    return dataloader, len(dataset)\n",
        "\n",
        "# instantiate the datasets\n",
        "training_dataset, len_training_ds = get_training_dataset()\n",
        "validation_dataset, len_validation_ds = get_validation_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfgUx1srt90c"
      },
      "outputs": [],
      "source": [
        "class CNNBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(CNNBlock, self).__init__()\n",
        "        self.conv = torch.nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "        self.bn = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.l_relu = torch.nn.LeakyReLU(0.1)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        x = self.conv(input)\n",
        "        x = self.bn(x)\n",
        "        return self.l_relu(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VjCkTEce-CT"
      },
      "outputs": [],
      "source": [
        "class YoloMNIST(torch.nn.Module):\n",
        "    def __init__(self, sizeHW, S, C, B):\n",
        "        super(YoloMNIST, self).__init__()\n",
        "        self.S, self.C, self.B = S, C, B\n",
        "        self.sizeHW = sizeHW\n",
        "        self.cell_size = self.sizeHW / self.S\n",
        "\n",
        "        self.seq = torch.nn.Sequential()        \n",
        "        self.seq.add_module(f\"conv_1\", CNNBlock(1, 32, stride=2, kernel_size=7, padding=2))\n",
        "        self.seq.add_module(f\"maxpool_1\", torch.nn.MaxPool2d(2))\n",
        "        self.seq.add_module(f\"conv_3\", CNNBlock(32, 128, stride=1, kernel_size=3, padding=0))\n",
        "        self.seq.add_module(f\"maxpool_2\", torch.nn.MaxPool2d(2))\n",
        "        self.seq.add_module(f\"conv_5\", CNNBlock(128, 64, stride=1, kernel_size=1, padding=0))\n",
        "        self.seq.add_module(f\"conv_4\", CNNBlock(64, 128, stride=1, kernel_size=3, padding=0))\n",
        "        self.seq.add_module(f\"conv_6\", CNNBlock(128, 128, stride=1, kernel_size=3, padding=1))\n",
        "        \n",
        "        self.fcs = self._create_fcs()\n",
        "\n",
        "    def _size_output(self, sizeHW:int, kernel:int, stride:int, padding:int=0, isMaxPool:bool=False)->int:\n",
        "        \"\"\"\n",
        "        Output size (width/height) of convolutional or maxpool layers.\n",
        "\n",
        "        Args:\n",
        "            sizeHW : int\n",
        "                Image size (we suppose this is a square image)\n",
        "            kernel : int\n",
        "                Size of a square kernel\n",
        "            stride : int\n",
        "                Stride of convolution layer\n",
        "            padding : int\n",
        "                Padding of convolution layer\n",
        "            isMaxPool : Bool, default is False.\n",
        "                Specify if it is a Maxpool layer (True) or not (False). \n",
        "\n",
        "        Return:\n",
        "            output_size : int\n",
        "                Image output size after a convolutional or MaxPool layer.\n",
        "        \"\"\" \n",
        "        if isMaxPool == True:\n",
        "            output_size = int(sizeHW/2)\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "        if padding == 'same':\n",
        "            output_size = sizeHW\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "        else:\n",
        "            output_size = (sizeHW + 2 * padding - (kernel-1)-1)/stride\n",
        "            output_size = int(output_size + 1)\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "\n",
        "    def _create_fcs(self):\n",
        "        output = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(128 * self.S * self.S, 4096),\n",
        "            torch.nn.LeakyReLU(0.1),\n",
        "            torch.nn.Linear(4096, self.S * self.S * (self.C + self.B*5))\n",
        "        )\n",
        "        return output\n",
        "    \n",
        "\n",
        "    def forward(self, input:torch.Tensor)->tuple:\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            input : torch.Tensor of shape (N, C, H, W)\n",
        "                Batch of images.\n",
        "\n",
        "        Return:\n",
        "            box_coord : torch.Tensor of shape (N, 6, 6, 5)\n",
        "                Contains xc_rcell, yc_rcell, rw, rh and the confidence number c\n",
        "                over 6x6 grid cells.\n",
        "            classifier : torch.Tensor of shape (N, 6, 6, 10)\n",
        "                Contains the one-hot encoding of each digit number over\n",
        "                6x6 grid cells.\n",
        "        \"\"\"     \n",
        "        x = self.seq(input)\n",
        "        x = self.fcs(x)\n",
        "        x = x.view(x.size(0), self.S, self.S, self.B * 5 + self.C)\n",
        "        box_coord = x[:,:,:,0:5]\n",
        "        classifier = x[:,:,:,5:]\n",
        "        return box_coord, classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_jOg_i_p2_c"
      },
      "outputs": [],
      "source": [
        "class YoloLoss(torch.nn.Module):\n",
        "    def __init__(self, lambd_coord:int, lambd_noobj:float, device:torch.device, S:int=6):\n",
        "        super(YoloLoss, self).__init__()\n",
        "        self.LAMBD_COORD = lambd_coord\n",
        "        self.LAMBD_NOOBJ = lambd_noobj\n",
        "        self.S = S\n",
        "        self.device = device\n",
        "\n",
        "    def _coordloss(self, pred_coord_rcell, true_coord_rcell):\n",
        "        \"\"\"\n",
        "        Args : \n",
        "            pred_coord_rcell : torch.Tensor of shape (N, 2)\n",
        "            true_coord_rcell : torch.Tensor of shape (N, 2)\n",
        "        Returns :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        xc_hat, yc_hat = pred_coord_rcell.permute(1,0)\n",
        "        xc, yc = true_coord_rcell.permute(1,0)\n",
        "\n",
        "        squared_error = torch.pow(xc - xc_hat,2) + torch.pow(yc - yc_hat,2)\n",
        "        return squared_error\n",
        "\n",
        "    def _sizeloss(self, pred_size, true_size):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_size : torch.Tensor of shape (N, 2)\n",
        "            true_size : torch.Tensor of shape (N, 2)\n",
        "        Returns : \n",
        "            root_squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        rw_hat, rh_hat = pred_size.permute(1,0)\n",
        "        rw, rh = true_size.permute(1,0)\n",
        "\n",
        "        #sizes can't be negative\n",
        "        rw_hat = rw_hat.clip(min=0)\n",
        "        rh_hat = rh_hat.clip(min=0)\n",
        "\n",
        "        root_squared_error_w = torch.pow(torch.sqrt(rw) - torch.sqrt(rw_hat),2)\n",
        "        root_squared_error_h = torch.pow(torch.sqrt(rh) - torch.sqrt(rh_hat),2)\n",
        "        root_squared_error = root_squared_error_w + root_squared_error_h\n",
        "        return root_squared_error\n",
        "\n",
        "    def _confidenceloss(self, pred_c, true_c):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_c : torch.Tensor of shape (N)\n",
        "            true_c : torch.Tensor of shape (N)\n",
        "        Return :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        squared_error = torch.pow(true_c - pred_c, 2)\n",
        "        return squared_error\n",
        "\n",
        "    def _classloss(self, pred_class, true_class):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_class : torch.Tensor of shape (N, 10)\n",
        "            true_class : torch.Tensor of shape (N, 10)\n",
        "        Returns :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        squared_error = torch.pow(true_class - pred_class, 2)\n",
        "        return torch.sum(squared_error, dim=1)\n",
        "\n",
        "    def forward(self, pred_box:torch.Tensor, true_box:torch.Tensor, pred_class:torch.Tensor, true_class:torch.Tensor):\n",
        "        \"\"\"\n",
        "        Grid forward pass.\n",
        "\n",
        "        Args:\n",
        "            pred_box : torch.Tensor of shape (N, S, S, 5)\n",
        "                Batch predicted outputs containing xc_rcell, yc_rcell, rw, rh,\n",
        "                and confident number c for each grid cell.\n",
        "            true_box : torch.Tensor of shape (N, S, S, 5)\n",
        "                Groundtrue batch containing bbox values for each cell and\n",
        "                c indicate if there is an object to detect or not (1/0).\n",
        "            pred_class : torch.Tensor of shape (N, S, S, 10)\n",
        "                Probability of each digit class in each grid cell\n",
        "            true_class : torch.Tensor of shape (N, 10)\n",
        "                one-hot vect of each digit\n",
        "\n",
        "        Return:\n",
        "            loss : float\n",
        "                The batch loss value of the grid\n",
        "        \"\"\"\n",
        "        BATCH_SIZE = len(pred_box)\n",
        "\n",
        "        ### Initialization of the losses\n",
        "        losses_list = ['loss_xy', 'loss_wh', 'loss_conf_obj', 'loss_conf_noobj', 'loss_class']\n",
        "        losses = {key : torch.zeros(BATCH_SIZE).to(self.device) for key in losses_list}\n",
        "        \n",
        "        ### Compute the losses for all images in the batch\n",
        "        for i in range(self.S):\n",
        "            for j in range(self.S):\n",
        "                ### Intersection over Union\n",
        "                IoU = self._intersection_over_union(pred_box[:,i,j], true_box[:,i,j])\n",
        "\n",
        "                ### bbox coordinates\n",
        "                xy_hat = pred_box[:,i,j,:2]\n",
        "                xy = true_box[:,i,j,:2]\n",
        "                wh_hat = pred_box[:,i,j,2:4]\n",
        "                wh = true_box[:,i,j,2:4]\n",
        "                \n",
        "                ### confidence numbers\n",
        "                pred_c = pred_box[:,i,j,4]# * IoU\n",
        "                true_c = true_box[:,i,j,4]\n",
        "\n",
        "                ### objects to detect\n",
        "                isObject = true_c.to(torch.bool)\n",
        "                isNoObject = torch.logical_not(true_c) #(~bool) doesn't work on MPS device\n",
        "\n",
        "                ### sum the losses over the grid\n",
        "                losses['loss_xy'] += isObject * self._coordloss(xy_hat, xy)\n",
        "                losses['loss_wh'] += isObject * self._sizeloss(wh_hat, wh)\n",
        "                losses['loss_conf_obj'] += isObject * self._confidenceloss(pred_c, true_c)\n",
        "                losses['loss_conf_noobj'] += isNoObject * self._confidenceloss(pred_c, true_c)\n",
        "                losses['loss_class'] += isObject * self._classloss(pred_class[:,i,j], true_class)\n",
        "\n",
        "        ### Yolo_v1 loss over the batch, shape : (BATCH_SIZE)\n",
        "        loss = self.LAMBD_COORD * losses['loss_xy'] \\\n",
        "                + self.LAMBD_COORD * losses['loss_wh'] \\\n",
        "                + losses['loss_conf_obj'] \\\n",
        "                + self.LAMBD_NOOBJ * losses['loss_conf_noobj'] \\\n",
        "                + losses['loss_class']\n",
        "        \n",
        "        loss = torch.sum(loss) / BATCH_SIZE\n",
        "\n",
        "        return losses, loss\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdIlkN61mXJj"
      },
      "outputs": [],
      "source": [
        "def bbox2Tensor(bbox:torch.Tensor, S:int=6, sizeHW:int=75, device=torch.device('cpu'))->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Constructs en Tensor and puts bbox values in the corresponding i,j grid cell.\n",
        "\n",
        "    Args :\n",
        "        bbox : torch.Tensor of shape (N,4)\n",
        "            Contains bbox values xc_rcell, yc_rcell, rw and rh.\n",
        "        S : int, default is 6\n",
        "            Size of the grid.\n",
        "        sizeHW : int, default is 75\n",
        "            Size of the image.\n",
        "\n",
        "    Return :\n",
        "        bbox_t : torch.Tensor of shape (N, S, S, 5)\n",
        "            Tensor containing all 4 bbox values in the corresponding i,j grid\n",
        "            cell position i.e. in the i,j position where an object should be\n",
        "            detected.\n",
        "    \"\"\"\n",
        "    N = len(bbox)\n",
        "    bbox_t = torch.zeros(N,S,S,5).to(device)\n",
        "    cell_size = sizeHW/S\n",
        "\n",
        "    xc_rcell, yc_rcell, rw, rh = bbox.permute(1,0).to(device)\n",
        "    xc = xc_rcell * cell_size - (1/cell_size) * (xc_rcell/cell_size).to(torch.int32)\n",
        "    yc = yc_rcell * cell_size - (1/cell_size) * (yc_rcell/cell_size).to(torch.int32)\n",
        "\n",
        "    N_range = torch.arange(N)\n",
        "    lines = (yc * S).to(torch.long)\n",
        "    columns = (xc * S).to(torch.long)\n",
        "    bbox_t[N_range, lines, columns] = torch.stack((xc_rcell, yc_rcell, rw, rh, torch.ones(N))).permute(1,0)\n",
        "    \n",
        "    return bbox_t.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9QFEV2oweFN"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHUOkfWPe-CW"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.0001\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "model_MNIST = YoloMNIST(sizeHW=75, S=6, C=10, B=1)\n",
        "model_MNIST = model_MNIST.to(device)\n",
        "optimizer = torch.optim.Adam(params=model_MNIST.parameters(), lr=learning_rate, weight_decay=0.0005)\n",
        "loss_yolo = YoloLoss(lambd_coord=5, lambd_noobj=0.5, S=6, sizeHW=75, device=device)\n",
        "\n",
        "# print(optimizer)\n",
        "#summary(model_MNIST, input_size = (BATCH_SIZE,1,75,75))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tSO0Qo7e-CX",
        "outputId": "fe3724f9-01f5-4db2-9842-4de2f5637f76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[START] : 2022-08-25 18:50:21 :\n",
            "[Training on] : CPU\n",
            "--------------------\n",
            "     2022-08-25 18:50:21 : EPOCH 1/3\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 5.51166\n",
            "xy_coord training loss for this batch : 0.05202\n",
            "wh_sizes training loss for this batch : 0.45522\n",
            "confidence with object training loss for this batch : 1.20043\n",
            "confidence without object training loss for this batch : 1.12338\n",
            "class proba training loss for this batch : 1.21335\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 1.06861\n",
            "xy_coord training loss for this batch : 0.00255\n",
            "wh_sizes training loss for this batch : 0.00385\n",
            "confidence with object training loss for this batch : 0.07596\n",
            "confidence without object training loss for this batch : 0.21245\n",
            "class proba training loss for this batch : 0.85442\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 1.01707\n",
            "xy_coord training loss for this batch : 0.00245\n",
            "wh_sizes training loss for this batch : 0.00243\n",
            "confidence with object training loss for this batch : 0.05722\n",
            "confidence without object training loss for this batch : 0.16733\n",
            "class proba training loss for this batch : 0.85180\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.84756\n",
            "xy_coord training loss for this batch : 0.00233\n",
            "wh_sizes training loss for this batch : 0.00201\n",
            "confidence with object training loss for this batch : 0.05992\n",
            "confidence without object training loss for this batch : 0.18375\n",
            "class proba training loss for this batch : 0.67407\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.70653\n",
            "xy_coord training loss for this batch : 0.00323\n",
            "wh_sizes training loss for this batch : 0.00167\n",
            "confidence with object training loss for this batch : 0.06483\n",
            "confidence without object training loss for this batch : 0.14680\n",
            "class proba training loss for this batch : 0.54377\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.65482\n",
            "xy_coord training loss for this batch : 0.00229\n",
            "wh_sizes training loss for this batch : 0.00208\n",
            "confidence with object training loss for this batch : 0.05109\n",
            "confidence without object training loss for this batch : 0.14109\n",
            "class proba training loss for this batch : 0.51135\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.62638\n",
            "xy_coord training loss for this batch : 0.00181\n",
            "wh_sizes training loss for this batch : 0.00154\n",
            "confidence with object training loss for this batch : 0.05868\n",
            "confidence without object training loss for this batch : 0.16164\n",
            "class proba training loss for this batch : 0.47011\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.48214\n",
            "xy_coord training loss for this batch : 0.00206\n",
            "wh_sizes training loss for this batch : 0.00126\n",
            "confidence with object training loss for this batch : 0.05565\n",
            "confidence without object training loss for this batch : 0.14310\n",
            "class proba training loss for this batch : 0.33835\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.48896\n",
            "xy_coord training loss for this batch : 0.00205\n",
            "wh_sizes training loss for this batch : 0.00118\n",
            "confidence with object training loss for this batch : 0.05860\n",
            "confidence without object training loss for this batch : 0.14183\n",
            "class proba training loss for this batch : 0.34327\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.42017\n",
            "xy_coord training loss for this batch : 0.00167\n",
            "wh_sizes training loss for this batch : 0.00157\n",
            "confidence with object training loss for this batch : 0.05702\n",
            "confidence without object training loss for this batch : 0.12059\n",
            "class proba training loss for this batch : 0.28670\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.37054\n",
            "xy_coord training loss for this batch : 0.00188\n",
            "wh_sizes training loss for this batch : 0.00173\n",
            "confidence with object training loss for this batch : 0.04680\n",
            "confidence without object training loss for this batch : 0.10384\n",
            "class proba training loss for this batch : 0.25378\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:19.876407\n",
            "Mean training loss for this epoch : 0.77085\n",
            "--------------------\n",
            "     2022-08-25 18:50:21 : EPOCH 2/3\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.35636\n",
            "xy_coord training loss for this batch : 0.00159\n",
            "wh_sizes training loss for this batch : 0.00145\n",
            "confidence with object training loss for this batch : 0.03554\n",
            "confidence without object training loss for this batch : 0.10185\n",
            "class proba training loss for this batch : 0.25470\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.35975\n",
            "xy_coord training loss for this batch : 0.00175\n",
            "wh_sizes training loss for this batch : 0.00101\n",
            "confidence with object training loss for this batch : 0.03837\n",
            "confidence without object training loss for this batch : 0.08729\n",
            "class proba training loss for this batch : 0.26397\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.32352\n",
            "xy_coord training loss for this batch : 0.00131\n",
            "wh_sizes training loss for this batch : 0.00111\n",
            "confidence with object training loss for this batch : 0.05082\n",
            "confidence without object training loss for this batch : 0.10483\n",
            "class proba training loss for this batch : 0.20820\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.33323\n",
            "xy_coord training loss for this batch : 0.00145\n",
            "wh_sizes training loss for this batch : 0.00081\n",
            "confidence with object training loss for this batch : 0.05426\n",
            "confidence without object training loss for this batch : 0.11901\n",
            "class proba training loss for this batch : 0.20820\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.29494\n",
            "xy_coord training loss for this batch : 0.00111\n",
            "wh_sizes training loss for this batch : 0.00083\n",
            "confidence with object training loss for this batch : 0.02809\n",
            "confidence without object training loss for this batch : 0.08617\n",
            "class proba training loss for this batch : 0.21407\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.23544\n",
            "xy_coord training loss for this batch : 0.00082\n",
            "wh_sizes training loss for this batch : 0.00087\n",
            "confidence with object training loss for this batch : 0.04439\n",
            "confidence without object training loss for this batch : 0.09621\n",
            "class proba training loss for this batch : 0.13446\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.26659\n",
            "xy_coord training loss for this batch : 0.00065\n",
            "wh_sizes training loss for this batch : 0.00088\n",
            "confidence with object training loss for this batch : 0.03848\n",
            "confidence without object training loss for this batch : 0.08570\n",
            "class proba training loss for this batch : 0.17762\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.29695\n",
            "xy_coord training loss for this batch : 0.00099\n",
            "wh_sizes training loss for this batch : 0.00070\n",
            "confidence with object training loss for this batch : 0.06111\n",
            "confidence without object training loss for this batch : 0.11112\n",
            "class proba training loss for this batch : 0.17185\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.26951\n",
            "xy_coord training loss for this batch : 0.00101\n",
            "wh_sizes training loss for this batch : 0.00071\n",
            "confidence with object training loss for this batch : 0.05147\n",
            "confidence without object training loss for this batch : 0.11809\n",
            "class proba training loss for this batch : 0.15038\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.24018\n",
            "xy_coord training loss for this batch : 0.00074\n",
            "wh_sizes training loss for this batch : 0.00070\n",
            "confidence with object training loss for this batch : 0.03943\n",
            "confidence without object training loss for this batch : 0.08889\n",
            "class proba training loss for this batch : 0.14911\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.26007\n",
            "xy_coord training loss for this batch : 0.00084\n",
            "wh_sizes training loss for this batch : 0.00064\n",
            "confidence with object training loss for this batch : 0.05849\n",
            "confidence without object training loss for this batch : 0.09615\n",
            "class proba training loss for this batch : 0.14606\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:17.054675\n",
            "Mean training loss for this epoch : 0.30563\n",
            "--------------------\n",
            "     2022-08-25 18:50:21 : EPOCH 3/3\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.21165\n",
            "xy_coord training loss for this batch : 0.00073\n",
            "wh_sizes training loss for this batch : 0.00060\n",
            "confidence with object training loss for this batch : 0.03174\n",
            "confidence without object training loss for this batch : 0.08035\n",
            "class proba training loss for this batch : 0.13307\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.20642\n",
            "xy_coord training loss for this batch : 0.00066\n",
            "wh_sizes training loss for this batch : 0.00050\n",
            "confidence with object training loss for this batch : 0.03324\n",
            "confidence without object training loss for this batch : 0.06504\n",
            "class proba training loss for this batch : 0.13486\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.19127\n",
            "xy_coord training loss for this batch : 0.00059\n",
            "wh_sizes training loss for this batch : 0.00063\n",
            "confidence with object training loss for this batch : 0.03404\n",
            "confidence without object training loss for this batch : 0.07022\n",
            "class proba training loss for this batch : 0.11604\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.25650\n",
            "xy_coord training loss for this batch : 0.00064\n",
            "wh_sizes training loss for this batch : 0.00054\n",
            "confidence with object training loss for this batch : 0.03949\n",
            "confidence without object training loss for this batch : 0.08927\n",
            "class proba training loss for this batch : 0.16644\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.22176\n",
            "xy_coord training loss for this batch : 0.00066\n",
            "wh_sizes training loss for this batch : 0.00040\n",
            "confidence with object training loss for this batch : 0.03411\n",
            "confidence without object training loss for this batch : 0.06778\n",
            "class proba training loss for this batch : 0.14848\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.19102\n",
            "xy_coord training loss for this batch : 0.00046\n",
            "wh_sizes training loss for this batch : 0.00046\n",
            "confidence with object training loss for this batch : 0.03147\n",
            "confidence without object training loss for this batch : 0.06129\n",
            "class proba training loss for this batch : 0.12432\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.19446\n",
            "xy_coord training loss for this batch : 0.00054\n",
            "wh_sizes training loss for this batch : 0.00059\n",
            "confidence with object training loss for this batch : 0.04341\n",
            "confidence without object training loss for this batch : 0.06797\n",
            "class proba training loss for this batch : 0.11141\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.20157\n",
            "xy_coord training loss for this batch : 0.00033\n",
            "wh_sizes training loss for this batch : 0.00052\n",
            "confidence with object training loss for this batch : 0.04034\n",
            "confidence without object training loss for this batch : 0.07865\n",
            "class proba training loss for this batch : 0.11768\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.18245\n",
            "xy_coord training loss for this batch : 0.00039\n",
            "wh_sizes training loss for this batch : 0.00058\n",
            "confidence with object training loss for this batch : 0.03089\n",
            "confidence without object training loss for this batch : 0.07045\n",
            "class proba training loss for this batch : 0.11152\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.21350\n",
            "xy_coord training loss for this batch : 0.00036\n",
            "wh_sizes training loss for this batch : 0.00047\n",
            "confidence with object training loss for this batch : 0.03894\n",
            "confidence without object training loss for this batch : 0.08531\n",
            "class proba training loss for this batch : 0.12772\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.14128\n",
            "xy_coord training loss for this batch : 0.00040\n",
            "wh_sizes training loss for this batch : 0.00039\n",
            "confidence with object training loss for this batch : 0.03471\n",
            "confidence without object training loss for this batch : 0.07456\n",
            "class proba training loss for this batch : 0.06533\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:16.098383\n",
            "Mean training loss for this epoch : 0.21776\n"
          ]
        }
      ],
      "source": [
        "delta_time = datetime.timedelta(hours=1)\n",
        "timezone = datetime.timezone(offset=delta_time)\n",
        "\n",
        "t = datetime.datetime.now(tz=timezone)\n",
        "str_t = '{:%Y-%m-%d %H:%M:%S}'.format(t)\n",
        "print(f\"[START] : {str_t} :\")\n",
        "print(f\"[Training on] : {str(device).upper()}\")\n",
        "\n",
        "EPOCHS = 3\n",
        "size_grid = 6\n",
        "batch_loss_list = []\n",
        "model_MNIST = model_MNIST.to(device)\n",
        "\n",
        "for epoch in range(EPOCHS) : \n",
        "    begin_time = timer()\n",
        "    epochs_loss = 0.\n",
        "    \n",
        "    print(\"-\"*20)\n",
        "    str_t = '{:%Y-%m-%d %H:%M:%S}'.format(t)\n",
        "    print(\" \"*5 + f\"{str_t} : EPOCH {epoch+1}/{EPOCHS}\")\n",
        "    print(\"-\"*20)\n",
        "\n",
        "    model_MNIST.train()\n",
        "    for batch, (img, labels, bbox_true) in enumerate(training_dataset):\n",
        "        loss = 0\n",
        "        begin_batch_time = timer()\n",
        "        img, labels, bbox_true = img.to(device), labels.to(device), bbox_true.to(device)\n",
        "        \n",
        "        ### turn bbox into NxSxSx5 tensor\n",
        "        bbox_true_6x6 = bbox2Tensor(bbox_true).to(device)\n",
        "        \n",
        "        ### clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        ### compute predictions\n",
        "        bbox_preds, label_preds = model_MNIST(img)\n",
        "        \n",
        "        ### compute losses over each grid cell for each image in the batch\n",
        "        losses, loss = loss_yolo(bbox_preds, bbox_true_6x6, label_preds, labels)\n",
        "    \n",
        "        ### compute gradients\n",
        "        loss.backward()\n",
        "        \n",
        "        ### Weight updates\n",
        "        optimizer.step()\n",
        "        \n",
        "        ######### print part #######################\n",
        "        current_loss = loss.item()\n",
        "        batch_loss_list.append(current_loss)\n",
        "        epochs_loss = epochs_loss + current_loss\n",
        "\n",
        "        if batch+1 <= len_training_ds//BATCH_SIZE:\n",
        "            current_training_sample = (batch+1)*BATCH_SIZE\n",
        "        else:\n",
        "            current_training_sample = (batch)*BATCH_SIZE + len_training_ds%BATCH_SIZE\n",
        "        \n",
        "        if (batch) == 0 or (batch+1)%100 == 0 or batch == len_training_ds//BATCH_SIZE:\n",
        "            print(f\" --- Image : {current_training_sample}/{len_training_ds}\",\\\n",
        "                    f\" : loss = {current_loss:.5f}\")\n",
        "            print(f\"xy_coord training loss for this batch : {torch.sum(losses['loss_xy']) / len(img):.5f}\")\n",
        "            print(f\"wh_sizes training loss for this batch : {torch.sum(losses['loss_wh']) / len(img):.5f}\")\n",
        "            print(f\"confidence with object training loss for this batch : {torch.sum(losses['loss_conf_obj']) / len(img):.5f}\")\n",
        "            print(f\"confidence without object training loss for this batch : {torch.sum(losses['loss_conf_noobj']) / len(img):.5f}\")\n",
        "            print(f\"class proba training loss for this batch : {torch.sum(losses['loss_class']) / len(img):.5f}\")\n",
        "            print('\\n')\n",
        "            if batch == (len_training_ds//BATCH_SIZE):\n",
        "                print(f\"Total elapsed time for training : {datetime.timedelta(seconds=timer()-begin_time)}\")\n",
        "                print(f\"Mean training loss for this epoch : {epochs_loss / len(training_dataset):.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS0A-3zLweFR"
      },
      "outputs": [],
      "source": [
        "torch.save(model_MNIST.state_dict(), \"yolo_mnist_model_3epochs.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "7O8u0S3PweFR"
      },
      "outputs": [],
      "source": [
        "def relative2absolute(bbox_relative:torch.Tensor, SIZEHW=75, S=6)->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Turns bounding box relative to cell coordinates into absolute coordinates \n",
        "    (pixels). Used to calculate IoU. \n",
        "\n",
        "    Args:\n",
        "        bbox_relative : torch.Tensor of shape (N, 4)\n",
        "            Bounding box coordinates to convert.\n",
        "    Return:\n",
        "        bbox_absolute : torch.Tensor of shape (N, 4)\n",
        "    \"\"\"\n",
        "    CELL_SIZE = SIZEHW/S\n",
        "\n",
        "    cx_rcell, cy_rcell, rw, rh = bbox_relative[:,:4].permute(1,0)\n",
        "    \n",
        "    ### xc,yc centers relative to the frame coordinates\n",
        "    cx = cx_rcell * CELL_SIZE - (1/CELL_SIZE) * (cx_rcell/CELL_SIZE).to(torch.int32)\n",
        "    cy = cy_rcell * CELL_SIZE - (1/CELL_SIZE) * (cy_rcell/CELL_SIZE).to(torch.int32)\n",
        "\n",
        "    ### xc,yc centers absolute coordinates\n",
        "    cx_abs = SIZEHW * cx\n",
        "    cy_abs = SIZEHW * cy\n",
        "\n",
        "    ### x,y absolute positions \n",
        "    x_min = cx_abs - (SIZEHW * (rw/2))\n",
        "    y_min = cy_abs - (SIZEHW * (rh/2))\n",
        "    x_max = cx_abs + (SIZEHW * (rw/2))\n",
        "    y_max = cy_abs + (SIZEHW * (rh/2))\n",
        "\n",
        "    bbox_absolute = torch.stack((x_min, y_min, x_max, y_max), dim=-1)\n",
        "    return bbox_absolute\n",
        "\n",
        "def intersection_over_union(pred_box:torch.Tensor, true_box:torch.Tensor)->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Intersection over Union method.\n",
        "\n",
        "    Args:\n",
        "        pred_box : torch.Tensor of shape (N, 5)\n",
        "            Predicted bounding boxes of a batch, in a given cell.\n",
        "        true_box : torch.Tensor of shape (N, 5)\n",
        "            Ground truth bounding boxes of a batch, in a given cell.\n",
        "\n",
        "    Return:\n",
        "        iou : float\n",
        "            Number between 0 and 1 where 1 is a perfect overlap.\n",
        "    \"\"\"\n",
        "    ### Convert cell reltative coordinates to absolute coordinates\n",
        "    pred_box = relative2absolute(pred_box)\n",
        "    true_box = relative2absolute(true_box)   \n",
        "    xmin_pred, ymin_pred, xmax_pred, ymax_pred = pred_box.permute(1,0)\n",
        "    xmin_true, ymin_true, xmax_true, ymax_true = true_box.permute(1,0)\n",
        "\n",
        "    ### There is no object if all coordinates are zero\n",
        "    isObject = xmin_true + ymin_true + xmax_true + ymax_true\n",
        "    isObject = isObject.to(torch.bool)\n",
        "\n",
        "    smoothing_factor = 1e-10\n",
        "\n",
        "    ### x, y overlaps btw pred and groundtrue\n",
        "    xmin_overlap = torch.maximum(xmin_pred, xmin_true)\n",
        "    xmax_overlap = torch.minimum(xmax_pred, xmax_true)\n",
        "    ymin_overlap = torch.maximum(ymin_pred, ymin_true)\n",
        "    ymax_overlap = torch.minimum(ymax_pred, ymax_true)\n",
        "    \n",
        "    ### Pred and groundtrue areas\n",
        "    pred_box_area = (xmax_pred - xmin_pred) * (ymax_pred - ymin_pred)\n",
        "    true_box_area = (xmax_true - xmin_true) * (ymax_true - ymin_true)\n",
        "\n",
        "    ### Compute intersection area, union area and IoU\n",
        "    overlap_area = torch.maximum((xmax_overlap - xmin_overlap), torch.Tensor([0]).to(device)) * torch.maximum((ymax_overlap - ymin_overlap), torch.Tensor([0]).to(device))\n",
        "    union_area = (pred_box_area + true_box_area) - overlap_area\n",
        "    iou = (overlap_area + smoothing_factor) / (union_area + smoothing_factor)\n",
        "    \n",
        "    ### Set IoU to zero when there is no coordinates (i.e. no object)\n",
        "    iou = iou * isObject\n",
        "\n",
        "    return iou   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWc1Cr8yrSH8",
        "outputId": "bd79409e-b55c-42a0-fa11-f609b48225cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE BOX :  tensor(0.0010)\n",
            "MSE confidence score :  tensor(0.5613)\n",
            "class acc :  tensor(97.1700) %\n"
          ]
        }
      ],
      "source": [
        "S=6\n",
        "for (img, labels, bbox_true) in validation_dataset:\n",
        "    img, labels, bbox_true = img.to(device), labels.to(device), bbox_true.to(device)\n",
        "    model_MNIST.eval()\n",
        "    with torch.no_grad():\n",
        "        ### prediction\n",
        "        bbox_preds, label_preds = model_MNIST(img)\n",
        "\n",
        "        ### (N,4) -> (N, S, S, 5)\n",
        "        bbox_true_6x6 = bbox2Tensor(bbox_true).to(device)\n",
        "\n",
        "        ### keeping only cells (i,j) with an object \n",
        "        cells_with_obj = bbox_true_6x6.nonzero()[::5]\n",
        "        N, cells_i, cells_j, _ = cells_with_obj.permute(1,0)\n",
        "\n",
        "        ### MSE along bbox coordinates and sizes in the cells containing an object\n",
        "        mse_box = (1/len(img)) * torch.sum(torch.pow(bbox_true - bbox_preds[N, cells_i, cells_j,:4],2))\n",
        "        \n",
        "        ### confidence score accuracy : sum of the all grid confidence scores\n",
        "        ### pred confidence score is confidence score times IoU.\n",
        "        mse_confidence_score = torch.zeros(len(img))\n",
        "        for i in range(S):\n",
        "            for j in range(S):\n",
        "                iou = intersection_over_union(bbox_true_6x6[:,i,j], bbox_preds[:,i,j])\n",
        "                mse_confidence_score += torch.pow(bbox_true_6x6[:,i,j,-1] - bbox_preds[:,i,j,-1] * iou,2)\n",
        "        \n",
        "        mse_confidence_score = (1/(len(img))) * torch.sum(mse_confidence_score)\n",
        "\n",
        "        ### applied softmax to class predictions and compute accuracy\n",
        "        softmax_pred_classes = torch.softmax(label_preds[N, cells_i, cells_j], dim=1)\n",
        "        classes_acc = (1/len(img)) * torch.sum(torch.argmax(labels, dim=1) == torch.argmax(softmax_pred_classes, dim=1))\n",
        "\n",
        "print(\"MSE BOX : \", mse_box)\n",
        "print(\"MSE confidence score : \", mse_confidence_score)\n",
        "print(\"class acc : \", classes_acc*100,\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "im_width = 75\n",
        "im_height = 75\n",
        "use_normalized_coordinates = True\n",
        "\n",
        "def draw_ONE_bounding_box_on_image(image, ymin:int, xmin:int, ymax:int, xmax:int, \n",
        "                               color:str='red', thickness:int=1, display_str:bool=None, \n",
        "                               use_normalized_coordinates:bool=True):\n",
        "  \"\"\"Adds a bounding box to an image.\n",
        "  Bounding box coordinates can be specified in either absolute (pixel) or\n",
        "  normalized coordinates by setting the use_normalized_coordinates argument.\n",
        "  \n",
        "  Args:\n",
        "    image: a PIL.Image object.\n",
        "    ymin: ymin of bounding box.\n",
        "    xmin: xmin of bounding box.\n",
        "    ymax: ymax of bounding box.\n",
        "    xmax: xmax of bounding box.\n",
        "    color: color to draw bounding box. Default is red.\n",
        "    thickness: line thickness. Default value is 4.\n",
        "    display_str_list: string to display in box\n",
        "    use_normalized_coordinates: If True (default), treat coordinates\n",
        "      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n",
        "      coordinates as absolute.\n",
        "  \"\"\"\n",
        "  draw = PIL.ImageDraw.Draw(image)\n",
        "  im_width, im_height = image.size\n",
        "  \n",
        "  if use_normalized_coordinates:\n",
        "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
        "                                  ymin * im_height, ymax * im_height)\n",
        "  else:\n",
        "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
        "  draw.line([(left, top), (left, bottom), (right, bottom),\n",
        "             (right, top), (left, top)], width=thickness, fill=color)\n",
        "\n",
        "\n",
        "\n",
        "def draw_bounding_boxes_on_image(image, boxes:np.ndarray, color:list=[], \n",
        "                                 thickness:int=1, display_str_list:tuple=()):\n",
        "  \"\"\"Draws bounding boxes on image.\n",
        "\n",
        "  Args:\n",
        "    image: a PIL.Image object.\n",
        "    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
        "           The coordinates are in normalized format between [0, 1].\n",
        "    color: color to draw bounding box. Default is red.\n",
        "    thickness: line thickness. Default value is 4.\n",
        "    display_str_list: a list of strings for each bounding box.\n",
        "                           \n",
        "  Raises:\n",
        "    ValueError: if boxes is not a [N, 4] array\n",
        "  \"\"\"\n",
        "  boxes_shape = boxes.shape\n",
        "  if not boxes_shape:\n",
        "    return\n",
        "  if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n",
        "    raise ValueError('Input must be of size [N, 4]')\n",
        "  \n",
        "  for i in range(boxes_shape[0]):\n",
        "    draw_ONE_bounding_box_on_image(image, \n",
        "                                   boxes[i, 1], boxes[i, 0], \n",
        "                                   boxes[i, 3], boxes[i, 2], \n",
        "                                   color[i], thickness, display_str_list[i])\n",
        "\n",
        "\n",
        "def draw_bounding_boxes_on_image_array(image:np.ndarray, boxes:np.ndarray, color:list=[], \n",
        "                                       thickness:int=1, display_str_list:tuple=()):\n",
        "  \"\"\"Draws bounding boxes on image (numpy array).\n",
        "\n",
        "  Args:\n",
        "    image: a numpy array object.\n",
        "    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
        "           The coordinates are in normalized format between [0, 1].\n",
        "    color: color to draw bounding box. Default is red.\n",
        "    thickness: line thickness. Default value is 4.\n",
        "    display_str_list_list: a list of strings for each bounding box.\n",
        "  Raises:\n",
        "    ValueError: if boxes is not a [N, 4] array\n",
        "  \"\"\"\n",
        "  image_pil = PIL.Image.fromarray(image)\n",
        "  rgbimg = PIL.Image.new(\"RGBA\", image_pil.size)\n",
        "  rgbimg.paste(image_pil)\n",
        "  draw_bounding_boxes_on_image(rgbimg, boxes, color, thickness, display_str_list)\n",
        "  return np.array(rgbimg)"
      ],
      "metadata": {
        "id": "yb1WSNT-xqSI"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############### Matplotlib config\n",
        "plt.rc('image', cmap='gray')\n",
        "plt.rc('grid', linewidth=0)\n",
        "plt.rc('xtick', top=False, bottom=False, labelsize='large')\n",
        "plt.rc('ytick', left=False, right=False, labelsize='large')\n",
        "plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')\n",
        "plt.rc('text', color='a8151a')\n",
        "plt.rc('figure', facecolor='F0F0F0')# Matplotlib fonts\n",
        "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), \"mpl-data/fonts/ttf\")\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "iAUtKFv24Rhq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "def display_digits_with_boxes(digits, predictions, labels, pred_bboxes, bboxes, iou, title):\n",
        "  \"\"\"Utility to display a row of digits with their predictions.\n",
        "\n",
        "  Args:\n",
        "    digits : np.ndarray of shape (N,75,75,1)\n",
        "        Raw image with normalized pixel values (from 0 to 1)\n",
        "    predictions : np.ndarray of shape (N,)\n",
        "        Predicted label with the same shape as labels\n",
        "    labels : np.ndarray of shape (N,)\n",
        "        Labels of the digits (from 0 to 9)\n",
        "    pred_bboxes : np.ndarray of shape (n, N) ??\n",
        "        Predicted bboxes locations\n",
        "    bboxes : np.ndarray of shape (n, N)\n",
        "        Ground true bboxe locations\n",
        "    iou : list of shape (???)\n",
        "        IoU of each bboxes\n",
        "    title : str\n",
        "        Figure's title\n",
        "  \"\"\"\n",
        "  n = 10\n",
        "  indexes = np.random.choice(len(predictions), size=n)\n",
        "  n_digits = digits[indexes]\n",
        "  n_predictions = predictions[indexes]\n",
        "  n_labels = labels[indexes]\n",
        "\n",
        "  n_iou = []\n",
        "  if len(iou) > 0:\n",
        "    # If multiple bboxes\n",
        "    n_iou = iou[indexes]\n",
        "\n",
        "  if (len(pred_bboxes) > 0):\n",
        "    # If multiple bboxes predicted\n",
        "    n_pred_bboxes = pred_bboxes[indexes,:]\n",
        "\n",
        "  if (len(bboxes) > 0):\n",
        "    # If multiple ground truth bboxes\n",
        "    n_bboxes = bboxes[indexes,:]\n",
        "\n",
        "  # Rescale pixel values to un-normed values (from 0 -black- to 255 -white-)\n",
        "  n_digits = n_digits * 255.0\n",
        "  n_digits = n_digits.reshape(n, 75, 75)\n",
        "\n",
        "  # Set plot config\n",
        "  fig = plt.figure(figsize=(20, 4))\n",
        "  plt.title(title)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  \n",
        "  for i in range(10):\n",
        "    ax = fig.add_subplot(1, 10, i+1)\n",
        "    bboxes_to_plot = []\n",
        "    if (len(pred_bboxes) > i):\n",
        "      bboxes_to_plot.append(n_pred_bboxes[i])\n",
        "    \n",
        "    if (len(bboxes) > i):\n",
        "      bboxes_to_plot.append(n_bboxes[i])\n",
        "\n",
        "    img_to_draw = draw_bounding_boxes_on_image_array(image=n_digits[i], boxes=np.asarray(bboxes_to_plot), color=['red', 'green'], display_str_list=[\"true\", \"pred\"])\n",
        "    plt.xlabel(n_predictions[i])\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    \n",
        "    if n_predictions[i] != n_labels[i]:\n",
        "      ax.xaxis.label.set_color('red')\n",
        "    \n",
        "    plt.imshow(img_to_draw)\n",
        "\n",
        "    if len(iou) > i :\n",
        "      color = \"black\"\n",
        "      if (n_iou[i][0] < iou_threshold):\n",
        "        color = \"red\"\n",
        "      ax.text(0.2, -0.3, \"iou: %s\" %(n_iou[i][0]), color=color, transform=ax.transAxes)\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "def dataset_to_numpy_util(training_dataset, validation_dataset, N):\n",
        "  \"\"\"\n",
        "  Pull a batch from the datasets. This code is not very nice.\n",
        "  \n",
        "  Args:\n",
        "    training_dataset : torch.utils.data.Dataset\n",
        "        Dataset from the torch.utils.data.Dataset Pytorch class, returning the \n",
        "        training digits, labels and bboxes coordinates as batches such as : \n",
        "            - training digits : torch.Tensor of shape (batch_size, 1, 75, 75)\n",
        "            - labels : torch.Tensor of shape (batch_size, 10)\n",
        "            - bboxes coordinates : torch.Tensor of shape (batch_size, 4)\n",
        "    validation_dataset : torch.utils.data.Dataset \n",
        "        Dataset from the torch.utils.data.Dataset Pytorch class, returning the \n",
        "        whole validation digits, labels and bboxes coordinates.\n",
        "    N : int\n",
        "        Size of the training sample to extract from the training dataset\n",
        "\n",
        "  Returns:\n",
        "    N_train_ds_digits : np.ndarray of shape (N, 1, 75, 75)\n",
        "    N_train_ds_labels : np.ndarray of shape (N, 10)\n",
        "    N_train_ds_bboxes : np.ndarray of shape (N, 4)\n",
        "    validation_digits : np.ndarray of shape (len(validation_dataset), 1, 75, 75)\n",
        "    validation_labels : np.ndarray of shape (len(validation_dataset), 10)\n",
        "    validation_bboxes : np.ndarray of shape (len(validation_dataset), 4)\n",
        "  \"\"\"\n",
        "  ### get N training digits, labels and bboxes from one batch\n",
        "  ### turning the bboxes coordinates into ndarrays\n",
        "  one_batch_train_ds_digits, one_batch_train_ds_labels, one_batch_train_ds_bboxes = next(iter(training_dataset))\n",
        "  N_train_ds_digits = one_batch_train_ds_digits[:N].numpy()\n",
        "  N_train_ds_labels = one_batch_train_ds_labels[:N].numpy()\n",
        "  N_train_ds_bboxes = one_batch_train_ds_bboxes[:N].numpy()\n",
        "  \n",
        "  ### get the whole validation digits, labels and bboxes\n",
        "  ### turning the bboxes coordinates into ndarrays\n",
        "  for validation_digits, validation_labels, validation_bboxes in validation_dataset:\n",
        "      validation_digits = validation_digits.numpy()\n",
        "      validation_labels = validation_labels.numpy()\n",
        "      validation_bboxes = validation_bboxes.numpy()\n",
        "      break\n",
        "\n",
        "  # turning one hot encoding labels into the corresponding digit\n",
        "  validation_labels = np.argmax(validation_labels, axis=1)\n",
        "  N_train_ds_labels = np.argmax(N_train_ds_labels, axis=1)\n",
        "\n",
        "  return (N_train_ds_digits, N_train_ds_labels, N_train_ds_bboxes,\n",
        "          validation_digits, validation_labels, validation_bboxes)"
      ],
      "metadata": {
        "id": "rEEBQaok5TFu"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(training_digits, training_labels, training_bboxes,\n",
        " validation_digits, validation_labels, validation_bboxes) = dataset_to_numpy_util(training_dataset, validation_dataset, 10)\n",
        "\n",
        "validation_bboxes = relative2absolute(torch.as_tensor(validation_bboxes))\n",
        "validation_bboxes = np.asarray(validation_bboxes)/75\n",
        "#iou = intersection_over_union(predicted_bboxes.numpy(), validation_bboxes)\n",
        "\n",
        "iou_threshold = 0.6\n",
        "\n",
        "#print(\"Number of predictions where iou > threshold(%s): %s\" % (iou_threshold, (iou >= iou_threshold).sum()))\n",
        "#print(\"Number of predictions where iou < threshold(%s): %s\" % (iou_threshold, (iou < iou_threshold).sum()))\n",
        "\n",
        "\n",
        "display_digits_with_boxes(validation_digits, validation_labels, validation_labels, np.array([]), validation_bboxes, np.array([]), \"validation digits and their labels\")"
      ],
      "metadata": {
        "id": "NDK68abLxos6",
        "outputId": "2200125f-2447-4f7d-8d90-88819290f473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x288 with 11 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-C3WG30n0ggV"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "euSLQ6z_8aL_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "YOLO_MNIST_Localization.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "bfc2e043f4a41bbce066f7921047bdcfe8769546c7198e4df6482fd90de274f1"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a9c9fb5cfe5b401d86845703a998f6db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6aba0ea5bfdf4f14a8c518ff3d9a71f5",
              "IPY_MODEL_2bc7b2045344477786066f7f2608fdb3",
              "IPY_MODEL_307c3ca345474ff7b4f0f9fb3a9d807f"
            ],
            "layout": "IPY_MODEL_8916b530b9a845b98eb2d9c20caed78e"
          }
        },
        "6aba0ea5bfdf4f14a8c518ff3d9a71f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9208ff95819b4239a4f95946ff74133b",
            "placeholder": "​",
            "style": "IPY_MODEL_6b4f8fd39d0a4ee4ac61c69782bf188c",
            "value": "100%"
          }
        },
        "2bc7b2045344477786066f7f2608fdb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a709533f2f9b40738441a55a6c6b7c97",
            "max": 9912422,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5683d25400ba49568b5fe1d064de61f6",
            "value": 9912422
          }
        },
        "307c3ca345474ff7b4f0f9fb3a9d807f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f505185d1274002a36479074afdba33",
            "placeholder": "​",
            "style": "IPY_MODEL_00636579a0ec4bf593858001d88d6433",
            "value": " 9912422/9912422 [00:00&lt;00:00, 19596100.36it/s]"
          }
        },
        "8916b530b9a845b98eb2d9c20caed78e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9208ff95819b4239a4f95946ff74133b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b4f8fd39d0a4ee4ac61c69782bf188c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a709533f2f9b40738441a55a6c6b7c97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5683d25400ba49568b5fe1d064de61f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f505185d1274002a36479074afdba33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00636579a0ec4bf593858001d88d6433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43eab58b65014e8f99743e3418d66142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c4394133c644462b01f881969ffca90",
              "IPY_MODEL_6d0eb0f2ad6d48f198e010b2131449c5",
              "IPY_MODEL_60ca8581c0784d118955e2243e7bf07d"
            ],
            "layout": "IPY_MODEL_2612c008838f4d04ac954a9757ee2ff4"
          }
        },
        "9c4394133c644462b01f881969ffca90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ad1d80c567e40bb84d63b82f80ba3c0",
            "placeholder": "​",
            "style": "IPY_MODEL_79d28217e64d4437ba797af011bf8876",
            "value": "100%"
          }
        },
        "6d0eb0f2ad6d48f198e010b2131449c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ccdcd2be1de4c2cbeca443df198f9f1",
            "max": 28881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f6945eee0284efcb07275ec0200768c",
            "value": 28881
          }
        },
        "60ca8581c0784d118955e2243e7bf07d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9656c9e32c04fc88a22707afe80751d",
            "placeholder": "​",
            "style": "IPY_MODEL_80295e93550f426499dc157e2a1191e4",
            "value": " 28881/28881 [00:00&lt;00:00, 317853.02it/s]"
          }
        },
        "2612c008838f4d04ac954a9757ee2ff4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ad1d80c567e40bb84d63b82f80ba3c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79d28217e64d4437ba797af011bf8876": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ccdcd2be1de4c2cbeca443df198f9f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f6945eee0284efcb07275ec0200768c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b9656c9e32c04fc88a22707afe80751d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80295e93550f426499dc157e2a1191e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f5e919103fb4395af74f6d45a099609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77298f5e1af84c62bb652e42f54cf9ac",
              "IPY_MODEL_a17a84f17723487fa5a91300a8e7c9c9",
              "IPY_MODEL_4c6d6b6b59324cc4937833ff1d3bc87e"
            ],
            "layout": "IPY_MODEL_840d28bff63440949877e251461fee19"
          }
        },
        "77298f5e1af84c62bb652e42f54cf9ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fd26fe686e349e9b93ae78429c9134f",
            "placeholder": "​",
            "style": "IPY_MODEL_d627a7fa5be045079b232b7d28511916",
            "value": "100%"
          }
        },
        "a17a84f17723487fa5a91300a8e7c9c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84f46fccb72c4e938867304d1520b697",
            "max": 1648877,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d9e90b4e3844f2687538b4e2959ed24",
            "value": 1648877
          }
        },
        "4c6d6b6b59324cc4937833ff1d3bc87e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70267f10866f4c618801e0107ed9f63d",
            "placeholder": "​",
            "style": "IPY_MODEL_58f79da6016e4aa898cf16c8e0199801",
            "value": " 1648877/1648877 [00:00&lt;00:00, 3358740.25it/s]"
          }
        },
        "840d28bff63440949877e251461fee19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fd26fe686e349e9b93ae78429c9134f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d627a7fa5be045079b232b7d28511916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84f46fccb72c4e938867304d1520b697": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d9e90b4e3844f2687538b4e2959ed24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "70267f10866f4c618801e0107ed9f63d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58f79da6016e4aa898cf16c8e0199801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18a8f093259a4e0ca40d5774e79daef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0fed3cb6c89d477699865cf55eff0c90",
              "IPY_MODEL_f991d13ade0a4db0b0d8f5c6a4d06cf3",
              "IPY_MODEL_0ec81182a61d411f830be71719af3272"
            ],
            "layout": "IPY_MODEL_c12a275fbaff4d90a5740b9d3e21eaf6"
          }
        },
        "0fed3cb6c89d477699865cf55eff0c90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b62c2bd9be5445481ccd8d2c361654a",
            "placeholder": "​",
            "style": "IPY_MODEL_4c3d9324a7c945ababe6aec06c94386e",
            "value": "100%"
          }
        },
        "f991d13ade0a4db0b0d8f5c6a4d06cf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2307ed6b07414d7e943c925db0586990",
            "max": 4542,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a798ceb39614164acafab5379d93118",
            "value": 4542
          }
        },
        "0ec81182a61d411f830be71719af3272": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e71976373e374c83a95018bb38eabe91",
            "placeholder": "​",
            "style": "IPY_MODEL_c19b7809f56a4316bf0445469e18cb2f",
            "value": " 4542/4542 [00:00&lt;00:00, 41222.60it/s]"
          }
        },
        "c12a275fbaff4d90a5740b9d3e21eaf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b62c2bd9be5445481ccd8d2c361654a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c3d9324a7c945ababe6aec06c94386e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2307ed6b07414d7e943c925db0586990": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a798ceb39614164acafab5379d93118": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e71976373e374c83a95018bb38eabe91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c19b7809f56a4316bf0445469e18cb2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}