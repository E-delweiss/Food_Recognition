{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThOpaque/Food_Recognition/blob/main/YOLO_MNIST_Localization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAHeh1BRufX4",
        "outputId": "9de7ad46-f8a8-4a9a-e9c7-b472862e06c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 41.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.9.3\n"
          ]
        }
      ],
      "source": [
        "import os, time, datetime\n",
        "from timeit import default_timer as timer\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "%pip install torchinfo;\n",
        "%pip install torchmetrics;\n",
        "from torchmetrics import MeanSquaredError;\n",
        "from torchinfo import summary;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOraK1TX7XZB",
        "outputId": "f85e007e-e33e-4001-d5ba-fc16d5cf3999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------------------------\n",
            "Execute notebook on - cuda -\n",
            "------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Choosing device between CPU or GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "elif torch.has_mps:\n",
        "    device=torch.device('mps')\n",
        "else:\n",
        "    device=torch.device('cpu')\n",
        "    \n",
        "print(\"\\n------------------------------------\")\n",
        "print(f\"Execute notebook on - {device} -\")\n",
        "print(\"------------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "M7VztqFE71JZ"
      },
      "outputs": [],
      "source": [
        "class my_mnist_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root:str, split:str=None, download:bool=False, S=6, sizeHW=75):\n",
        "        assert split, \"You have to specify the split.\"\n",
        "        \n",
        "        if split == \"train\":\n",
        "            train = True\n",
        "        elif split == \"test\":\n",
        "            train = False\n",
        "        \n",
        "        self.dataset = torchvision.datasets.MNIST(root=root, train=train, download=download)\n",
        "        \n",
        "        self.cell_size = sizeHW / S\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def _numpy_pad_to_bounding_box(self, image, offset_height=0, offset_width=0, target_height=0, target_width=0):\n",
        "        assert image.shape[:-1][0] <= target_height-offset_height, \"height must be <= target - offset\"\n",
        "        assert image.shape[:-1][1] <= target_width-offset_width, \"width must be <= target - offset\"\n",
        "        \n",
        "        target_array = np.zeros((target_height, target_width, image.shape[-1]))\n",
        "\n",
        "        for k in range(image.shape[0]):\n",
        "            target_array[offset_height+k][offset_width:image.shape[1]+offset_width] = image[k]\n",
        "        \n",
        "        return target_array\n",
        "\n",
        "    def _transform_pasting75(self, image, label):\n",
        "        ### xmin, ymin of digit\n",
        "        xmin = torch.randint(0, 48, (1,))\n",
        "        ymin = torch.randint(0, 48, (1,))\n",
        "        \n",
        "        image = torchvision.transforms.ToTensor()(image)\n",
        "        image = torch.reshape(image, (28,28,1,))\n",
        "        image = torch.from_numpy(self._numpy_pad_to_bounding_box(image, ymin, xmin, 75, 75))\n",
        "        image = image.permute(2, 0, 1) #(C,H,W)\n",
        "        image = image.to(torch.float)\n",
        "        \n",
        "        xmin, ymin = xmin.to(torch.float), ymin.to(torch.float)\n",
        "\n",
        "        xmax_bbox, ymax_bbox = (xmin + 28), (ymin + 28)\n",
        "        xmin_bbox, ymin_bbox = xmin, ymin\n",
        "        w_bbox = xmax_bbox-xmin_bbox\n",
        "        h_bbox = ymax_bbox-ymin_bbox\n",
        "\n",
        "        rw = w_bbox / 75\n",
        "        rh = h_bbox / 75\n",
        "        cx = (xmin + (w_bbox/2))/75\n",
        "        cy = (ymin + (h_bbox/2))/75\n",
        "\n",
        "        cx_rcell = cx % self.cell_size / self.cell_size\n",
        "        cy_rcell = cy % self.cell_size / self.cell_size\n",
        "\n",
        "\n",
        "        label_one_hot = F.one_hot(torch.as_tensor(label, dtype=torch.int64), 10)\n",
        "        bbox_coord = torch.Tensor([cx_rcell, cy_rcell, rw, rh])\n",
        "\n",
        "        return image, label_one_hot, bbox_coord\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        image, one_hot_label, bbox_coord = self._transform_pasting75(self.dataset[idx][0], self.dataset[idx][1])\n",
        "        \n",
        "        return image, one_hot_label.to(torch.float), bbox_coord\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295,
          "referenced_widgets": [
            "e78f76aad3ba466aa76e6000e06974d7",
            "81ec169e88a34a51a0f2f25030f6f336",
            "c03d2a3cfa0e4d958c7ff255113001ec",
            "1629fc5b6d644e11b6fc895a39553807",
            "4bfeea2a09374739a89ce1b774e7d647",
            "2e584742a69646678c3995da54a01902",
            "514b097815e941b09d2c5660b53403c0",
            "cd1af987f91c4309b138444ba940930f",
            "06287fcbe81242178cbb40ccb2677922",
            "7d96a7494e8544b282738d8d59e4e1b2",
            "748604520514468da3d8d80759169bd6",
            "c04dbe42a37f4dbda9d22a666fcc19ba",
            "edd52f2e07624bf792d3d4a320923247",
            "87a42523125345bbb247a4dca8110f45",
            "7302a680436d4d2597689f3e3814a657",
            "aa929078b5204f4fa0d3f108becd9512",
            "9b893464e51e47f8aec6a3b334c5029a",
            "c6a5a8fb183f4b41b78a368485f52129",
            "30839291036944ad9dee8af44ada9e9d",
            "4f8a0055577544298d16cb7f135c9241",
            "04abac823bf148228b4d7943ddb50412",
            "3e4c9699fe794e079957da4aaf86b2a2",
            "078506e8af2843b5b9191f9405f94372",
            "2a345eda307f45179703d899379f0f2e",
            "0a5a381ddeb54da7908f91c294232855",
            "d146d83e45854cbd88c5d52babc3fdf2",
            "da792f1d25784806a025d334e40848b4",
            "642676f86c744e96996b182c32f4c2f6",
            "f52cb61ab96e4ae180a49a9472b9bf4d",
            "003b0852f9464715b79575347e347d9b",
            "b3e2709b880240879f764319496ec40c",
            "bef540a945ed416cae626a5d534310d0",
            "6f74edff5c1a4baabb64d5d648038f76",
            "1d674581dac64a3aaa29bebcbb5b0e34",
            "22d9e6194f7c4f919073fc7af0f03aef",
            "eba57518d4e34a0193136f85aaee8af0",
            "5e2a9f63558d43d29aef14f042b6bc46",
            "e805ce371c824e1d905d13cb4b399d9e",
            "a581c8d44c52462fad681849910f0dfe",
            "efb9d5fc80c04baa853cd7375e66e96f",
            "1f46b04f6b0b489896739ddc33734019",
            "52bf0e0cf0d84be192c9452c0e13756d",
            "e9d285ccbc214a76bd84e2560431d608",
            "04ff799305da40e5bd9af7259b933d56"
          ]
        },
        "id": "yQNznLfO8jOx",
        "outputId": "91d0f386-066f-4913-b4d6-bce50dbfeed2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/9912422 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e78f76aad3ba466aa76e6000e06974d7"
            },
            "application/json": {
              "n": 0,
              "total": 9912422,
              "elapsed": 0.019474029541015625,
              "ncols": null,
              "nrows": null,
              "prefix": "",
              "ascii": false,
              "unit": "it",
              "unit_scale": false,
              "rate": null,
              "bar_format": null,
              "postfix": null,
              "unit_divisor": 1000,
              "initial": 0,
              "colour": null
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/28881 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c04dbe42a37f4dbda9d22a666fcc19ba"
            },
            "application/json": {
              "n": 0,
              "total": 28881,
              "elapsed": 0.019633054733276367,
              "ncols": null,
              "nrows": null,
              "prefix": "",
              "ascii": false,
              "unit": "it",
              "unit_scale": false,
              "rate": null,
              "bar_format": null,
              "postfix": null,
              "unit_divisor": 1000,
              "initial": 0,
              "colour": null
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1648877 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "078506e8af2843b5b9191f9405f94372"
            },
            "application/json": {
              "n": 0,
              "total": 1648877,
              "elapsed": 0.019672870635986328,
              "ncols": null,
              "nrows": null,
              "prefix": "",
              "ascii": false,
              "unit": "it",
              "unit_scale": false,
              "rate": null,
              "bar_format": null,
              "postfix": null,
              "unit_divisor": 1000,
              "initial": 0,
              "colour": null
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4542 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d674581dac64a3aaa29bebcbb5b0e34"
            },
            "application/json": {
              "n": 0,
              "total": 4542,
              "elapsed": 0.019474267959594727,
              "ncols": null,
              "nrows": null,
              "prefix": "",
              "ascii": false,
              "unit": "it",
              "unit_scale": false,
              "rate": null,
              "bar_format": null,
              "postfix": null,
              "unit_divisor": 1000,
              "initial": 0,
              "colour": null
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def get_training_dataset(BATCH_SIZE=64):\n",
        "    \"\"\"\n",
        "    Loads and maps the training split of the dataset using the custom dataset class. \n",
        "    \"\"\"\n",
        "    dataset = my_mnist_dataset(root=\"data\", split=\"train\", download=True)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    return dataloader, len(dataset)\n",
        "\n",
        "def get_validation_dataset(BATCH_SIZE = None):\n",
        "    \"\"\"\n",
        "    Loads and maps the validation split of the datasetusing the custom dataset class. \n",
        "    \"\"\"\n",
        "    dataset = my_mnist_dataset(root=\"data\", split=\"test\", download=True)\n",
        "    if BATCH_SIZE is None:\n",
        "        BATCH_SIZE = len(dataset)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    return dataloader, len(dataset)\n",
        "\n",
        "# instantiate the datasets\n",
        "training_dataset, len_training_ds = get_training_dataset()\n",
        "validation_dataset, len_validation_ds = get_validation_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wfgUx1srt90c"
      },
      "outputs": [],
      "source": [
        "class CNNBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(CNNBlock, self).__init__()\n",
        "        self.conv = torch.nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "        self.bn = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.l_relu = torch.nn.LeakyReLU(0.1)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        x = self.conv(input)\n",
        "        x = self.bn(x)\n",
        "        return self.l_relu(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3VjCkTEce-CT"
      },
      "outputs": [],
      "source": [
        "class YoloMNIST(torch.nn.Module):\n",
        "    def __init__(self, sizeHW, S, C, B):\n",
        "        super(YoloMNIST, self).__init__()\n",
        "        self.S, self.C, self.B = S, C, B\n",
        "        self.sizeHW = sizeHW\n",
        "        self.cell_size = self.sizeHW / self.S\n",
        "\n",
        "        self.seq = torch.nn.Sequential()        \n",
        "        self.seq.add_module(f\"conv_1\", CNNBlock(1, 32, stride=2, kernel_size=7, padding=2))\n",
        "        self.seq.add_module(f\"maxpool_1\", torch.nn.MaxPool2d(2))\n",
        "        self.seq.add_module(f\"conv_3\", CNNBlock(32, 128, stride=1, kernel_size=3, padding=0))\n",
        "        self.seq.add_module(f\"maxpool_2\", torch.nn.MaxPool2d(2))\n",
        "        self.seq.add_module(f\"conv_5\", CNNBlock(128, 64, stride=1, kernel_size=1, padding=0))\n",
        "        self.seq.add_module(f\"conv_4\", CNNBlock(64, 128, stride=1, kernel_size=3, padding=0))\n",
        "        self.seq.add_module(f\"conv_6\", CNNBlock(128, 128, stride=1, kernel_size=3, padding=1))\n",
        "        \n",
        "        self.fcs = self._create_fcs()\n",
        "\n",
        "    def _size_output(self, sizeHW:int, kernel:int, stride:int, padding:int=0, isMaxPool:bool=False)->int:\n",
        "        \"\"\"\n",
        "        Output size (width/height) of convolutional or maxpool layers.\n",
        "\n",
        "        Args:\n",
        "            sizeHW : int\n",
        "                Image size (we suppose this is a square image)\n",
        "            kernel : int\n",
        "                Size of a square kernel\n",
        "            stride : int\n",
        "                Stride of convolution layer\n",
        "            padding : int\n",
        "                Padding of convolution layer\n",
        "            isMaxPool : Bool, default is False.\n",
        "                Specify if it is a Maxpool layer (True) or not (False). \n",
        "\n",
        "        Return:\n",
        "            output_size : int\n",
        "                Image output size after a convolutional or MaxPool layer.\n",
        "        \"\"\" \n",
        "        if isMaxPool == True:\n",
        "            output_size = int(sizeHW/2)\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "        if padding == 'same':\n",
        "            output_size = sizeHW\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "        else:\n",
        "            output_size = (sizeHW + 2 * padding - (kernel-1)-1)/stride\n",
        "            output_size = int(output_size + 1)\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "\n",
        "    def _create_fcs(self):\n",
        "        output = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(128 * self.S * self.S, 4096),\n",
        "            torch.nn.LeakyReLU(0.1),\n",
        "            torch.nn.Linear(4096, self.S * self.S * (self.C + self.B*5))\n",
        "        )\n",
        "        return output\n",
        "    \n",
        "\n",
        "    def forward(self, input:torch.Tensor)->tuple:\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            input : torch.Tensor of shape (N, C, H, W)\n",
        "                Batch of images.\n",
        "\n",
        "        Return:\n",
        "            box_coord : torch.Tensor of shape (N, 6, 6, 5)\n",
        "                Contains xc_rcell, yc_rcell, rw, rh and the confidence number c\n",
        "                over 6x6 grid cells.\n",
        "            classifier : torch.Tensor of shape (N, 6, 6, 10)\n",
        "                Contains the one-hot encoding of each digit number over\n",
        "                6x6 grid cells.\n",
        "        \"\"\"     \n",
        "        x = self.seq(input)\n",
        "        x = self.fcs(x)\n",
        "        x = x.view(x.size(0), self.S, self.S, self.B * 5 + self.C)\n",
        "        box_coord = x[:,:,:,0:5]\n",
        "        classifier = x[:,:,:,5:]\n",
        "        return box_coord, classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UrYIfm2he-CV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "b078d240-d0a6-41ce-9b40-19aac0f965dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### OUTPUT DATASET ###\n",
            "La taille d'un élément est de :  3\n",
            "Le premier élément est l'image de shape :  torch.Size([64, 1, 75, 75])\n",
            "Le deuxieme élément est les label de shape :  torch.Size([64, 10])\n",
            "Le dernier élément est la bbox [xc_rcell, yc_rcell, xc, yc, rw, hw] :  torch.Size([64, 4])\n",
            "\n",
            "\n",
            "### OUTPUT PRED ###\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d73a9accc6df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"### OUTPUT PRED ###\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mpc_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_MNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Le premier élément est une grille de shape : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_MNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" correspondant aux nombres pc sur les 6x6 cellules\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Le deuxieme élement est, une grille de shape : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_MNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" correspondant aux [xc_rcell, yc_rcell, rw, rh]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ],
      "source": [
        "########### CELLULE TEST ##############################################\n",
        "model_MNIST = YoloMNIST(sizeHW=75, S=6, C=10, B=1)\n",
        "for k in training_dataset:\n",
        "    break\n",
        "print(\"### OUTPUT DATASET ###\")\n",
        "print(\"La taille d'un élément est de : \", len(k))\n",
        "print(\"Le premier élément est l'image de shape : \", k[0].shape)\n",
        "print(\"Le deuxieme élément est les label de shape : \", k[1].shape)\n",
        "print(\"Le dernier élément est la bbox [xc_rcell, yc_rcell, xc, yc, rw, hw] : \", k[2].shape)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"### OUTPUT PRED ###\")\n",
        "pc_pred, bbox_pred, class_pred = model_MNIST(k[0])\n",
        "print(\"Le premier élément est une grille de shape : \", model_MNIST(k[0])[0].shape, \" correspondant aux nombres pc sur les 6x6 cellules\")\n",
        "print(\"Le deuxieme élement est, une grille de shape : \", model_MNIST(k[0])[1].shape, \" correspondant aux [xc_rcell, yc_rcell, rw, rh]\")\n",
        "print(\"Le troisieme élement est, une grille de shape : \", model_MNIST(k[0])[2].shape, \" correspondant à des softmax vect sur chaque cellule de grille\")\n",
        "print(\"\\n\")\n",
        "\n",
        "bbox_true = k[2][:1]\n",
        "print(bbox_true.shape)\n",
        "bbox_true2 = torch.concat((bbox_true[0][0:2], bbox_true[0][4::])).unsqueeze(0)\n",
        "print(bbox_true2.shape)\n",
        "print(k[2][0])\n",
        "print(bbox_true2)\n",
        "#######################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "T_jOg_i_p2_c"
      },
      "outputs": [],
      "source": [
        "class YoloLoss(torch.nn.Module):\n",
        "    def __init__(self, lambd_coord:int, lambd_noobj:float, device:torch.device, S:int=6, sizeHW:int=75):\n",
        "        super(YoloLoss, self).__init__()\n",
        "        self.LAMBD_COORD = lambd_coord\n",
        "        self.LAMBD_NOOBJ = lambd_noobj\n",
        "        self.S = S\n",
        "        self.SIZEHW = sizeHW\n",
        "        self.CELL_SIZE = sizeHW/S\n",
        "        self.device = device\n",
        "\n",
        "    def _coordloss(self, pred_coord_rcell, true_coord_rcell):\n",
        "        \"\"\"\n",
        "        Args : \n",
        "            pred_coord_rcell : torch.Tensor of shape (N, 2)\n",
        "            true_coord_rcell : torch.Tensor of shape (N, 2)\n",
        "        Returns :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        xc_hat, yc_hat = pred_coord_rcell.permute(1,0)\n",
        "        xc, yc = true_coord_rcell.permute(1,0)\n",
        "        squared_error = torch.pow(xc - xc_hat,2) + torch.pow(yc - yc_hat,2)\n",
        "        return squared_error\n",
        "\n",
        "    def _sizeloss(self, pred_size, true_size):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_size : torch.Tensor of shape (N, 2)\n",
        "            true_size : torch.Tensor of shape (N, 2)\n",
        "        Returns : \n",
        "            root_squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        rw_hat, rh_hat = pred_size.permute(1,0)\n",
        "        rw, rh = true_size.permute(1,0)\n",
        "        \n",
        "        #sizes can't be negative\n",
        "        rw_hat = rw_hat.clip(min=0)\n",
        "        rh_hat = rh_hat.clip(min=0)\n",
        "\n",
        "        root_squared_error_w = torch.pow(torch.sqrt(rw) - torch.sqrt(rw_hat),2)\n",
        "        root_squared_error_h = torch.pow(torch.sqrt(rh) - torch.sqrt(rh_hat),2)\n",
        "        root_squared_error = root_squared_error_w + root_squared_error_h\n",
        "        return root_squared_error\n",
        "\n",
        "    def _confidenceloss(self, pred_c, true_c):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_c : torch.Tensor of shape (N)\n",
        "            true_c : torch.Tensor of shape (N)\n",
        "        Return :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        squared_error = torch.pow(true_c - pred_c, 2)\n",
        "        return squared_error\n",
        "\n",
        "    def _classloss(self, pred_class, true_class):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_class : torch.Tensor of shape (N, 10)\n",
        "            true_class : torch.Tensor of shape (N, 10)\n",
        "        Returns :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        squared_error = torch.pow(true_class - pred_class, 2)\n",
        "        return torch.sum(squared_error, dim=1)\n",
        "    \n",
        "    def _relative2absolute(self, bbox_relative:torch.Tensor)->torch.Tensor:\n",
        "        \"\"\"\n",
        "        Turns bounding box relative to cell coordinates into absolute coordinates \n",
        "        (pixels). Used to calculate IoU. \n",
        "\n",
        "        Args:\n",
        "            bbox_relative : torch.Tensor of shape (N, 4)\n",
        "                Bounding box coordinates to convert.\n",
        "        Return:\n",
        "            bbox_absolute : torch.Tensor of shape (N, 4)\n",
        "        \"\"\"\n",
        "        cx_rcell, cy_rcell, rw, rh = bbox_relative[:,:4].permute(1,0)\n",
        "        \n",
        "        ### xc,yc centers relative to the frame coordinates\n",
        "        cx = cx_rcell * self.CELL_SIZE - (1/self.CELL_SIZE) * (cx_rcell/self.CELL_SIZE).to(torch.int32)\n",
        "        cy = cy_rcell * self.CELL_SIZE - (1/self.CELL_SIZE) * (cy_rcell/self.CELL_SIZE).to(torch.int32)\n",
        "\n",
        "        ### xc,yc centers absolute coordinates\n",
        "        cx_abs = self.SIZEHW * cx\n",
        "        cy_abs = self.SIZEHW * cy\n",
        "\n",
        "        ### x,y absolute positions \n",
        "        x_min = cx_abs - (self.SIZEHW * (rw/2))\n",
        "        y_min = cy_abs - (self.SIZEHW * (rh/2))\n",
        "        x_max = cx_abs + (self.SIZEHW * (rw/2))\n",
        "        y_max = cy_abs + (self.SIZEHW * (rh/2))\n",
        "\n",
        "        bbox_absolute = torch.stack((x_min, y_min, x_max, y_max), dim=-1)\n",
        "        return bbox_absolute\n",
        "\n",
        "    def _intersection_over_union(self, pred_box:torch.Tensor, true_box:torch.Tensor)->float:\n",
        "        \"\"\"\n",
        "        Intersection over Union method.\n",
        "\n",
        "        Args:\n",
        "            pred_box : torch.Tensor of shape (N, 4)\n",
        "                Predicted bounding boxes of a batch, in a given cell.\n",
        "            true_box : torch.Tensor of shape (N, 4)\n",
        "                Ground truth bounding boxes of a batch, in a given cell.\n",
        "\n",
        "        Return:\n",
        "            iou : float\n",
        "                Number between 0 and 1 where 1 is a perfect overlap.\n",
        "        \"\"\"\n",
        "        ### Convert cell reltative coordinates to absolute coordinates\n",
        "        pred_box = self._relative2absolute(pred_box)\n",
        "        true_box = self._relative2absolute(true_box)   \n",
        "        xmin_pred, ymin_pred, xmax_pred, ymax_pred = pred_box.permute(1,0)\n",
        "        xmin_true, ymin_true, xmax_true, ymax_true = true_box.permute(1,0)\n",
        "\n",
        "        smoothing_factor = 1e-10\n",
        "\n",
        "        ### x, y overlaps btw pred and groundtrue\n",
        "        xmin_overlap = torch.maximum(xmin_pred, xmin_true)\n",
        "        xmax_overlap = torch.minimum(xmax_pred, xmax_true)\n",
        "        ymin_overlap = torch.maximum(ymin_pred, ymin_true)\n",
        "        ymax_overlap = torch.minimum(ymax_pred, ymax_true)\n",
        "        \n",
        "        ### Pred and groundtrue areas\n",
        "        pred_box_area = (xmax_pred - xmin_pred) * (ymax_pred - ymin_pred)\n",
        "        true_box_area = (xmax_true - xmin_true) * (ymax_true - ymin_true)\n",
        "\n",
        "        ### Compute intersection area, union area and IoU\n",
        "        overlap_area = torch.maximum((xmax_overlap - xmin_overlap), torch.Tensor([0]).to(device))  * torch.maximum((ymax_overlap - ymin_overlap), torch.Tensor([0]).to(device))\n",
        "        union_area = (pred_box_area + true_box_area) - overlap_area\n",
        "        iou = (overlap_area + smoothing_factor) / (union_area + smoothing_factor)\n",
        "        return iou   \n",
        "\n",
        "\n",
        "    def forward(self, pred_box:torch.Tensor, true_box:torch.Tensor, pred_class:torch.Tensor, true_class:torch.Tensor):\n",
        "        \"\"\"\n",
        "        Grid forward pass.\n",
        "\n",
        "        Args:\n",
        "            pred_box : torch.Tensor of shape (N, S, S, 5)\n",
        "                Batch predicted outputs containing xc_rcell, yc_rcell, rw, rh,\n",
        "                and confident number c for each grid cell.\n",
        "            true_box : torch.Tensor of shape (N, S, S, 5)\n",
        "                Groundtrue batch containing bbox values for each cell and\n",
        "                c indicate if there is an object to detect or not (1/0).\n",
        "            pred_class : torch.Tensor of shape (N, S, S, 10)\n",
        "                Probability of each digit class in each grid cell\n",
        "            true_class : torch.Tensor of shape (N, 10)\n",
        "                one-hot vect of each digit\n",
        "\n",
        "        Return:\n",
        "            loss : float\n",
        "                The batch loss value of the grid\n",
        "        \"\"\"\n",
        "        BATCH_SIZE = len(pred_box)\n",
        "\n",
        "        ### Initialization of the losses\n",
        "        losses_list = ['loss_xy', 'loss_wh', 'loss_conf_obj', 'loss_conf_noobj', 'loss_class']\n",
        "        losses = losses = {key : torch.zeros(BATCH_SIZE).to(self.device) for key in losses_list}    \n",
        "        \n",
        "        ### Compute the losses for all images in the batch\n",
        "        for i in range(self.S):\n",
        "            for j in range(self.S):\n",
        "                ### Intersection over Union\n",
        "                IoU = self._intersection_over_union(pred_box[:,i,j,:4], true_box[:,i,j,:4])\n",
        "\n",
        "                ### bbox coordinates\n",
        "                xy_hat = pred_box[:,i,j,:2]\n",
        "                xy = true_box[:,i,j,:2]\n",
        "                wh_hat = pred_box[:,i,j,2:4]\n",
        "                wh = true_box[:,i,j,2:4]\n",
        "                \n",
        "                ### confidence numbers\n",
        "                pred_c = pred_box[:,i,j,4] * IoU\n",
        "                true_c = true_box[:,i,j,4]\n",
        "\n",
        "                ### objects to detect\n",
        "                isObject = true_c.to(torch.bool)\n",
        "\n",
        "                ### sum the losses over the grid\n",
        "                losses['loss_xy'] += isObject * self._coordloss(xy_hat, xy)\n",
        "                losses['loss_wh'] += isObject * self._sizeloss(wh_hat, wh)\n",
        "                losses['loss_conf_obj'] += isObject * self._confidenceloss(pred_c, true_c)\n",
        "                losses['loss_conf_noobj'] += (~isObject) * self._confidenceloss(pred_c, true_c)\n",
        "                losses['loss_class'] += isObject * self._classloss(pred_class[:,i,j], true_class)\n",
        "\n",
        "\n",
        "        ### Yolo_v1 loss over the batch, shape : (BATCH_SIZE)\n",
        "        loss = self.LAMBD_COORD * losses['loss_xy'] \\\n",
        "                + self.LAMBD_COORD * losses['loss_wh'] \\\n",
        "                + losses['loss_conf_obj'] \\\n",
        "                + self.LAMBD_NOOBJ * losses['loss_conf_noobj'] \\\n",
        "                + losses['loss_class']\n",
        "                \n",
        "        return losses, loss\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "mHUOkfWPe-CW"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "model_MNIST = YoloMNIST(sizeHW=75, S=6, C=10, B=1)\n",
        "model_MNIST = model_MNIST.to(device)\n",
        "optimizer = torch.optim.Adam(params=model_MNIST.parameters(), lr=learning_rate, weight_decay=0.0005)\n",
        "loss_yolo = YoloLoss(lambd_coord=5, lambd_noobj=0.5, S=6, sizeHW=75, device=device)\n",
        "\n",
        "# print(optimizer)\n",
        "#summary(model_MNIST, input_size = (BATCH_SIZE,1,75,75))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "OdIlkN61mXJj"
      },
      "outputs": [],
      "source": [
        "def bbox2Tensor(bbox:torch.Tensor, S:int=6, sizeHW:int=75)->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Constructs en Tensor and puts bbox values in the corresponding i,j grid cell.\n",
        "\n",
        "    Args :\n",
        "        bbox : torch.Tensor of shape (N,4)\n",
        "            Contains bbox values xc_rcell, yc_rcell, rw and rh.\n",
        "        S : int, default is 6\n",
        "            Size of the grid.\n",
        "        sizeHW : int, default is 75\n",
        "            Size of the image.\n",
        "\n",
        "    Return :\n",
        "        bbox_t : torch.Tensor of shape (N, S, S, 4)\n",
        "            Tensor containing all 4 bbox values in the corresponding i,j grid\n",
        "            cell position i.e. in the i,j position where an object should be\n",
        "            detected.\n",
        "    \"\"\"\n",
        "    N = len(bbox)\n",
        "    bbox_t = torch.zeros(N,S,S,5)\n",
        "    cell_size = sizeHW/S\n",
        "\n",
        "    xc_rcell, yc_rcell, rw, rh = bbox.permute(1,0)\n",
        "    xc = xc_rcell * cell_size - (1/cell_size) * (xc_rcell/cell_size).to(torch.int32)\n",
        "    yc = yc_rcell * cell_size - (1/cell_size) * (yc_rcell/cell_size).to(torch.int32)\n",
        "\n",
        "    N_range = torch.arange(N)\n",
        "    lines = (yc * S).to(torch.long)\n",
        "    columns = (xc * S).to(torch.long)\n",
        "    bbox_t[N_range, lines, columns] = torch.stack((xc_rcell, yc_rcell, rw, rh, torch.ones(N))).permute(1,0)\n",
        "    \n",
        "    return bbox_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "DcZ9O1T2uLlc"
      },
      "outputs": [],
      "source": [
        "######## TODO : réfléchir à comment utiliser bbox_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tSO0Qo7e-CX",
        "outputId": "59e40883-dbae-46f4-d94f-ffe0273309a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[START] : 2022-08-22 13:58:15 :\n",
            "[Training on] : CUDA\n",
            "--------------------\n",
            "     2022-08-22 13:58:15 : EPOCH 1/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 4.85962\n",
            "xy_coord training loss for this epoch : 0.05774\n",
            "wh_sizes training loss for this epoch : 0.45408\n",
            "confidence with object training loss for this epoch : 1.00000\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 1.30053\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 4.17445\n",
            "xy_coord training loss for this epoch : 0.03842\n",
            "wh_sizes training loss for this epoch : 0.33377\n",
            "confidence with object training loss for this epoch : 0.99371\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 1.31982\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 4.36960\n",
            "xy_coord training loss for this epoch : 0.13605\n",
            "wh_sizes training loss for this epoch : 0.32105\n",
            "confidence with object training loss for this epoch : 1.00000\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 1.08413\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 3.17551\n",
            "xy_coord training loss for this epoch : 0.03852\n",
            "wh_sizes training loss for this epoch : 0.20179\n",
            "confidence with object training loss for this epoch : 1.00000\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.97396\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 3.18760\n",
            "xy_coord training loss for this epoch : 0.07555\n",
            "wh_sizes training loss for this epoch : 0.14662\n",
            "confidence with object training loss for this epoch : 1.00000\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 1.07678\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 2.90736\n",
            "xy_coord training loss for this epoch : 0.01945\n",
            "wh_sizes training loss for this epoch : 0.16863\n",
            "confidence with object training loss for this epoch : 1.00000\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.96696\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 3.22900\n",
            "xy_coord training loss for this epoch : 0.13181\n",
            "wh_sizes training loss for this epoch : 0.08911\n",
            "confidence with object training loss for this epoch : 0.99562\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 1.12876\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 4.77747\n",
            "xy_coord training loss for this epoch : 0.32347\n",
            "wh_sizes training loss for this epoch : 0.14150\n",
            "confidence with object training loss for this epoch : 1.00370\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 1.44895\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 2.30324\n",
            "xy_coord training loss for this epoch : 0.06428\n",
            "wh_sizes training loss for this epoch : 0.02383\n",
            "confidence with object training loss for this epoch : 0.90284\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.95983\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 12.42121\n",
            "xy_coord training loss for this epoch : 1.42590\n",
            "wh_sizes training loss for this epoch : 0.34357\n",
            "confidence with object training loss for this epoch : 1.00000\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 2.57382\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 3.61693\n",
            "xy_coord training loss for this epoch : 0.03773\n",
            "wh_sizes training loss for this epoch : 0.27056\n",
            "confidence with object training loss for this epoch : 1.00000\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 1.07548\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:27.240629\n",
            "Mean training loss for this epoch : 4.88971\n",
            "--------------------\n",
            "     2022-08-22 13:58:15 : EPOCH 2/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 3.72093\n",
            "xy_coord training loss for this epoch : 0.04070\n",
            "wh_sizes training loss for this epoch : 0.28353\n",
            "confidence with object training loss for this epoch : 1.00000\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 1.09977\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 2.78002\n",
            "xy_coord training loss for this epoch : 0.10708\n",
            "wh_sizes training loss for this epoch : 0.06631\n",
            "confidence with object training loss for this epoch : 0.92863\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.98442\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 2.39590\n",
            "xy_coord training loss for this epoch : 0.07967\n",
            "wh_sizes training loss for this epoch : 0.00872\n",
            "confidence with object training loss for this epoch : 1.00000\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.95394\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 2.05358\n",
            "xy_coord training loss for this epoch : 0.03973\n",
            "wh_sizes training loss for this epoch : 0.01525\n",
            "confidence with object training loss for this epoch : 0.84736\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.93132\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 2.56299\n",
            "xy_coord training loss for this epoch : 0.08336\n",
            "wh_sizes training loss for this epoch : 0.02335\n",
            "confidence with object training loss for this epoch : 0.94602\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 1.08341\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 2.26828\n",
            "xy_coord training loss for this epoch : 0.07791\n",
            "wh_sizes training loss for this epoch : 0.00510\n",
            "confidence with object training loss for this epoch : 0.94188\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.91138\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 3.21919\n",
            "xy_coord training loss for this epoch : 0.24584\n",
            "wh_sizes training loss for this epoch : 0.01255\n",
            "confidence with object training loss for this epoch : 0.99542\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.93180\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 2.09222\n",
            "xy_coord training loss for this epoch : 0.05005\n",
            "wh_sizes training loss for this epoch : 0.00158\n",
            "confidence with object training loss for this epoch : 0.93833\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.89575\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 2.19045\n",
            "xy_coord training loss for this epoch : 0.06530\n",
            "wh_sizes training loss for this epoch : 0.00119\n",
            "confidence with object training loss for this epoch : 0.93112\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.92688\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 3.05313\n",
            "xy_coord training loss for this epoch : 0.22118\n",
            "wh_sizes training loss for this epoch : 0.00964\n",
            "confidence with object training loss for this epoch : 0.94132\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.95773\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 1.93759\n",
            "xy_coord training loss for this epoch : 0.01982\n",
            "wh_sizes training loss for this epoch : 0.00194\n",
            "confidence with object training loss for this epoch : 0.92867\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.90014\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:28.550059\n",
            "Mean training loss for this epoch : 2.74766\n",
            "--------------------\n",
            "     2022-08-22 13:58:15 : EPOCH 3/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 2.16862\n",
            "xy_coord training loss for this epoch : 0.06630\n",
            "wh_sizes training loss for this epoch : 0.00240\n",
            "confidence with object training loss for this epoch : 0.91823\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.90692\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 2.08956\n",
            "xy_coord training loss for this epoch : 0.05649\n",
            "wh_sizes training loss for this epoch : 0.00382\n",
            "confidence with object training loss for this epoch : 0.98436\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.80362\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 1.83152\n",
            "xy_coord training loss for this epoch : 0.01833\n",
            "wh_sizes training loss for this epoch : 0.00200\n",
            "confidence with object training loss for this epoch : 0.86759\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.86228\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 4.50062\n",
            "xy_coord training loss for this epoch : 0.48347\n",
            "wh_sizes training loss for this epoch : 0.02481\n",
            "confidence with object training loss for this epoch : 0.94908\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 1.01014\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 4.65154\n",
            "xy_coord training loss for this epoch : 0.19989\n",
            "wh_sizes training loss for this epoch : 0.26689\n",
            "confidence with object training loss for this epoch : 0.96275\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 1.35494\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 2.23414\n",
            "xy_coord training loss for this epoch : 0.04225\n",
            "wh_sizes training loss for this epoch : 0.04642\n",
            "confidence with object training loss for this epoch : 0.94429\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.84648\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 2.15604\n",
            "xy_coord training loss for this epoch : 0.08551\n",
            "wh_sizes training loss for this epoch : 0.00184\n",
            "confidence with object training loss for this epoch : 0.86946\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.84981\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 2.06687\n",
            "xy_coord training loss for this epoch : 0.06433\n",
            "wh_sizes training loss for this epoch : 0.00144\n",
            "confidence with object training loss for this epoch : 0.92403\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.81398\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 1.91349\n",
            "xy_coord training loss for this epoch : 0.02461\n",
            "wh_sizes training loss for this epoch : 0.00502\n",
            "confidence with object training loss for this epoch : 0.89397\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.87138\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 2.17079\n",
            "xy_coord training loss for this epoch : 0.06089\n",
            "wh_sizes training loss for this epoch : 0.00783\n",
            "confidence with object training loss for this epoch : 0.92126\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.90592\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 1.89862\n",
            "xy_coord training loss for this epoch : 0.02811\n",
            "wh_sizes training loss for this epoch : 0.00269\n",
            "confidence with object training loss for this epoch : 0.84460\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.90004\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:26.548354\n",
            "Mean training loss for this epoch : 2.61765\n",
            "--------------------\n",
            "     2022-08-22 13:58:15 : EPOCH 4/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 2.23653\n",
            "xy_coord training loss for this epoch : 0.08733\n",
            "wh_sizes training loss for this epoch : 0.00282\n",
            "confidence with object training loss for this epoch : 0.92576\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.86003\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 1.73354\n",
            "xy_coord training loss for this epoch : 0.01384\n",
            "wh_sizes training loss for this epoch : 0.00217\n",
            "confidence with object training loss for this epoch : 0.84425\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.80924\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 1.97897\n",
            "xy_coord training loss for this epoch : 0.02935\n",
            "wh_sizes training loss for this epoch : 0.00163\n",
            "confidence with object training loss for this epoch : 0.93646\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.88762\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 1.75150\n",
            "xy_coord training loss for this epoch : 0.04533\n",
            "wh_sizes training loss for this epoch : 0.00681\n",
            "confidence with object training loss for this epoch : 0.66514\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.82564\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 1.68269\n",
            "xy_coord training loss for this epoch : 0.02805\n",
            "wh_sizes training loss for this epoch : 0.00340\n",
            "confidence with object training loss for this epoch : 0.68213\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.84332\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 1.98634\n",
            "xy_coord training loss for this epoch : 0.02560\n",
            "wh_sizes training loss for this epoch : 0.01020\n",
            "confidence with object training loss for this epoch : 0.87103\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.93633\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 1.80989\n",
            "xy_coord training loss for this epoch : 0.02738\n",
            "wh_sizes training loss for this epoch : 0.00143\n",
            "confidence with object training loss for this epoch : 0.85558\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.81026\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 1.76580\n",
            "xy_coord training loss for this epoch : 0.02244\n",
            "wh_sizes training loss for this epoch : 0.00188\n",
            "confidence with object training loss for this epoch : 0.84132\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.80289\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 1.82869\n",
            "xy_coord training loss for this epoch : 0.03563\n",
            "wh_sizes training loss for this epoch : 0.00204\n",
            "confidence with object training loss for this epoch : 0.83596\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.80441\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 1.75504\n",
            "xy_coord training loss for this epoch : 0.03333\n",
            "wh_sizes training loss for this epoch : 0.00473\n",
            "confidence with object training loss for this epoch : 0.71018\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.85453\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 1.49430\n",
            "xy_coord training loss for this epoch : 0.01072\n",
            "wh_sizes training loss for this epoch : 0.00291\n",
            "confidence with object training loss for this epoch : 0.65355\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.77258\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:27.570610\n",
            "Mean training loss for this epoch : 2.03002\n",
            "--------------------\n",
            "     2022-08-22 13:58:15 : EPOCH 5/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 1.44319\n",
            "xy_coord training loss for this epoch : 0.01151\n",
            "wh_sizes training loss for this epoch : 0.00307\n",
            "confidence with object training loss for this epoch : 0.57566\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.79460\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 2.11812\n",
            "xy_coord training loss for this epoch : 0.10012\n",
            "wh_sizes training loss for this epoch : 0.01697\n",
            "confidence with object training loss for this epoch : 0.66305\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.86962\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 1.99917\n",
            "xy_coord training loss for this epoch : 0.08985\n",
            "wh_sizes training loss for this epoch : 0.00539\n",
            "confidence with object training loss for this epoch : 0.78475\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.73823\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 1.57769\n",
            "xy_coord training loss for this epoch : 0.01376\n",
            "wh_sizes training loss for this epoch : 0.01328\n",
            "confidence with object training loss for this epoch : 0.68620\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.75630\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 1.48274\n",
            "xy_coord training loss for this epoch : 0.01050\n",
            "wh_sizes training loss for this epoch : 0.00365\n",
            "confidence with object training loss for this epoch : 0.62975\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.78223\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 1.48065\n",
            "xy_coord training loss for this epoch : 0.03715\n",
            "wh_sizes training loss for this epoch : 0.00470\n",
            "confidence with object training loss for this epoch : 0.43631\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.83508\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 1.42958\n",
            "xy_coord training loss for this epoch : 0.02396\n",
            "wh_sizes training loss for this epoch : 0.00428\n",
            "confidence with object training loss for this epoch : 0.43212\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.85626\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 1.26096\n",
            "xy_coord training loss for this epoch : 0.01232\n",
            "wh_sizes training loss for this epoch : 0.00504\n",
            "confidence with object training loss for this epoch : 0.36764\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.80656\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 1.22516\n",
            "xy_coord training loss for this epoch : 0.02192\n",
            "wh_sizes training loss for this epoch : 0.00495\n",
            "confidence with object training loss for this epoch : 0.31727\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.77353\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 1.10790\n",
            "xy_coord training loss for this epoch : 0.00448\n",
            "wh_sizes training loss for this epoch : 0.00353\n",
            "confidence with object training loss for this epoch : 0.30855\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.75932\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 1.34018\n",
            "xy_coord training loss for this epoch : 0.00523\n",
            "wh_sizes training loss for this epoch : 0.00319\n",
            "confidence with object training loss for this epoch : 0.59411\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.70397\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:29.322332\n",
            "Mean training loss for this epoch : 1.52491\n",
            "--------------------\n",
            "     2022-08-22 13:58:15 : EPOCH 6/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 1.10713\n",
            "xy_coord training loss for this epoch : 0.00495\n",
            "wh_sizes training loss for this epoch : 0.00382\n",
            "confidence with object training loss for this epoch : 0.31265\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.75061\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 1.06072\n",
            "xy_coord training loss for this epoch : 0.00133\n",
            "wh_sizes training loss for this epoch : 0.00396\n",
            "confidence with object training loss for this epoch : 0.27948\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.75476\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 1.49586\n",
            "xy_coord training loss for this epoch : 0.00930\n",
            "wh_sizes training loss for this epoch : 0.01310\n",
            "confidence with object training loss for this epoch : 0.60242\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.78143\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 1.31097\n",
            "xy_coord training loss for this epoch : 0.00728\n",
            "wh_sizes training loss for this epoch : 0.00547\n",
            "confidence with object training loss for this epoch : 0.46754\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.77967\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 1.02025\n",
            "xy_coord training loss for this epoch : 0.00127\n",
            "wh_sizes training loss for this epoch : 0.00398\n",
            "confidence with object training loss for this epoch : 0.22742\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.76660\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.98280\n",
            "xy_coord training loss for this epoch : 0.00150\n",
            "wh_sizes training loss for this epoch : 0.00841\n",
            "confidence with object training loss for this epoch : 0.21900\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.71423\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.94527\n",
            "xy_coord training loss for this epoch : 0.00271\n",
            "wh_sizes training loss for this epoch : 0.00369\n",
            "confidence with object training loss for this epoch : 0.20739\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.70591\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.89494\n",
            "xy_coord training loss for this epoch : 0.00181\n",
            "wh_sizes training loss for this epoch : 0.00258\n",
            "confidence with object training loss for this epoch : 0.15787\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.71510\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.85590\n",
            "xy_coord training loss for this epoch : 0.00121\n",
            "wh_sizes training loss for this epoch : 0.00331\n",
            "confidence with object training loss for this epoch : 0.18082\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.65246\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.94990\n",
            "xy_coord training loss for this epoch : 0.01315\n",
            "wh_sizes training loss for this epoch : 0.00293\n",
            "confidence with object training loss for this epoch : 0.19999\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.66954\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.89446\n",
            "xy_coord training loss for this epoch : 0.00207\n",
            "wh_sizes training loss for this epoch : 0.00295\n",
            "confidence with object training loss for this epoch : 0.15668\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.71271\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:27.125623\n",
            "Mean training loss for this epoch : 1.16071\n",
            "--------------------\n",
            "     2022-08-22 13:58:15 : EPOCH 7/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.90716\n",
            "xy_coord training loss for this epoch : 0.00120\n",
            "wh_sizes training loss for this epoch : 0.00289\n",
            "confidence with object training loss for this epoch : 0.19195\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.69476\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.83144\n",
            "xy_coord training loss for this epoch : 0.00036\n",
            "wh_sizes training loss for this epoch : 0.00186\n",
            "confidence with object training loss for this epoch : 0.14188\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.67847\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.80876\n",
            "xy_coord training loss for this epoch : 0.00096\n",
            "wh_sizes training loss for this epoch : 0.00279\n",
            "confidence with object training loss for this epoch : 0.16562\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.62439\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.77753\n",
            "xy_coord training loss for this epoch : 0.00028\n",
            "wh_sizes training loss for this epoch : 0.00192\n",
            "confidence with object training loss for this epoch : 0.14205\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.62444\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.83864\n",
            "xy_coord training loss for this epoch : 0.00023\n",
            "wh_sizes training loss for this epoch : 0.00244\n",
            "confidence with object training loss for this epoch : 0.17888\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.64641\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.76105\n",
            "xy_coord training loss for this epoch : 0.00031\n",
            "wh_sizes training loss for this epoch : 0.00188\n",
            "confidence with object training loss for this epoch : 0.11114\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.63895\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.70082\n",
            "xy_coord training loss for this epoch : 0.00019\n",
            "wh_sizes training loss for this epoch : 0.00213\n",
            "confidence with object training loss for this epoch : 0.07704\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.61219\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.68174\n",
            "xy_coord training loss for this epoch : 0.00020\n",
            "wh_sizes training loss for this epoch : 0.00225\n",
            "confidence with object training loss for this epoch : 0.10973\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.55978\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.59955\n",
            "xy_coord training loss for this epoch : 0.00008\n",
            "wh_sizes training loss for this epoch : 0.00187\n",
            "confidence with object training loss for this epoch : 0.06923\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.52056\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.67902\n",
            "xy_coord training loss for this epoch : 0.00010\n",
            "wh_sizes training loss for this epoch : 0.00122\n",
            "confidence with object training loss for this epoch : 0.09278\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.57966\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.63953\n",
            "xy_coord training loss for this epoch : 0.00012\n",
            "wh_sizes training loss for this epoch : 0.00173\n",
            "confidence with object training loss for this epoch : 0.11061\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.51967\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:29.809121\n",
            "Mean training loss for this epoch : 0.76652\n",
            "--------------------\n",
            "     2022-08-22 13:58:15 : EPOCH 8/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.63104\n",
            "xy_coord training loss for this epoch : 0.00016\n",
            "wh_sizes training loss for this epoch : 0.00222\n",
            "confidence with object training loss for this epoch : 0.12027\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.49891\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.59632\n",
            "xy_coord training loss for this epoch : 0.00007\n",
            "wh_sizes training loss for this epoch : 0.00170\n",
            "confidence with object training loss for this epoch : 0.06296\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.52452\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.58392\n",
            "xy_coord training loss for this epoch : 0.00009\n",
            "wh_sizes training loss for this epoch : 0.00220\n",
            "confidence with object training loss for this epoch : 0.08396\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.48849\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.58901\n",
            "xy_coord training loss for this epoch : 0.00008\n",
            "wh_sizes training loss for this epoch : 0.00224\n",
            "confidence with object training loss for this epoch : 0.07357\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.50386\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.43418\n",
            "xy_coord training loss for this epoch : 0.00008\n",
            "wh_sizes training loss for this epoch : 0.00200\n",
            "confidence with object training loss for this epoch : 0.07185\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.35191\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.46267\n",
            "xy_coord training loss for this epoch : 0.00008\n",
            "wh_sizes training loss for this epoch : 0.00225\n",
            "confidence with object training loss for this epoch : 0.07485\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.37617\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.40798\n",
            "xy_coord training loss for this epoch : 0.00007\n",
            "wh_sizes training loss for this epoch : 0.00194\n",
            "confidence with object training loss for this epoch : 0.07488\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.32303\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.39157\n",
            "xy_coord training loss for this epoch : 0.00006\n",
            "wh_sizes training loss for this epoch : 0.00237\n",
            "confidence with object training loss for this epoch : 0.04124\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.33819\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.35282\n",
            "xy_coord training loss for this epoch : 0.00009\n",
            "wh_sizes training loss for this epoch : 0.00222\n",
            "confidence with object training loss for this epoch : 0.09744\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.24386\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.36357\n",
            "xy_coord training loss for this epoch : 0.00009\n",
            "wh_sizes training loss for this epoch : 0.00324\n",
            "confidence with object training loss for this epoch : 0.08355\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.26333\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.32063\n",
            "xy_coord training loss for this epoch : 0.00015\n",
            "wh_sizes training loss for this epoch : 0.00239\n",
            "confidence with object training loss for this epoch : 0.13215\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.17577\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:30.234000\n",
            "Mean training loss for this epoch : 0.47317\n",
            "--------------------\n",
            "     2022-08-22 13:58:15 : EPOCH 9/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.38927\n",
            "xy_coord training loss for this epoch : 0.00011\n",
            "wh_sizes training loss for this epoch : 0.00257\n",
            "confidence with object training loss for this epoch : 0.10295\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.27288\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.38931\n",
            "xy_coord training loss for this epoch : 0.00008\n",
            "wh_sizes training loss for this epoch : 0.00249\n",
            "confidence with object training loss for this epoch : 0.06018\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.31630\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.27616\n",
            "xy_coord training loss for this epoch : 0.00008\n",
            "wh_sizes training loss for this epoch : 0.00200\n",
            "confidence with object training loss for this epoch : 0.07497\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.19079\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.27552\n",
            "xy_coord training loss for this epoch : 0.00011\n",
            "wh_sizes training loss for this epoch : 0.00203\n",
            "confidence with object training loss for this epoch : 0.08545\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.17942\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.17366\n",
            "xy_coord training loss for this epoch : 0.00007\n",
            "wh_sizes training loss for this epoch : 0.00245\n",
            "confidence with object training loss for this epoch : 0.05740\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.10364\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.20144\n",
            "xy_coord training loss for this epoch : 0.00006\n",
            "wh_sizes training loss for this epoch : 0.00228\n",
            "confidence with object training loss for this epoch : 0.04972\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.14004\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.21037\n",
            "xy_coord training loss for this epoch : 0.00007\n",
            "wh_sizes training loss for this epoch : 0.00197\n",
            "confidence with object training loss for this epoch : 0.05196\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.14823\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.22374\n",
            "xy_coord training loss for this epoch : 0.00007\n",
            "wh_sizes training loss for this epoch : 0.00243\n",
            "confidence with object training loss for this epoch : 0.06762\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.14363\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.20809\n",
            "xy_coord training loss for this epoch : 0.00009\n",
            "wh_sizes training loss for this epoch : 0.00207\n",
            "confidence with object training loss for this epoch : 0.07887\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.11846\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.14325\n",
            "xy_coord training loss for this epoch : 0.00006\n",
            "wh_sizes training loss for this epoch : 0.00186\n",
            "confidence with object training loss for this epoch : 0.04420\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.08948\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.15620\n",
            "xy_coord training loss for this epoch : 0.00006\n",
            "wh_sizes training loss for this epoch : 0.00208\n",
            "confidence with object training loss for this epoch : 0.05991\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.08561\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:29.175892\n",
            "Mean training loss for this epoch : 0.24964\n",
            "--------------------\n",
            "     2022-08-22 13:58:15 : EPOCH 10/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.15950\n",
            "xy_coord training loss for this epoch : 0.00008\n",
            "wh_sizes training loss for this epoch : 0.00241\n",
            "confidence with object training loss for this epoch : 0.06458\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.08250\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.16167\n",
            "xy_coord training loss for this epoch : 0.00006\n",
            "wh_sizes training loss for this epoch : 0.00209\n",
            "confidence with object training loss for this epoch : 0.05189\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.09902\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.23218\n",
            "xy_coord training loss for this epoch : 0.00009\n",
            "wh_sizes training loss for this epoch : 0.00223\n",
            "confidence with object training loss for this epoch : 0.09670\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.12390\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.15482\n",
            "xy_coord training loss for this epoch : 0.00007\n",
            "wh_sizes training loss for this epoch : 0.00191\n",
            "confidence with object training loss for this epoch : 0.05875\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.08617\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.17573\n",
            "xy_coord training loss for this epoch : 0.00007\n",
            "wh_sizes training loss for this epoch : 0.00206\n",
            "confidence with object training loss for this epoch : 0.05375\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.11134\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.20816\n",
            "xy_coord training loss for this epoch : 0.00008\n",
            "wh_sizes training loss for this epoch : 0.00147\n",
            "confidence with object training loss for this epoch : 0.08054\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.11988\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.20292\n",
            "xy_coord training loss for this epoch : 0.00007\n",
            "wh_sizes training loss for this epoch : 0.00173\n",
            "confidence with object training loss for this epoch : 0.06772\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.12619\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.15425\n",
            "xy_coord training loss for this epoch : 0.00008\n",
            "wh_sizes training loss for this epoch : 0.00166\n",
            "confidence with object training loss for this epoch : 0.07649\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.06908\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.17765\n",
            "xy_coord training loss for this epoch : 0.00007\n",
            "wh_sizes training loss for this epoch : 0.00139\n",
            "confidence with object training loss for this epoch : 0.07287\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.09746\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.13171\n",
            "xy_coord training loss for this epoch : 0.00005\n",
            "wh_sizes training loss for this epoch : 0.00200\n",
            "confidence with object training loss for this epoch : 0.04459\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.07686\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.16629\n",
            "xy_coord training loss for this epoch : 0.00006\n",
            "wh_sizes training loss for this epoch : 0.00177\n",
            "confidence with object training loss for this epoch : 0.07193\n",
            "confidence without object training loss for this epoch : 0.00000\n",
            "class proba training loss for this epoch : 0.08521\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:27.822781\n",
            "Mean training loss for this epoch : 0.17457\n"
          ]
        }
      ],
      "source": [
        "delta_time = datetime.timedelta(hours=1)\n",
        "timezone = datetime.timezone(offset=delta_time)\n",
        "\n",
        "t = datetime.datetime.now(tz=timezone)\n",
        "str_t = '{:%Y-%m-%d %H:%M:%S}'.format(t)\n",
        "print(f\"[START] : {str_t} :\")\n",
        "print(f\"[Training on] : {str(device).upper()}\")\n",
        "\n",
        "EPOCHS = 10\n",
        "size_grid = 6\n",
        "batch_loss_list = []\n",
        "model_MNIST = model_MNIST.to(device)\n",
        "\n",
        "for epoch in range(EPOCHS) : \n",
        "    begin_time = timer()\n",
        "    epochs_loss = 0.\n",
        "    \n",
        "    print(\"-\"*20)\n",
        "    str_t = '{:%Y-%m-%d %H:%M:%S}'.format(t)\n",
        "    print(\" \"*5 + f\"{str_t} : EPOCH {epoch+1}/{EPOCHS}\")\n",
        "    print(\"-\"*20)\n",
        "\n",
        "    model_MNIST.train()\n",
        "    for batch, (img, labels, bbox_true) in enumerate(training_dataset):\n",
        "        loss = 0\n",
        "        begin_batch_time = timer()\n",
        "        img, labels, bbox_true = img.to(device), labels.to(device), bbox_true.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        bbox_preds, label_preds = model_MNIST(img)\n",
        "        \n",
        "        bbox_true_6x6 = bbox2Tensor(bbox_true.to('cpu')).to(device)\n",
        "        \n",
        "        ############### REFLECHIR ICI\n",
        "        losses, loss = loss_yolo(bbox_preds, bbox_true_6x6, label_preds, labels)\n",
        "\n",
        "        loss = torch.sum(loss)/len(img)\n",
        "        loss.backward()\n",
        "                \n",
        "        optimizer.step()\n",
        "        \n",
        "        ######### print part #######################\n",
        "        current_loss = loss.item()\n",
        "        batch_loss_list.append(current_loss)\n",
        "        epochs_loss = epochs_loss + current_loss\n",
        "\n",
        "        if batch+1 <= len_training_ds//BATCH_SIZE:\n",
        "            current_training_sample = (batch+1)*BATCH_SIZE\n",
        "        else:\n",
        "            current_training_sample = (batch)*BATCH_SIZE + len_training_ds%BATCH_SIZE\n",
        "        \n",
        "        if (batch) == 0 or (batch+1)%100 == 0 or batch == len_training_ds//BATCH_SIZE:\n",
        "            print(f\" --- Image : {current_training_sample}/{len_training_ds}\",\\\n",
        "                    f\" : loss = {current_loss:.5f}\")\n",
        "            print(f\"xy_coord training loss for this epoch : {torch.sum(losses['loss_xy']) / len(img):.5f}\")\n",
        "            print(f\"wh_sizes training loss for this epoch : {torch.sum(losses['loss_wh']) / len(img):.5f}\")\n",
        "            print(f\"confidence with object training loss for this epoch : {torch.sum(losses['loss_conf_obj']) / len(img):.5f}\")\n",
        "            print(f\"confidence without object training loss for this epoch : {torch.sum(losses['loss_conf_noobj']) / len(img):.5f}\")\n",
        "            print(f\"class proba training loss for this epoch : {torch.sum(losses['loss_class']) / len(img):.5f}\")\n",
        "            print('\\n')\n",
        "            if batch == (len_training_ds//BATCH_SIZE):\n",
        "                print(f\"Total elapsed time for training : {datetime.timedelta(seconds=timer()-begin_time)}\")\n",
        "                print(f\"Mean training loss for this epoch : {epochs_loss / len(training_dataset):.5f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for k in validation_dataset:\n",
        "    break"
      ],
      "metadata": {
        "id": "-DnZNXlRRfhx"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8R6iTYLLh2y",
        "outputId": "a7616703-06a3-4a09-c4c5-55224c8e837f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10])"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "k[1][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "kvNPOMbPLh2y"
      },
      "outputs": [],
      "source": [
        "model_MNIST.eval()\n",
        "with torch.no_grad():\n",
        "    bbox_preds, label_preds = model_MNIST(k[0][0].unsqueeze(0).to(device))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.argmax(k[1][0])"
      ],
      "metadata": {
        "id": "-wm01tZ6w9jX",
        "outputId": "fa57b020-8fbd-46c4-f75c-140d968b1c49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7)"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1ynRt-yfzP0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbox_preds.shape"
      ],
      "metadata": {
        "id": "b-vHeHG0zPvH",
        "outputId": "7f471e66-7684-44c0-c1e3-e415aec95697",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 6, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bbox2Tensor(bbox_preds).shape"
      ],
      "metadata": {
        "id": "V3MQZCeVxWIU",
        "outputId": "d2bf256a-b6f8-49d0-e045-566f5fbb08e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-134-928923bfb485>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbbox2Tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-85-08dbb7aba449>\u001b[0m in \u001b[0;36mbbox2Tensor\u001b[0;34m(bbox, S, sizeHW)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mcell_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msizeHW\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mxc_rcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myc_rcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mxc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxc_rcell\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcell_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcell_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxc_rcell\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcell_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0myc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myc_rcell\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcell_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcell_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0myc_rcell\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcell_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(6):\n",
        "    for l in range(6):\n",
        "        print(torch.argmax(label_preds[0][t,l]))"
      ],
      "metadata": {
        "id": "2pMPDWAIxYuo",
        "outputId": "aae2c3e3-0ef6-4fa9-a5de-5cc7f51e5f3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0, device='cuda:0')\n",
            "tensor(5, device='cuda:0')\n",
            "tensor(9, device='cuda:0')\n",
            "tensor(3, device='cuda:0')\n",
            "tensor(2, device='cuda:0')\n",
            "tensor(5, device='cuda:0')\n",
            "tensor(6, device='cuda:0')\n",
            "tensor(9, device='cuda:0')\n",
            "tensor(9, device='cuda:0')\n",
            "tensor(9, device='cuda:0')\n",
            "tensor(9, device='cuda:0')\n",
            "tensor(7, device='cuda:0')\n",
            "tensor(6, device='cuda:0')\n",
            "tensor(9, device='cuda:0')\n",
            "tensor(9, device='cuda:0')\n",
            "tensor(9, device='cuda:0')\n",
            "tensor(7, device='cuda:0')\n",
            "tensor(2, device='cuda:0')\n",
            "tensor(7, device='cuda:0')\n",
            "tensor(7, device='cuda:0')\n",
            "tensor(7, device='cuda:0')\n",
            "tensor(7, device='cuda:0')\n",
            "tensor(7, device='cuda:0')\n",
            "tensor(4, device='cuda:0')\n",
            "tensor(6, device='cuda:0')\n",
            "tensor(7, device='cuda:0')\n",
            "tensor(7, device='cuda:0')\n",
            "tensor(7, device='cuda:0')\n",
            "tensor(7, device='cuda:0')\n",
            "tensor(3, device='cuda:0')\n",
            "tensor(0, device='cuda:0')\n",
            "tensor(2, device='cuda:0')\n",
            "tensor(2, device='cuda:0')\n",
            "tensor(6, device='cuda:0')\n",
            "tensor(3, device='cuda:0')\n",
            "tensor(4, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ppfy3fmmyIHi",
        "outputId": "cfbbdfcf-81f3-4e70-b348-f19d5ac8efb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 4.0078e-06,  2.7483e-06,  6.8029e-07, -2.3788e-06,  6.7194e-07,\n",
              "        -3.8964e-07,  7.3100e-07,  1.6381e-06,  1.6896e-06, -9.6789e-07],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "v7cqKPKgyyC6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "YOLO_MNIST_Localization.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "bfc2e043f4a41bbce066f7921047bdcfe8769546c7198e4df6482fd90de274f1"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e78f76aad3ba466aa76e6000e06974d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_81ec169e88a34a51a0f2f25030f6f336",
              "IPY_MODEL_c03d2a3cfa0e4d958c7ff255113001ec",
              "IPY_MODEL_1629fc5b6d644e11b6fc895a39553807"
            ],
            "layout": "IPY_MODEL_4bfeea2a09374739a89ce1b774e7d647",
            "tabbable": null,
            "tooltip": null
          }
        },
        "81ec169e88a34a51a0f2f25030f6f336": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_2e584742a69646678c3995da54a01902",
            "placeholder": "​",
            "style": "IPY_MODEL_514b097815e941b09d2c5660b53403c0",
            "tabbable": null,
            "tooltip": null,
            "value": "100%"
          }
        },
        "c03d2a3cfa0e4d958c7ff255113001ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_cd1af987f91c4309b138444ba940930f",
            "max": 9912422,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_06287fcbe81242178cbb40ccb2677922",
            "tabbable": null,
            "tooltip": null,
            "value": 9912422
          }
        },
        "1629fc5b6d644e11b6fc895a39553807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_7d96a7494e8544b282738d8d59e4e1b2",
            "placeholder": "​",
            "style": "IPY_MODEL_748604520514468da3d8d80759169bd6",
            "tabbable": null,
            "tooltip": null,
            "value": " 9912422/9912422 [00:00&lt;00:00, 71397879.77it/s]"
          }
        },
        "4bfeea2a09374739a89ce1b774e7d647": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e584742a69646678c3995da54a01902": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "514b097815e941b09d2c5660b53403c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "cd1af987f91c4309b138444ba940930f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06287fcbe81242178cbb40ccb2677922": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d96a7494e8544b282738d8d59e4e1b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "748604520514468da3d8d80759169bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "c04dbe42a37f4dbda9d22a666fcc19ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_edd52f2e07624bf792d3d4a320923247",
              "IPY_MODEL_87a42523125345bbb247a4dca8110f45",
              "IPY_MODEL_7302a680436d4d2597689f3e3814a657"
            ],
            "layout": "IPY_MODEL_aa929078b5204f4fa0d3f108becd9512",
            "tabbable": null,
            "tooltip": null
          }
        },
        "edd52f2e07624bf792d3d4a320923247": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_9b893464e51e47f8aec6a3b334c5029a",
            "placeholder": "​",
            "style": "IPY_MODEL_c6a5a8fb183f4b41b78a368485f52129",
            "tabbable": null,
            "tooltip": null,
            "value": "100%"
          }
        },
        "87a42523125345bbb247a4dca8110f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_30839291036944ad9dee8af44ada9e9d",
            "max": 28881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f8a0055577544298d16cb7f135c9241",
            "tabbable": null,
            "tooltip": null,
            "value": 28881
          }
        },
        "7302a680436d4d2597689f3e3814a657": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_04abac823bf148228b4d7943ddb50412",
            "placeholder": "​",
            "style": "IPY_MODEL_3e4c9699fe794e079957da4aaf86b2a2",
            "tabbable": null,
            "tooltip": null,
            "value": " 28881/28881 [00:00&lt;00:00, 928847.86it/s]"
          }
        },
        "aa929078b5204f4fa0d3f108becd9512": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b893464e51e47f8aec6a3b334c5029a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6a5a8fb183f4b41b78a368485f52129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "30839291036944ad9dee8af44ada9e9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f8a0055577544298d16cb7f135c9241": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "04abac823bf148228b4d7943ddb50412": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e4c9699fe794e079957da4aaf86b2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "078506e8af2843b5b9191f9405f94372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a345eda307f45179703d899379f0f2e",
              "IPY_MODEL_0a5a381ddeb54da7908f91c294232855",
              "IPY_MODEL_d146d83e45854cbd88c5d52babc3fdf2"
            ],
            "layout": "IPY_MODEL_da792f1d25784806a025d334e40848b4",
            "tabbable": null,
            "tooltip": null
          }
        },
        "2a345eda307f45179703d899379f0f2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_642676f86c744e96996b182c32f4c2f6",
            "placeholder": "​",
            "style": "IPY_MODEL_f52cb61ab96e4ae180a49a9472b9bf4d",
            "tabbable": null,
            "tooltip": null,
            "value": "100%"
          }
        },
        "0a5a381ddeb54da7908f91c294232855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_003b0852f9464715b79575347e347d9b",
            "max": 1648877,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3e2709b880240879f764319496ec40c",
            "tabbable": null,
            "tooltip": null,
            "value": 1648877
          }
        },
        "d146d83e45854cbd88c5d52babc3fdf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_bef540a945ed416cae626a5d534310d0",
            "placeholder": "​",
            "style": "IPY_MODEL_6f74edff5c1a4baabb64d5d648038f76",
            "tabbable": null,
            "tooltip": null,
            "value": " 1648877/1648877 [00:00&lt;00:00, 39221297.55it/s]"
          }
        },
        "da792f1d25784806a025d334e40848b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "642676f86c744e96996b182c32f4c2f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f52cb61ab96e4ae180a49a9472b9bf4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "003b0852f9464715b79575347e347d9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3e2709b880240879f764319496ec40c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bef540a945ed416cae626a5d534310d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f74edff5c1a4baabb64d5d648038f76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "1d674581dac64a3aaa29bebcbb5b0e34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22d9e6194f7c4f919073fc7af0f03aef",
              "IPY_MODEL_eba57518d4e34a0193136f85aaee8af0",
              "IPY_MODEL_5e2a9f63558d43d29aef14f042b6bc46"
            ],
            "layout": "IPY_MODEL_e805ce371c824e1d905d13cb4b399d9e",
            "tabbable": null,
            "tooltip": null
          }
        },
        "22d9e6194f7c4f919073fc7af0f03aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_a581c8d44c52462fad681849910f0dfe",
            "placeholder": "​",
            "style": "IPY_MODEL_efb9d5fc80c04baa853cd7375e66e96f",
            "tabbable": null,
            "tooltip": null,
            "value": "100%"
          }
        },
        "eba57518d4e34a0193136f85aaee8af0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_1f46b04f6b0b489896739ddc33734019",
            "max": 4542,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52bf0e0cf0d84be192c9452c0e13756d",
            "tabbable": null,
            "tooltip": null,
            "value": 4542
          }
        },
        "5e2a9f63558d43d29aef14f042b6bc46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_e9d285ccbc214a76bd84e2560431d608",
            "placeholder": "​",
            "style": "IPY_MODEL_04ff799305da40e5bd9af7259b933d56",
            "tabbable": null,
            "tooltip": null,
            "value": " 4542/4542 [00:00&lt;00:00, 146815.83it/s]"
          }
        },
        "e805ce371c824e1d905d13cb4b399d9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a581c8d44c52462fad681849910f0dfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efb9d5fc80c04baa853cd7375e66e96f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "1f46b04f6b0b489896739ddc33734019": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52bf0e0cf0d84be192c9452c0e13756d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e9d285ccbc214a76bd84e2560431d608": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04ff799305da40e5bd9af7259b933d56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}