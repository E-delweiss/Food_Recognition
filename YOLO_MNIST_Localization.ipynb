{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThOpaque/Food_Recognition/blob/main/YOLO_MNIST_Localization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAHeh1BRufX4",
        "outputId": "345cb88b-26a2-4a5b-9468-5f767daa550f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.7.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "import os, time,datetime\n",
        "from timeit import default_timer as timer\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "%pip install torchinfo;\n",
        "%pip install torchmetrics;\n",
        "from torchmetrics import MeanSquaredError;\n",
        "from torchinfo import summary;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "mgoDp_sPvIwN"
      },
      "outputs": [],
      "source": [
        "im_width = 75\n",
        "im_height = 75\n",
        "use_normalized_coordinates = True\n",
        "\n",
        "def draw_ONE_bounding_box_on_image(image, ymin:int, xmin:int, ymax:int, xmax:int, \n",
        "                               color:str='red', thickness:int=1, display_str:bool=None, \n",
        "                               use_normalized_coordinates:bool=True):\n",
        "  \"\"\"Adds a bounding box to an image.\n",
        "  Bounding box coordinates can be specified in either absolute (pixel) or\n",
        "  normalized coordinates by setting the use_normalized_coordinates argument.\n",
        "  \n",
        "  Args:\n",
        "    image: a PIL.Image object.\n",
        "    ymin: ymin of bounding box.\n",
        "    xmin: xmin of bounding box.\n",
        "    ymax: ymax of bounding box.\n",
        "    xmax: xmax of bounding box.\n",
        "    color: color to draw bounding box. Default is red.\n",
        "    thickness: line thickness. Default value is 4.\n",
        "    display_str_list: string to display in box\n",
        "    use_normalized_coordinates: If True (default), treat coordinates\n",
        "      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n",
        "      coordinates as absolute.\n",
        "  \"\"\"\n",
        "  draw = PIL.ImageDraw.Draw(image)\n",
        "  im_width, im_height = image.size\n",
        "  \n",
        "  if use_normalized_coordinates:\n",
        "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
        "                                  ymin * im_height, ymax * im_height)\n",
        "  else:\n",
        "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
        "  draw.line([(left, top), (left, bottom), (right, bottom),\n",
        "             (right, top), (left, top)], width=thickness, fill=color)\n",
        "\n",
        "\n",
        "\n",
        "def draw_bounding_boxes_on_image(image, boxes:np.ndarray, color:list=[], \n",
        "                                 thickness:int=1, display_str_list:tuple=()):\n",
        "  \"\"\"Draws bounding boxes on image.\n",
        "\n",
        "  Args:\n",
        "    image: a PIL.Image object.\n",
        "    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
        "           The coordinates are in normalized format between [0, 1].\n",
        "    color: color to draw bounding box. Default is red.\n",
        "    thickness: line thickness. Default value is 4.\n",
        "    display_str_list: a list of strings for each bounding box.\n",
        "                           \n",
        "  Raises:\n",
        "    ValueError: if boxes is not a [N, 4] array\n",
        "  \"\"\"\n",
        "  boxes_shape = boxes.shape\n",
        "  if not boxes_shape:\n",
        "    return\n",
        "  if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n",
        "    raise ValueError('Input must be of size [N, 4]')\n",
        "  \n",
        "  for i in range(boxes_shape[0]):\n",
        "    draw_ONE_bounding_box_on_image(image, \n",
        "                                   boxes[i, 1], boxes[i, 0], \n",
        "                                   boxes[i, 3], boxes[i, 2], \n",
        "                                   color[i], thickness, display_str_list[i])\n",
        "\n",
        "\n",
        "def draw_bounding_boxes_on_image_array(image:np.ndarray, boxes:np.ndarray, color:list=[], \n",
        "                                       thickness:int=1, display_str_list:tuple=()):\n",
        "  \"\"\"Draws bounding boxes on image (numpy array).\n",
        "\n",
        "  Args:\n",
        "    image: a numpy array object.\n",
        "    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
        "           The coordinates are in normalized format between [0, 1].\n",
        "    color: color to draw bounding box. Default is red.\n",
        "    thickness: line thickness. Default value is 4.\n",
        "    display_str_list_list: a list of strings for each bounding box.\n",
        "  Raises:\n",
        "    ValueError: if boxes is not a [N, 4] array\n",
        "  \"\"\"\n",
        "  image_pil = PIL.Image.fromarray(image)\n",
        "  rgbimg = PIL.Image.new(\"RGBA\", image_pil.size)\n",
        "  rgbimg.paste(image_pil)\n",
        "  draw_bounding_boxes_on_image(rgbimg, boxes, color, thickness, display_str_list)\n",
        "  return np.array(rgbimg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "IhuDuRbavY9G"
      },
      "outputs": [],
      "source": [
        "############### Matplotlib config\n",
        "plt.rc('image', cmap='gray')\n",
        "plt.rc('grid', linewidth=0)\n",
        "plt.rc('xtick', top=False, bottom=False, labelsize='large')\n",
        "plt.rc('ytick', left=False, right=False, labelsize='large')\n",
        "plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')\n",
        "plt.rc('text', color='a8151a')\n",
        "plt.rc('figure', facecolor='F0F0F0')# Matplotlib fonts\n",
        "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), \"mpl-data/fonts/ttf\")\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "mc_ExKjv7VZr"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "def display_digits_with_boxes(digits, predictions, labels, pred_bboxes, bboxes, iou, title):\n",
        "  \"\"\"Utility to display a row of digits with their predictions.\n",
        "\n",
        "  Args:\n",
        "    digits : np.ndarray of shape (N,75,75,1)\n",
        "        Raw image with normalized pixel values (from 0 to 1)\n",
        "    predictions : np.ndarray of shape (N,)\n",
        "        Predicted label with the same shape as labels\n",
        "    labels : np.ndarray of shape (N,)\n",
        "        Labels of the digits (from 0 to 9)\n",
        "    pred_bboxes : np.ndarray of shape (n, N) ??\n",
        "        Predicted bboxes locations\n",
        "    bboxes : np.ndarray of shape (n, N)\n",
        "        Ground true bboxe locations\n",
        "    iou : list of shape (???)\n",
        "        IoU of each bboxes\n",
        "    title : str\n",
        "        Figure's title\n",
        "  \"\"\"\n",
        "  n = 10\n",
        "  indexes = np.random.choice(len(predictions), size=n)\n",
        "  n_digits = digits[indexes]\n",
        "  n_predictions = predictions[indexes]\n",
        "  n_labels = labels[indexes]\n",
        "\n",
        "  n_iou = []\n",
        "  if len(iou) > 0:\n",
        "    # If multiple bboxes\n",
        "    n_iou = iou[indexes]\n",
        "\n",
        "  if (len(pred_bboxes) > 0):\n",
        "    # If multiple bboxes predicted\n",
        "    n_pred_bboxes = pred_bboxes[indexes,:]\n",
        "\n",
        "  if (len(bboxes) > 0):\n",
        "    # If multiple ground truth bboxes\n",
        "    n_bboxes = bboxes[indexes,:]\n",
        "\n",
        "  # Rescale pixel values to un-normed values (from 0 -black- to 255 -white-)\n",
        "  n_digits = n_digits * 255.0\n",
        "  n_digits = n_digits.reshape(n, 75, 75)\n",
        "\n",
        "  # Set plot config\n",
        "  fig = plt.figure(figsize=(20, 4))\n",
        "  plt.title(title)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  \n",
        "  for i in range(10):\n",
        "    ax = fig.add_subplot(1, 10, i+1)\n",
        "    bboxes_to_plot = []\n",
        "    if (len(pred_bboxes) > i):\n",
        "      bboxes_to_plot.append(n_pred_bboxes[i])\n",
        "    \n",
        "    if (len(bboxes) > i):\n",
        "      bboxes_to_plot.append(n_bboxes[i])\n",
        "\n",
        "    img_to_draw = draw_bounding_boxes_on_image_array(image=n_digits[i], boxes=np.asarray(bboxes_to_plot), color=['red', 'green'], display_str_list=[\"true\", \"pred\"])\n",
        "    plt.xlabel(n_predictions[i])\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    \n",
        "    if n_predictions[i] != n_labels[i]:\n",
        "      ax.xaxis.label.set_color('red')\n",
        "    \n",
        "    plt.imshow(img_to_draw)\n",
        "\n",
        "    if len(iou) > i :\n",
        "      color = \"black\"\n",
        "      if (n_iou[i][0] < iou_threshold):\n",
        "        color = \"red\"\n",
        "      ax.text(0.2, -0.3, \"iou: %s\" %(n_iou[i][0]), color=color, transform=ax.transAxes)\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "def dataset_to_numpy_util(training_dataset, validation_dataset, N, S):\n",
        "  \"\"\"\n",
        "  Pull a batch from the datasets. This code is not very nice.\n",
        "  \n",
        "  Args:\n",
        "    training_dataset : torch.utils.data.Dataset\n",
        "        Dataset from the torch.utils.data.Dataset Pytorch class, returning the \n",
        "        training digits, labels and bboxes coordinates as batches such as : \n",
        "            - training digits : torch.Tensor of shape (batch_size, 1, 75, 75)\n",
        "            - labels : torch.Tensor of shape (batch_size, 10)\n",
        "            - bboxes coordinates : torch.Tensor of shape (batch_size, 4)\n",
        "    validation_dataset : torch.utils.data.Dataset \n",
        "        Dataset from the torch.utils.data.Dataset Pytorch class, returning the \n",
        "        whole validation digits, labels and bboxes coordinates.\n",
        "    N : int\n",
        "        Size of the training sample to extract from the training dataset\n",
        "    S : int\n",
        "        Number of cell in one direction\n",
        "\n",
        "  Returns:\n",
        "    N_train_ds_digits : np.ndarray of shape (N, 1, 75, 75)\n",
        "    N_train_ds_labels : np.ndarray of shape (N, 10)\n",
        "    N_train_ds_bboxes : np.ndarray of shape (N, 4)\n",
        "    validation_digits : np.ndarray of shape (len(validation_dataset), 1, 75, 75)\n",
        "    validation_labels : np.ndarray of shape (len(validation_dataset), 10)\n",
        "    validation_bboxes : np.ndarray of shape (len(validation_dataset), 4)\n",
        "  \"\"\"\n",
        "  ### get N training digits, labels and bboxes from one batch\n",
        "  ### turning the bboxes coordinates into ndarrays\n",
        "  one_batch_train_ds_digits, one_batch_train_ds_labels, one_batch_train_ds_bboxes = next(iter(training_dataset))\n",
        "  cell_size = 75/6 #one_batch_train_ds_digits[0][0].shape[1]/ S\n",
        "\n",
        "  N_train_ds_digits = one_batch_train_ds_digits[:N].numpy()\n",
        "  N_train_ds_labels = one_batch_train_ds_labels[:N].numpy()\n",
        "  \n",
        "  N_train_ds_bboxes = one_batch_train_ds_bboxes[:N].numpy()\n",
        "  N_train_ds_bboxes_abs = np.zeros((N,4))\n",
        "\n",
        "\n",
        "  x_min = ((N_train_ds_bboxes[:,2])) - (N_train_ds_bboxes[:,4]/2)\n",
        "  y_min = ((N_train_ds_bboxes[:,3])) - (N_train_ds_bboxes[:,5]/2)\n",
        "  x_max = ((N_train_ds_bboxes[:,2])) + (N_train_ds_bboxes[:,4]/2)\n",
        "  y_max = ((N_train_ds_bboxes[:,3])) + (N_train_ds_bboxes[:,5]/2)\n",
        "\n",
        "\n",
        "  N_train_ds_bboxes_abs[:,0] = x_min\n",
        "  N_train_ds_bboxes_abs[:,1] = y_min\n",
        "  N_train_ds_bboxes_abs[:,2] = x_max\n",
        "  N_train_ds_bboxes_abs[:,3] = y_max\n",
        "\n",
        "\n",
        "  print(N_train_ds_bboxes_abs[5])\n",
        "  \n",
        "  ### get the whole validation digits, labels and bboxes\n",
        "  ### turning the bboxes coordinates into ndarrays\n",
        "  for validation_digits, validation_labels, validation_bboxes in validation_dataset:\n",
        "      validation_digits = validation_digits.numpy()\n",
        "      validation_labels = validation_labels.numpy()\n",
        "      validation_bboxes = validation_bboxes.numpy()\n",
        "      break\n",
        "\n",
        "  # turning one hot encoding labels into the corresponding digit\n",
        "  validation_labels = np.argmax(validation_labels, axis=1)\n",
        "  N_train_ds_labels = np.argmax(N_train_ds_labels, axis=1)\n",
        "\n",
        "  return (N_train_ds_digits, N_train_ds_labels, N_train_ds_bboxes_abs,\n",
        "          validation_digits, validation_labels, validation_bboxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOraK1TX7XZB",
        "outputId": "b595e38a-fea4-4713-85cc-2343d0a474c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------------------------\n",
            "Execute notebook on - cpu -\n",
            "------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Choosing device between CPU or GPU\n",
        "#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device = torch.device('mps') if torch.has_mps else torch.device('cpu')\n",
        "print(\"\\n------------------------------------\")\n",
        "print(f\"Execute notebook on - {device} -\")\n",
        "print(\"------------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "TRspEjkE7aEK"
      },
      "outputs": [],
      "source": [
        "def numpy_pad_to_bounding_box(image, offset_height=0, offset_width=0, target_height=0, target_width=0):\n",
        "    assert image.shape[:-1][0] <= target_height-offset_height, \"height must be <= target - offset\"\n",
        "    assert image.shape[:-1][1] <= target_width-offset_width, \"width must be <= target - offset\"\n",
        "    \n",
        "    target_array = np.zeros((target_height, target_width, image.shape[-1]))\n",
        "\n",
        "    for k in range(image.shape[0]):\n",
        "        target_array[offset_height+k][offset_width:image.shape[1]+offset_width] = image[k]\n",
        "    \n",
        "    return target_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "M7VztqFE71JZ"
      },
      "outputs": [],
      "source": [
        "class my_mnist_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root:str, split:str=None, download:bool=False, S=6):\n",
        "        assert split, \"You have to specify the split.\"\n",
        "        \n",
        "        if split == \"train\":\n",
        "            train = True\n",
        "        elif split == \"test\":\n",
        "            train = False\n",
        "        \n",
        "        self.dataset = torchvision.datasets.MNIST(root=root, train=train, download=download)\n",
        "        \n",
        "        self.cell_size = 75 / S\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def _transform_pasting75(self, image, label):\n",
        "        ### xmin, ymin of digit\n",
        "        xmin = torch.randint(0, 48, (1,))\n",
        "        ymin = torch.randint(0, 48, (1,))\n",
        "        \n",
        "        image = torchvision.transforms.ToTensor()(image)\n",
        "        image = torch.reshape(image, (28,28,1,))\n",
        "        image = torch.from_numpy(numpy_pad_to_bounding_box(image, ymin, xmin, 75, 75))\n",
        "        image = image.permute(2, 0, 1) #(C,H,W)\n",
        "        image = image.to(torch.float)\n",
        "        \n",
        "        xmin, ymin = xmin.to(torch.float), ymin.to(torch.float)\n",
        "\n",
        "        xmax_bbox, ymax_bbox = (xmin + 28), (ymin + 28)\n",
        "        xmin_bbox, ymin_bbox = xmin, ymin\n",
        "        w_bbox = xmax_bbox-xmin_bbox\n",
        "        h_bbox = ymax_bbox-ymin_bbox\n",
        "\n",
        "        rw = w_bbox / 75\n",
        "        rh = h_bbox / 75\n",
        "        cx = (xmin + (w_bbox/2))/75\n",
        "        cy = (ymin + (h_bbox/2))/75\n",
        "\n",
        "        cx_rcell = cx % self.cell_size / self.cell_size\n",
        "        cy_rcell = cy % self.cell_size / self.cell_size\n",
        "\n",
        "\n",
        "        label_one_hot = F.one_hot(torch.as_tensor(label, dtype=torch.int64), 10)\n",
        "        bbox_coord = torch.Tensor([cx_rcell, cy_rcell, rw, rh])\n",
        "\n",
        "        return image, label_one_hot, bbox_coord\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        image, one_hot_label, bbox_coord = self._transform_pasting75(self.dataset[idx][0], self.dataset[idx][1])\n",
        "        \n",
        "        return image, one_hot_label.to(torch.float), bbox_coord\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "yQNznLfO8jOx"
      },
      "outputs": [],
      "source": [
        "def get_training_dataset(BATCH_SIZE=64):\n",
        "    \"\"\"\n",
        "    Loads and maps the training split of the dataset using the custom dataset class. \n",
        "    \"\"\"\n",
        "    dataset = my_mnist_dataset(root=\"data\", split=\"train\", download=True)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    return dataloader, len(dataset)\n",
        "\n",
        "def get_validation_dataset(BATCH_SIZE = None):\n",
        "    \"\"\"\n",
        "    Loads and maps the validation split of the datasetusing the custom dataset class. \n",
        "    \"\"\"\n",
        "    dataset = my_mnist_dataset(root=\"data\", split=\"test\", download=True)\n",
        "    if BATCH_SIZE is None:\n",
        "        BATCH_SIZE = len(dataset)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    return dataloader, len(dataset)\n",
        "\n",
        "# instantiate the datasets\n",
        "training_dataset, len_training_ds = get_training_dataset()\n",
        "validation_dataset, len_validation_ds = get_validation_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UARQiNGd8rxL"
      },
      "outputs": [],
      "source": [
        "(training_digits, training_labels, training_bboxes,\n",
        " validation_digits, validation_labels, validation_bboxes) = dataset_to_numpy_util(training_dataset, validation_dataset, 10, S=6)\n",
        "display_digits_with_boxes(training_digits, training_labels, training_labels, np.array([]), training_bboxes, np.array([]), \"training digits and their labels\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "wfgUx1srt90c"
      },
      "outputs": [],
      "source": [
        "class CNNBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(CNNBlock, self).__init__()\n",
        "        self.conv = torch.nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "        self.bn = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.l_relu = torch.nn.LeakyReLU(0.1)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        x = self.conv(input)\n",
        "        x = self.bn(x)\n",
        "        return self.l_relu(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "3VjCkTEce-CT"
      },
      "outputs": [],
      "source": [
        "class YoloMNIST(torch.nn.Module):\n",
        "    def __init__(self, sizeHW, S, C, B):\n",
        "        super(YoloMNIST, self).__init__()\n",
        "        self.S, self.C, self.B = S, C, B\n",
        "        self.sizeHW = sizeHW\n",
        "        self.cell_size = self.sizeHW / self.S\n",
        "\n",
        "        self.seq = torch.nn.Sequential()        \n",
        "        self.seq.add_module(f\"conv_1\", CNNBlock(1, 32, stride=2, kernel_size=7, padding=2))\n",
        "        self.seq.add_module(f\"maxpool_1\", torch.nn.MaxPool2d(2))\n",
        "        self.seq.add_module(f\"conv_3\", CNNBlock(32, 128, stride=1, kernel_size=3, padding=0))\n",
        "        self.seq.add_module(f\"maxpool_2\", torch.nn.MaxPool2d(2))\n",
        "        self.seq.add_module(f\"conv_5\", CNNBlock(128, 64, stride=1, kernel_size=1, padding=0))\n",
        "        self.seq.add_module(f\"conv_4\", CNNBlock(64, 128, stride=1, kernel_size=3, padding=0))\n",
        "        self.seq.add_module(f\"conv_6\", CNNBlock(128, 128, stride=1, kernel_size=3, padding=1))\n",
        "        \n",
        "        self.fcs = self._create_fcs()\n",
        "\n",
        "    def _size_output(self, sizeHW:int, kernel:int, stride:int, padding:int=0, isMaxPool:bool=False)->int:\n",
        "        \"\"\"\n",
        "        Output size (width/height) of convolutional or maxpool layers.\n",
        "\n",
        "        Args:\n",
        "            sizeHW : int\n",
        "                Image size (we suppose this is a square image)\n",
        "            kernel : int\n",
        "                Size of a square kernel\n",
        "            stride : int\n",
        "                Stride of convolution layer\n",
        "            padding : int\n",
        "                Padding of convolution layer\n",
        "            isMaxPool : Bool, default is False.\n",
        "                Specify if it is a Maxpool layer (True) or not (False). \n",
        "\n",
        "        Return:\n",
        "            output_size : int\n",
        "                Image output size after a convolutional or MaxPool layer.\n",
        "        \"\"\" \n",
        "        if isMaxPool == True:\n",
        "            output_size = int(sizeHW/2)\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "        if padding == 'same':\n",
        "            output_size = sizeHW\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "        else:\n",
        "            output_size = (sizeHW + 2 * padding - (kernel-1)-1)/stride\n",
        "            output_size = int(output_size + 1)\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "\n",
        "    def _create_fcs(self):\n",
        "        output = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(128 * self.S * self.S, 4096),\n",
        "            torch.nn.LeakyReLU(0.1),\n",
        "            torch.nn.Linear(4096, self.S * self.S * (self.C + self.B*5))\n",
        "        )\n",
        "        return output\n",
        "    \n",
        "\n",
        "    def forward(self, input:torch.Tensor)->tuple:\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            input : torch.Tensor of shape (N, C, H, W)\n",
        "                Batch of images.\n",
        "\n",
        "        Return:\n",
        "            box_coord : torch.Tensor of shape (N, 6, 6, 5)\n",
        "                Contains xc_rcell, yc_rcell, rw, rh and the confidence number c\n",
        "                over 6x6 grid cells.\n",
        "            classifier : torch.Tensor of shape (N, 6, 6, 10)\n",
        "                Contains the one-hot encoding of each digit number over\n",
        "                6x6 grid cells.\n",
        "        \"\"\"     \n",
        "        x = self.seq(input)\n",
        "        x = self.fcs(x)\n",
        "        x = x.view(x.size(0), self.S, self.S, self.B * 5 + self.C)\n",
        "        box_coord = x[:,:,:,0:5]\n",
        "        classifier = x[:,:,:,5:]\n",
        "        return box_coord, classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrYIfm2he-CV"
      },
      "outputs": [],
      "source": [
        "########### CELLULE TEST ##############################################\n",
        "model_MNIST = YoloMNIST(sizeHW=75, S=6, C=10, B=1)\n",
        "for k in training_dataset:\n",
        "    break\n",
        "print(\"### OUTPUT DATASET ###\")\n",
        "print(\"La taille d'un élément est de : \", len(k))\n",
        "print(\"Le premier élément est l'image de shape : \", k[0].shape)\n",
        "print(\"Le deuxieme élément est les label de shape : \", k[1].shape)\n",
        "print(\"Le dernier élément est la bbox [xc_rcell, yc_rcell, xc, yc, rw, hw] : \", k[2].shape)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"### OUTPUT PRED ###\")\n",
        "pc_pred, bbox_pred, class_pred = model_MNIST(k[0])\n",
        "print(\"Le premier élément est une grille de shape : \", model_MNIST(k[0])[0].shape, \" correspondant aux nombres pc sur les 6x6 cellules\")\n",
        "print(\"Le deuxieme élement est, une grille de shape : \", model_MNIST(k[0])[1].shape, \" correspondant aux [xc_rcell, yc_rcell, rw, rh]\")\n",
        "print(\"Le troisieme élement est, une grille de shape : \", model_MNIST(k[0])[2].shape, \" correspondant à des softmax vect sur chaque cellule de grille\")\n",
        "print(\"\\n\")\n",
        "\n",
        "bbox_true = k[2][:1]\n",
        "print(bbox_true.shape)\n",
        "bbox_true2 = torch.concat((bbox_true[0][0:2], bbox_true[0][4::])).unsqueeze(0)\n",
        "print(bbox_true2.shape)\n",
        "print(k[2][0])\n",
        "print(bbox_true2)\n",
        "#######################################################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class YoloLoss(torch.nn.Module):\n",
        "    def __init__(self, lambd_coord:int, lambd_noobj:float, S:int=6, sizeHW:int=75):\n",
        "        super(YoloLoss, self).__init__()\n",
        "        self.lambd_coord = lambd_coord\n",
        "        self.lambd_noobj = lambd_noobj\n",
        "        self.S = S\n",
        "        self.cell_size = sizeHW/S\n",
        "\n",
        "    def _loss_over_coord(self, pred_coord_rcell, true_coord_rcell):\n",
        "        \"\"\"\n",
        "        pred_coord_rcell : torch.Tensor of shape (N, 2)\n",
        "        true_coord_rcell : torch.Tensor of shape (N, 2)\n",
        "        \"\"\"\n",
        "        xc_hat, yc_hat = pred_coord_rcell\n",
        "        xc, yc = true_coord_rcell\n",
        "        squared_error = torch.pow(xc - xc_hat,2) - torch.pow(yc - yc_hat,2)\n",
        "        return squared_error\n",
        "\n",
        "    def _loss_over_size(self, pred_size, true_size):\n",
        "        \"\"\"\n",
        "        pred_size : torch.Tensor of shape (N, 2)\n",
        "        true_size : torch.Tensor of shape (N, 2)\n",
        "        \"\"\"\n",
        "        rw_hat, rh_hat = pred_size\n",
        "        rw, rh = true_size\n",
        "        root_squared_error_w = torch.pow(torch.sqrt(rw) - torch.sqrt(rw_hat),2)\n",
        "        root_squared_error_h = torch.pow(torch.sqrt(rw) - torch.sqrt(rw_hat),2)\n",
        "        root_squared_error = root_squared_error_w - root_squared_error_h\n",
        "        return root_squared_error\n",
        "\n",
        "    def _loss_over_confidence(self, pred_c, c):\n",
        "        \"\"\"\n",
        "        pred_c : torch.Tensor of shape (N, 1)\n",
        "        c      : torch.Tensor of shape (1)\n",
        "        \"\"\"\n",
        "        squared_error = torch.pow(c - pred_c,2)\n",
        "        return squared_error\n",
        "\n",
        "    def _loss_over_classprob(self, pred_class, true_class):\n",
        "        \"\"\"\n",
        "        pred_class : torch.Tensor of shape (N, 10)\n",
        "        true_class : torch.Tensor of shape (N, 10)\n",
        "        \"\"\"\n",
        "        squared_error = torch.pow(true_class - pred_class,2)\n",
        "        return squared_error\n",
        "    \n",
        "    def _relative2absolute(self, bbox_relative:torch.Tensor)->torch.Tensor:\n",
        "        \"\"\"\n",
        "        Bounding box relative to cell coordinates into absolute coordinates \n",
        "        (pixels). Used to calculate IoU. \n",
        "\n",
        "        Args:\n",
        "            bbox_relative : torch.Tensor of shape (N, 4)\n",
        "                Bounding box coordinates to convert.\n",
        "        Return:\n",
        "            bbox_absolute : torch.Tensor of shape (N, 4)\n",
        "        \"\"\"\n",
        "        cx_rcell, cy_rcell, rw, rh = bbox_relative.permute(1,0)\n",
        "        \n",
        "        cx = cx_rcell * self.cell_size - (1/self.cell_size) * int(cx_rcell/self.cell_size)\n",
        "        cy = cy_rcell * self.cell_size - (1/self.cell_size) * int(cy_rcell/self.cell_size)\n",
        "\n",
        "        cx_abs = self.sizeHW * cx\n",
        "        cy_abs = self.sizeHW * cy\n",
        "        x_min = cx_abs - (self.sizeHW * (rw/2))\n",
        "        y_min = cy_abs - (self.sizeHW * (rh/2))\n",
        "        x_max = cx_abs + (self.sizeHW * (rw/2))\n",
        "        y_max = cy_abs + (self.sizeHW * (rh/2))\n",
        "\n",
        "        bbox_absolute = torch.stack((x_min, y_min, x_max, y_max), dim=-1)\n",
        "        return bbox_absolute\n",
        "\n",
        "    def _intersection_over_union(self, pred_box:torch.Tensor, true_box:torch.Tensor)->float:\n",
        "        \"\"\"\n",
        "        Intersection over Union method.\n",
        "\n",
        "        Args:\n",
        "            pred_box : torch.Tensor of shape (N, 4)\n",
        "                Predicted bounding boxes of a batch, in a given cell.\n",
        "            true_box : torch.Tensor of shape (N, 4)\n",
        "                Ground truth bounding boxes of a batch, in a given cell.\n",
        "\n",
        "        Return:\n",
        "            iou : float\n",
        "                Number between 0 and 1 where 1 is a perfect overlap.\n",
        "        \"\"\"        \n",
        "        pred_box = self._relative2absolute(pred_box)\n",
        "        true_box = self._relative2absolute(true_box)        \n",
        "        xmin_pred, ymin_pred, xmax_pred, ymax_pred = pred_box.permute(1,0)\n",
        "        xmin_true, ymin_true, xmax_true, ymax_true = true_box.permute(1,0)\n",
        "\n",
        "        smoothing_factor = 1e-10\n",
        "\n",
        "        xmin_overlap = torch.maximum(xmin_pred, xmin_true)\n",
        "        xmax_overlap = torch.minimum(xmax_pred, xmax_true)\n",
        "        ymin_overlap = torch.maximum(ymin_pred, ymin_true)\n",
        "        ymax_overlap = torch.minimum(ymax_pred, ymax_true)\n",
        "        \n",
        "        pred_box_area = (xmax_pred - xmin_pred) * (ymax_pred - ymin_pred)\n",
        "        true_box_area = (xmax_true - xmin_true) * (ymax_true - ymin_true)\n",
        "       \n",
        "        overlap_area = torch.maximum((xmax_overlap - xmin_overlap), torch.Tensor([0]))  * torch.maximum((ymax_overlap - ymin_overlap), torch.Tensor([0]))\n",
        "        union_area = (pred_box_area + true_box_area) - overlap_area\n",
        "        iou = (overlap_area + smoothing_factor) / (union_area + smoothing_factor)\n",
        "        return iou   \n",
        "\n",
        "\n",
        "    def forward(self, pred:torch.Tensor, truth:torch.Tensor, isObject:bool)->torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass for the current grid cell.\n",
        "\n",
        "        Args:\n",
        "            pred : torch.Tensor of shape (N, 4+1+10)\n",
        "                Batch predicted outputs containing xc_rcell, yc_rcell, rw, rh,\n",
        "                confident number c and the one-hot vect of the digit class.\n",
        "            truth : torch.Tensor of shape (N, 4+10) >>> 5+10 ???\n",
        "                Groundtrue batch containing bbox values and digit class one-hot vect.\n",
        "            isObject : bool\n",
        "                Indicates if the center of the groundtrue bbox is in the current\n",
        "                grid cell. In other words, indicates if there is an object to \n",
        "                detect in this grid cell.  \n",
        "\n",
        "        Return:\n",
        "            loss : float\n",
        "                The loss value for the current grid cell\n",
        "        \"\"\"\n",
        "        pred_box = pred[:,:4]\n",
        "        true_box = truth[:,:4]\n",
        "        \n",
        "        pred_class = pred[:,5:]\n",
        "        true_class = truth[:,4:]\n",
        "\n",
        "        c = torch.Tensor([isObject])\n",
        "        pred_c = pred[:,4] * self._intersection_over_union(pred_box, true_box)\n",
        "\n",
        "        loss_1 = isObject * self._loss_over_coord(pred_box[:,:2], true_box[:,:2])\n",
        "        loss_2 = isObject * self._loss_over_size(pred_box[:,2:], true_box[:,2:])\n",
        "        loss_3 = isObject * self._loss_over_confidence(pred_c, c) # ?????\n",
        "        loss_3b = (1-isObject) * self._loss_over_confidence(pred_c, c) ### ???  \n",
        "        loss_4 = isObject * self._loss_over_classprob(pred_class, true_class)\n",
        "\n",
        "        loss = self.lambd_coord*loss_1 + self.lambd_coord*loss_2 + loss_3 \\\n",
        "                + self.lambd_noobj * loss_3b + loss_4\n",
        "\n",
        "        return loss/len(truth)"
      ],
      "metadata": {
        "id": "T_jOg_i_p2_c"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bbox2Tensor(bbox:torch.Tensor, S:int=6, sizeHW:int=75)->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Constructs en Tensor and puts bbox values in the corresponding i,j grid cell.\n",
        "\n",
        "    Args :\n",
        "        bbox : torch.Tensor of shape (N,4)\n",
        "            Contains bbox values xc_rcell, yc_rcell, rw and rh.\n",
        "        S : int, default is 6\n",
        "            Size of the grid.\n",
        "        sizeHW : int, default is 75\n",
        "            Size of the image.\n",
        "\n",
        "    Return :\n",
        "        bbox_t : torch.Tensor of shape (N, S, S, 4)\n",
        "            Tensor containing all 4 bbox values in the corresponding i,j grid\n",
        "            cell position i.e. in the i,j position where an object should be\n",
        "            detected.\n",
        "    \"\"\"\n",
        "    N = len(bbox)\n",
        "    bbox_t = torch.zeros(N,S,S,4)\n",
        "    gcell_size = 1/S\n",
        "    cell_size = sizeHW/S\n",
        "\n",
        "    xc_rcell, yc_rcell, rw, rh = bbox.permute(1,0)\n",
        "    xc = xc_rcell * cell_size - (1/cell_size) * (xc_rcell/cell_size).to(torch.int32)\n",
        "    yc = yc_rcell * cell_size - (1/cell_size) * (yc_rcell/cell_size).to(torch.int32)\n",
        "\n",
        "    N_range = torch.arange(N)\n",
        "    lines = (yc / gcell_size).to(torch.long)\n",
        "    columns = (xc / gcell_size).to(torch.long)\n",
        "    bbox_t[N_range, lines, columns] = torch.stack((xc_rcell, yc_rcell, rw, rh)).permute(1,0)\n",
        "    \n",
        "    return bbox_t"
      ],
      "metadata": {
        "id": "OdIlkN61mXJj"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "mHUOkfWPe-CW",
        "outputId": "f54c6b56-ec91-40b4-86b6-e8f119334b64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "YoloMNIST                                [64, 6, 6, 5]             --\n",
              "├─Sequential: 1-1                        [64, 128, 6, 6]           --\n",
              "│    └─CNNBlock: 2-1                     [64, 32, 37, 37]          --\n",
              "│    │    └─Conv2d: 3-1                  [64, 32, 37, 37]          1,568\n",
              "│    │    └─BatchNorm2d: 3-2             [64, 32, 37, 37]          64\n",
              "│    │    └─LeakyReLU: 3-3               [64, 32, 37, 37]          --\n",
              "│    └─MaxPool2d: 2-2                    [64, 32, 18, 18]          --\n",
              "│    └─CNNBlock: 2-3                     [64, 128, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-4                  [64, 128, 16, 16]         36,864\n",
              "│    │    └─BatchNorm2d: 3-5             [64, 128, 16, 16]         256\n",
              "│    │    └─LeakyReLU: 3-6               [64, 128, 16, 16]         --\n",
              "│    └─MaxPool2d: 2-4                    [64, 128, 8, 8]           --\n",
              "│    └─CNNBlock: 2-5                     [64, 64, 8, 8]            --\n",
              "│    │    └─Conv2d: 3-7                  [64, 64, 8, 8]            8,192\n",
              "│    │    └─BatchNorm2d: 3-8             [64, 64, 8, 8]            128\n",
              "│    │    └─LeakyReLU: 3-9               [64, 64, 8, 8]            --\n",
              "│    └─CNNBlock: 2-6                     [64, 128, 6, 6]           --\n",
              "│    │    └─Conv2d: 3-10                 [64, 128, 6, 6]           73,728\n",
              "│    │    └─BatchNorm2d: 3-11            [64, 128, 6, 6]           256\n",
              "│    │    └─LeakyReLU: 3-12              [64, 128, 6, 6]           --\n",
              "│    └─CNNBlock: 2-7                     [64, 128, 6, 6]           --\n",
              "│    │    └─Conv2d: 3-13                 [64, 128, 6, 6]           147,456\n",
              "│    │    └─BatchNorm2d: 3-14            [64, 128, 6, 6]           256\n",
              "│    │    └─LeakyReLU: 3-15              [64, 128, 6, 6]           --\n",
              "├─Sequential: 1-2                        [64, 540]                 --\n",
              "│    └─Flatten: 2-8                      [64, 4608]                --\n",
              "│    └─Linear: 2-9                       [64, 4096]                18,878,464\n",
              "│    └─LeakyReLU: 2-10                   [64, 4096]                --\n",
              "│    └─Linear: 2-11                      [64, 540]                 2,212,380\n",
              "==========================================================================================\n",
              "Total params: 21,359,612\n",
              "Trainable params: 21,359,612\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 2.63\n",
              "==========================================================================================\n",
              "Input size (MB): 1.44\n",
              "Forward/backward pass size (MB): 94.42\n",
              "Params size (MB): 85.44\n",
              "Estimated Total Size (MB): 181.30\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ],
      "source": [
        "learning_rate = 0.001\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "model_MNIST = YoloMNIST(sizeHW=75, S=6, C=10, B=1)\n",
        "model_MNIST = model_MNIST.to(device)\n",
        "optimizer = torch.optim.Adam(params=model_MNIST.parameters(), lr=learning_rate)\n",
        "loss_yolo = YoloLoss(lambd_coord=5, lambd_noobj=0.5, S=6, sizeHW=75)\n",
        "#isDigit_loss = torch.nn.MSELoss()\n",
        "#regression_loss = nn.MSELoss()\n",
        "#classification_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "print(optimizer)\n",
        "summary(model_MNIST, input_size = (BATCH_SIZE,1,75,75))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######## TODO : réfléchir à comment utiliser bbox_tensor"
      ],
      "metadata": {
        "id": "DcZ9O1T2uLlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tSO0Qo7e-CX",
        "outputId": "5deca27f-eb00-40bc-9745-b7e2f0880321"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[START] : 2022-08-14 19:44:19 :\n",
            "[Training on] : MPS\n",
            "--------------------\n",
            "     2022-08-14 19:44:19 : EPOCH 1/1\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "delta_time = datetime.timedelta(hours=1)\n",
        "timezone = datetime.timezone(offset=delta_time)\n",
        "\n",
        "t = datetime.datetime.now(tz=timezone)\n",
        "str_t = '{:%Y-%m-%d %H:%M:%S}'.format(t)\n",
        "print(f\"[START] : {str_t} :\")\n",
        "print(f\"[Training on] : {str(device).upper()}\")\n",
        "\n",
        "EPOCHS = 1\n",
        "size_grid = 6\n",
        "training_losses_list = []\n",
        "box_losses_list = []\n",
        "class_losses_list = []\n",
        "pc_losses_list = []\n",
        "model_MNIST = model_MNIST.to(device)\n",
        "\n",
        "for epoch in range(EPOCHS) : \n",
        "    begin_time = timer()\n",
        "    training_losses = 0\n",
        "    \n",
        "    print(\"-\"*20)\n",
        "    str_t = '{:%Y-%m-%d %H:%M:%S}'.format(t)\n",
        "    print(\" \"*5 + f\"{str_t} : EPOCH {epoch+1}/{EPOCHS}\")\n",
        "    print(\"-\"*20)\n",
        "\n",
        "    model_MNIST.train()\n",
        "    for batch, (img, labels, bbox_true) in enumerate(training_dataset):\n",
        "        begin_batch_time = timer()\n",
        "        img, labels, bbox_true = img.to(device), labels.to(device), bbox_true.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        bbox_preds, label_preds = model_MNIST(img)\n",
        "        \n",
        "        bbox_preds_6x6 = bbox2Tensor(bbox_preds)\n",
        "        bbox_true_6x6 = bbox2Tensor(bbox_true)\n",
        "        \n",
        "        ############### REFLECHIR ICI\n",
        "        loss = 0\n",
        "        for i in range(size_grid):\n",
        "            for j in range(size_grid):\n",
        "                loss += loss_yolo(bbox_preds_6x6[:,i,j], bbox_true_6x6[:,i,j], )\n",
        "        loss.backward()\n",
        "\n",
        "        ############### REFLECHIR ICI\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        ######### print part #######################\n",
        "        training_losses += training_loss.item()\n",
        "        training_losses_list.append(training_loss)\n",
        "        box_losses_list.append(box_loss.item())\n",
        "        class_losses_list.append(class_loss.item())\n",
        "        pc_losses_list.append(pc_loss.item())\n",
        "\n",
        "        if batch+1 <= len_training_ds//BATCH_SIZE:\n",
        "            current_training_sample = (batch+1)*BATCH_SIZE\n",
        "        else:\n",
        "            current_training_sample = (batch)*BATCH_SIZE + len_training_ds%BATCH_SIZE\n",
        "        \n",
        "        if (batch+1) == 1 or (batch+1)%100 == 0 or (batch+1) == len_training_ds//BATCH_SIZE +1:\n",
        "            print(f\" --- Image : {current_training_sample}/{len_training_ds}\",\\\n",
        "                  f\" : loss = {training_loss:.5f}\", \\\n",
        "                  f\", classification loss = {class_loss.item():.5f}\", \\\n",
        "                  f\", bounding box loss = {box_loss.item():.5f}\", \\\n",
        "                  f\", pc loss = {pc_loss.item():.5f}\")\n",
        "            if batch+1 == (len_training_ds//BATCH_SIZE)+1:\n",
        "                print(f\"Total elapsed time for training : {datetime.timedelta(seconds=timer()-begin_time)}\")\n",
        "                print(f\"Mean training loss for this epoch : {training_losses / len(training_dataset):.5f}\")\n",
        "    training_losses /= len(training_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNOs4jhIe-CZ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "YOLO_MNIST_Localization.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "bfc2e043f4a41bbce066f7921047bdcfe8769546c7198e4df6482fd90de274f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}