{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThOpaque/Food_Recognition/blob/main/YOLO_MNIST_Localization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAHeh1BRufX4",
        "outputId": "a193c888-45ab-41b6-a85f-d234ad958ced"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchinfo in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (1.7.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: torchmetrics in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (0.9.3)\n",
            "Requirement already satisfied: packaging in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from torchmetrics) (1.23.1)\n",
            "Requirement already satisfied: torch>=1.3.1 in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from torchmetrics) (1.13.0.dev20220730)\n",
            "Requirement already satisfied: typing-extensions in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from torch>=1.3.1->torchmetrics) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "import os, time, datetime\n",
        "from timeit import default_timer as timer\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "%pip install torchinfo;\n",
        "%pip install torchmetrics;\n",
        "from torchmetrics import MeanSquaredError;\n",
        "from torchinfo import summary;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOraK1TX7XZB",
        "outputId": "917d3947-108d-42f7-b499-4b5924b0279a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------------\n",
            "Execute notebook on - mps -\n",
            "------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Choosing device between CPU or GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "elif torch.has_mps:\n",
        "    device=torch.device('mps')\n",
        "else:\n",
        "    device=torch.device('cpu')\n",
        "    \n",
        "print(\"\\n------------------------------------\")\n",
        "print(f\"Execute notebook on - {device} -\")\n",
        "print(\"------------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "M7VztqFE71JZ"
      },
      "outputs": [],
      "source": [
        "class my_mnist_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root:str, split:str=None, download:bool=False, S=6, sizeHW=75):\n",
        "        assert split, \"You have to specify the split.\"\n",
        "        \n",
        "        if split == \"train\":\n",
        "            train = True\n",
        "        elif split == \"test\":\n",
        "            train = False\n",
        "        \n",
        "        self.dataset = torchvision.datasets.MNIST(root=root, train=train, download=download)\n",
        "        \n",
        "        self.cell_size = sizeHW / S\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def _numpy_pad_to_bounding_box(self, image, offset_height=0, offset_width=0, target_height=0, target_width=0):\n",
        "        assert image.shape[:-1][0] <= target_height-offset_height, \"height must be <= target - offset\"\n",
        "        assert image.shape[:-1][1] <= target_width-offset_width, \"width must be <= target - offset\"\n",
        "        \n",
        "        target_array = np.zeros((target_height, target_width, image.shape[-1]))\n",
        "\n",
        "        for k in range(image.shape[0]):\n",
        "            target_array[offset_height+k][offset_width:image.shape[1]+offset_width] = image[k]\n",
        "        \n",
        "        return target_array\n",
        "\n",
        "    def _transform_pasting75(self, image, label):\n",
        "        ### xmin, ymin of digit\n",
        "        xmin = torch.randint(0, 48, (1,))\n",
        "        ymin = torch.randint(0, 48, (1,))\n",
        "        \n",
        "        image = torchvision.transforms.ToTensor()(image)\n",
        "        image = torch.reshape(image, (28,28,1,))\n",
        "        image = torch.from_numpy(self._numpy_pad_to_bounding_box(image, ymin, xmin, 75, 75))\n",
        "        image = image.permute(2, 0, 1) #(C,H,W)\n",
        "        image = image.to(torch.float)\n",
        "        \n",
        "        xmin, ymin = xmin.to(torch.float), ymin.to(torch.float)\n",
        "\n",
        "        xmax_bbox, ymax_bbox = (xmin + 28), (ymin + 28)\n",
        "        xmin_bbox, ymin_bbox = xmin, ymin\n",
        "        w_bbox = xmax_bbox-xmin_bbox\n",
        "        h_bbox = ymax_bbox-ymin_bbox\n",
        "\n",
        "        rw = w_bbox / 75\n",
        "        rh = h_bbox / 75\n",
        "        cx = (xmin + (w_bbox/2))/75\n",
        "        cy = (ymin + (h_bbox/2))/75\n",
        "\n",
        "        cx_rcell = cx % self.cell_size / self.cell_size\n",
        "        cy_rcell = cy % self.cell_size / self.cell_size\n",
        "\n",
        "\n",
        "        label_one_hot = F.one_hot(torch.as_tensor(label, dtype=torch.int64), 10)\n",
        "        bbox_coord = torch.Tensor([cx_rcell, cy_rcell, rw, rh])\n",
        "\n",
        "        return image, label_one_hot, bbox_coord\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        image, one_hot_label, bbox_coord = self._transform_pasting75(self.dataset[idx][0], self.dataset[idx][1])\n",
        "        \n",
        "        return image, one_hot_label.to(torch.float), bbox_coord\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423,
          "referenced_widgets": [
            "9811f18275dd46f397a0dc445bcbd90a",
            "c63e5aa3160c49d28f9a6df08abb2558",
            "201eec9906304c4e95854aa2fca10439",
            "da28120108424e88bdb8599bd0f8d4f7",
            "83fbb691d40b4d02b029bcbdb2f85590",
            "30d2ecca2c46436198f644e8d6dc7f24",
            "6c99c88b52334f53a1a3c8e89f5c3d7f",
            "d5d88a84b1a24e98b3ee1d1b469defff",
            "cc8b573ffafd427495a1ccf3b818e815",
            "46612f5ff5024571917d3275717a5ae6",
            "8c60136a50bd4bf6ad83c4d26f8d8aa7",
            "06ff67cf46e64c3e8ba324f03a84c5ef",
            "b7a121501cfd438895e7bd2d85e56251",
            "c1c21fc51a224d60b314af87cbde69d4",
            "97522edebd964d2aa9671dac5a5f381d",
            "216234dec1484887a68608c520b3f093",
            "74c88f9633d541b0a60149df3d9a606c",
            "a4fef10f26744768b57986d380689412",
            "04f6d46789754cfd9f0a441baa32af45",
            "5f502d93a2fc46398195151da51c0083",
            "7dc6465a02ed4763b527f143ad9e254c",
            "7d20b57ac5b44fbd91e7cee432070247",
            "f85da4cd1a64460bb2580b69d8fa066d",
            "09ed36dea11c480d8e0f38478998b828",
            "e18767ed9f8944b5a8091d5f39e986d9",
            "58c90db1a5df4c0bb4d6c6cc8cd98dce",
            "85daafeb4a1a4b55bc6b63ba3f088bd5",
            "b60a12468347494d9c4fe04cd86af58e",
            "f68b6dd7bde64b71afb10a929c410ed9",
            "c136bc57bec64d38b2505a0c60e6f64b",
            "a66241a6cd464e339bef5fb2fceab412",
            "dd7117704fa347838c6b19067ebca54c",
            "15b3c78cbd51469ca2b870b993b6fca0",
            "809cb638d85e48999892dfd8cfc815a7",
            "9a64e70ecfbe46558b9865815d283596",
            "ae6fd6d93a5647a28ffd9af5e26942e6",
            "aad87ec81bb34aadac8c84ae9b505c68",
            "cdfd8330d4964e9fa41bfe4fd1639def",
            "c9418919bc204bb6af14a33815d1bd5f",
            "24e840cc10c54af0a0d40c6eb27beda1",
            "4ee695bbe8e543a2b6cc48b97ed63b3e",
            "54e7c876fe4d4be482733d96848b5a51",
            "b34a8acab2614688bef300d8be847cf6",
            "0963b6c3e45e4e5dabfd2e0d9b6cca38"
          ]
        },
        "id": "yQNznLfO8jOx",
        "outputId": "6979c79f-9d22-4eb4-aad5-bd90483983b6"
      },
      "outputs": [],
      "source": [
        "def get_training_dataset(BATCH_SIZE=64):\n",
        "    \"\"\"\n",
        "    Loads and maps the training split of the dataset using the custom dataset class. \n",
        "    \"\"\"\n",
        "    dataset = my_mnist_dataset(root=\"data\", split=\"train\", download=True)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    return dataloader, len(dataset)\n",
        "\n",
        "def get_validation_dataset(BATCH_SIZE = None):\n",
        "    \"\"\"\n",
        "    Loads and maps the validation split of the datasetusing the custom dataset class. \n",
        "    \"\"\"\n",
        "    dataset = my_mnist_dataset(root=\"data\", split=\"test\", download=True)\n",
        "    if BATCH_SIZE is None:\n",
        "        BATCH_SIZE = len(dataset)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    return dataloader, len(dataset)\n",
        "\n",
        "# instantiate the datasets\n",
        "training_dataset, len_training_ds = get_training_dataset()\n",
        "validation_dataset, len_validation_ds = get_validation_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wfgUx1srt90c"
      },
      "outputs": [],
      "source": [
        "class CNNBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(CNNBlock, self).__init__()\n",
        "        self.conv = torch.nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "        self.bn = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.l_relu = torch.nn.LeakyReLU(0.1)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        x = self.conv(input)\n",
        "        x = self.bn(x)\n",
        "        return self.l_relu(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3VjCkTEce-CT"
      },
      "outputs": [],
      "source": [
        "class YoloMNIST(torch.nn.Module):\n",
        "    def __init__(self, sizeHW, S, C, B):\n",
        "        super(YoloMNIST, self).__init__()\n",
        "        self.S, self.C, self.B = S, C, B\n",
        "        self.sizeHW = sizeHW\n",
        "        self.cell_size = self.sizeHW / self.S\n",
        "\n",
        "        self.seq = torch.nn.Sequential()        \n",
        "        self.seq.add_module(f\"conv_1\", CNNBlock(1, 32, stride=2, kernel_size=7, padding=2))\n",
        "        self.seq.add_module(f\"maxpool_1\", torch.nn.MaxPool2d(2))\n",
        "        self.seq.add_module(f\"conv_3\", CNNBlock(32, 128, stride=1, kernel_size=3, padding=0))\n",
        "        self.seq.add_module(f\"maxpool_2\", torch.nn.MaxPool2d(2))\n",
        "        self.seq.add_module(f\"conv_5\", CNNBlock(128, 64, stride=1, kernel_size=1, padding=0))\n",
        "        self.seq.add_module(f\"conv_4\", CNNBlock(64, 128, stride=1, kernel_size=3, padding=0))\n",
        "        self.seq.add_module(f\"conv_6\", CNNBlock(128, 128, stride=1, kernel_size=3, padding=1))\n",
        "        \n",
        "        self.fcs = self._create_fcs()\n",
        "\n",
        "    def _size_output(self, sizeHW:int, kernel:int, stride:int, padding:int=0, isMaxPool:bool=False)->int:\n",
        "        \"\"\"\n",
        "        Output size (width/height) of convolutional or maxpool layers.\n",
        "\n",
        "        Args:\n",
        "            sizeHW : int\n",
        "                Image size (we suppose this is a square image)\n",
        "            kernel : int\n",
        "                Size of a square kernel\n",
        "            stride : int\n",
        "                Stride of convolution layer\n",
        "            padding : int\n",
        "                Padding of convolution layer\n",
        "            isMaxPool : Bool, default is False.\n",
        "                Specify if it is a Maxpool layer (True) or not (False). \n",
        "\n",
        "        Return:\n",
        "            output_size : int\n",
        "                Image output size after a convolutional or MaxPool layer.\n",
        "        \"\"\" \n",
        "        if isMaxPool == True:\n",
        "            output_size = int(sizeHW/2)\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "        if padding == 'same':\n",
        "            output_size = sizeHW\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "        else:\n",
        "            output_size = (sizeHW + 2 * padding - (kernel-1)-1)/stride\n",
        "            output_size = int(output_size + 1)\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "\n",
        "    def _create_fcs(self):\n",
        "        output = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(128 * self.S * self.S, 4096),\n",
        "            torch.nn.LeakyReLU(0.1),\n",
        "            torch.nn.Linear(4096, self.S * self.S * (self.C + self.B*5))\n",
        "        )\n",
        "        return output\n",
        "    \n",
        "\n",
        "    def forward(self, input:torch.Tensor)->tuple:\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            input : torch.Tensor of shape (N, C, H, W)\n",
        "                Batch of images.\n",
        "\n",
        "        Return:\n",
        "            box_coord : torch.Tensor of shape (N, 6, 6, 5)\n",
        "                Contains xc_rcell, yc_rcell, rw, rh and the confidence number c\n",
        "                over 6x6 grid cells.\n",
        "            classifier : torch.Tensor of shape (N, 6, 6, 10)\n",
        "                Contains the one-hot encoding of each digit number over\n",
        "                6x6 grid cells.\n",
        "        \"\"\"     \n",
        "        x = self.seq(input)\n",
        "        x = self.fcs(x)\n",
        "        x = x.view(x.size(0), self.S, self.S, self.B * 5 + self.C)\n",
        "        box_coord = x[:,:,:,0:5]\n",
        "        classifier = x[:,:,:,5:]\n",
        "        return box_coord, classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "T_jOg_i_p2_c"
      },
      "outputs": [],
      "source": [
        "class YoloLoss(torch.nn.Module):\n",
        "    def __init__(self, lambd_coord:int, lambd_noobj:float, device:torch.device, S:int=6):\n",
        "        super(YoloLoss, self).__init__()\n",
        "        self.LAMBD_COORD = lambd_coord\n",
        "        self.LAMBD_NOOBJ = lambd_noobj\n",
        "        self.S = S\n",
        "        self.device = device\n",
        "\n",
        "    def _coordloss(self, pred_coord_rcell, true_coord_rcell):\n",
        "        \"\"\"\n",
        "        Args : \n",
        "            pred_coord_rcell : torch.Tensor of shape (N, 2)\n",
        "            true_coord_rcell : torch.Tensor of shape (N, 2)\n",
        "        Returns :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        xc_hat, yc_hat = pred_coord_rcell.permute(1,0)\n",
        "        xc, yc = true_coord_rcell.permute(1,0)\n",
        "\n",
        "        squared_error = torch.pow(xc - xc_hat,2) + torch.pow(yc - yc_hat,2)\n",
        "        return squared_error\n",
        "\n",
        "    def _sizeloss(self, pred_size, true_size):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_size : torch.Tensor of shape (N, 2)\n",
        "            true_size : torch.Tensor of shape (N, 2)\n",
        "        Returns : \n",
        "            root_squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        rw_hat, rh_hat = pred_size.permute(1,0)\n",
        "        rw, rh = true_size.permute(1,0)\n",
        "\n",
        "        #sizes can't be negative\n",
        "        rw_hat = rw_hat.clip(min=0)\n",
        "        rh_hat = rh_hat.clip(min=0)\n",
        "\n",
        "        root_squared_error_w = torch.pow(torch.sqrt(rw) - torch.sqrt(rw_hat),2)\n",
        "        root_squared_error_h = torch.pow(torch.sqrt(rh) - torch.sqrt(rh_hat),2)\n",
        "        root_squared_error = root_squared_error_w + root_squared_error_h\n",
        "        return root_squared_error\n",
        "\n",
        "    def _confidenceloss(self, pred_c, true_c):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_c : torch.Tensor of shape (N)\n",
        "            true_c : torch.Tensor of shape (N)\n",
        "        Return :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        squared_error = torch.pow(true_c - pred_c, 2)\n",
        "        return squared_error\n",
        "\n",
        "    def _classloss(self, pred_class, true_class):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_class : torch.Tensor of shape (N, 10)\n",
        "            true_class : torch.Tensor of shape (N, 10)\n",
        "        Returns :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        squared_error = torch.pow(true_class - pred_class, 2)\n",
        "        return torch.sum(squared_error, dim=1)\n",
        "\n",
        "    def forward(self, pred_box:torch.Tensor, true_box:torch.Tensor, pred_class:torch.Tensor, true_class:torch.Tensor):\n",
        "        \"\"\"\n",
        "        Grid forward pass.\n",
        "\n",
        "        Args:\n",
        "            pred_box : torch.Tensor of shape (N, S, S, 5)\n",
        "                Batch predicted outputs containing xc_rcell, yc_rcell, rw, rh,\n",
        "                and confident number c for each grid cell.\n",
        "            true_box : torch.Tensor of shape (N, S, S, 5)\n",
        "                Groundtrue batch containing bbox values for each cell and\n",
        "                c indicate if there is an object to detect or not (1/0).\n",
        "            pred_class : torch.Tensor of shape (N, S, S, 10)\n",
        "                Probability of each digit class in each grid cell\n",
        "            true_class : torch.Tensor of shape (N, 10)\n",
        "                one-hot vect of each digit\n",
        "\n",
        "        Return:\n",
        "            loss : float\n",
        "                The batch loss value of the grid\n",
        "        \"\"\"\n",
        "        BATCH_SIZE = len(pred_box)\n",
        "\n",
        "        ### Initialization of the losses\n",
        "        losses_list = ['loss_xy', 'loss_wh', 'loss_conf_obj', 'loss_conf_noobj', 'loss_class','isObject']\n",
        "        losses = {key : torch.zeros(BATCH_SIZE).to(self.device) for key in losses_list}\n",
        "        check_loss = []\n",
        "        ### Compute the losses for all images in the batch\n",
        "        for i in range(self.S):\n",
        "            for j in range(self.S):\n",
        "                ### Intersection over Union\n",
        "                #IoU = self._intersection_over_union(pred_box[:,i,j], true_box[:,i,j])\n",
        "\n",
        "                ### bbox coordinates\n",
        "                xy_hat = pred_box[:,i,j,:2]\n",
        "                xy = true_box[:,i,j,:2]\n",
        "                wh_hat = pred_box[:,i,j,2:4]\n",
        "                wh = true_box[:,i,j,2:4]\n",
        "                \n",
        "                ### confidence numbers\n",
        "                pred_c = pred_box[:,i,j,4]# * IoU\n",
        "                true_c = true_box[:,i,j,4]\n",
        "\n",
        "                ### objects to detect\n",
        "                isObject = true_c.to(torch.bool)\n",
        "                isNoObject = torch.logical_not(true_c) #(~bool) doesn't work on MPS device\n",
        "\n",
        "                ### sum the losses over the grid\n",
        "                losses['isObject'] += isObject\n",
        "                losses['loss_xy'] += isObject * self._coordloss(xy_hat, xy)\n",
        "                check_loss.append(losses['loss_xy'])\n",
        "                losses['loss_wh'] += isObject * self._sizeloss(wh_hat, wh)\n",
        "                losses['loss_conf_obj'] += isObject * self._confidenceloss(pred_c, true_c)\n",
        "                losses['loss_conf_noobj'] += isNoObject * self._confidenceloss(pred_c, true_c)\n",
        "                losses['loss_class'] += isObject * self._classloss(pred_class[:,i,j], true_class)\n",
        "        \n",
        "\n",
        "\n",
        "        ### Yolo_v1 loss over the batch, shape : (BATCH_SIZE)\n",
        "        loss = self.LAMBD_COORD * losses['loss_xy'] \\\n",
        "                + self.LAMBD_COORD * losses['loss_wh'] \\\n",
        "                + losses['loss_conf_obj'] \\\n",
        "                + self.LAMBD_NOOBJ * losses['loss_conf_noobj'] \\\n",
        "                + losses['loss_class']\n",
        "\n",
        "\n",
        "        assert torch.isnan(torch.sum(losses['loss_conf_obj']))==False, \"La loss {} est devenu nan\".format('loss_conf_obj')\n",
        "        assert torch.isnan(torch.sum(losses['loss_conf_noobj']))==False, \"La loss {} est devenu nan\".format('loss_conf_noobj')\n",
        "        assert torch.isnan(torch.sum(losses['loss_class']))==False, \"La loss {} est devenu nan\".format('loss_class')\n",
        "        assert torch.isnan(torch.sum(losses['isObject']))==False, \"La loss {} est devenu nan\".format('isObject')\n",
        "        assert torch.isnan(torch.sum(losses['loss_wh']))==False, \"La loss {} est devenu nan\".format('loss_wh')\n",
        "        assert torch.isnan(torch.sum(losses['loss_xy']))==False, \"La loss {} est devenu nan\".format('loss_xy')\n",
        "\n",
        "\n",
        "        loss = torch.sum(loss) / BATCH_SIZE\n",
        "\n",
        "        return check_loss, losses, loss\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OdIlkN61mXJj"
      },
      "outputs": [],
      "source": [
        "def bbox2Tensor(bbox:torch.Tensor, S:int=6, sizeHW:int=75, device=torch.device('cpu'))->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Constructs en Tensor and puts bbox values in the corresponding i,j grid cell.\n",
        "\n",
        "    Args :\n",
        "        bbox : torch.Tensor of shape (N,4)\n",
        "            Contains bbox values xc_rcell, yc_rcell, rw and rh.\n",
        "        S : int, default is 6\n",
        "            Size of the grid.\n",
        "        sizeHW : int, default is 75\n",
        "            Size of the image.\n",
        "\n",
        "    Return :\n",
        "        bbox_t : torch.Tensor of shape (N, S, S, 5)\n",
        "            Tensor containing all 4 bbox values in the corresponding i,j grid\n",
        "            cell position i.e. in the i,j position where an object should be\n",
        "            detected.\n",
        "    \"\"\"\n",
        "    N = len(bbox)\n",
        "    bbox_t = torch.zeros(N,S,S,5).to(device)\n",
        "    cell_size = sizeHW/S\n",
        "\n",
        "    xc_rcell, yc_rcell, rw, rh = bbox.permute(1,0).to(device)\n",
        "    xc = xc_rcell * cell_size - (1/cell_size) * (xc_rcell/cell_size).to(torch.int32)\n",
        "    yc = yc_rcell * cell_size - (1/cell_size) * (yc_rcell/cell_size).to(torch.int32)\n",
        "\n",
        "    N_range = torch.arange(N)\n",
        "    lines = (yc * S).to(torch.long)\n",
        "    columns = (xc * S).to(torch.long)\n",
        "    bbox_t[N_range, lines, columns] = torch.stack((xc_rcell, yc_rcell, rw, rh, torch.ones(N))).permute(1,0)\n",
        "    \n",
        "    return bbox_t.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "V9QFEV2oweFN"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cpu')\n",
        "loss_yolo = YoloLoss(lambd_coord=5, lambd_noobj=0.5, S=6, device=device)\n",
        "learning_rate = 0.00001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mHUOkfWPe-CW"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "model_MNIST = YoloMNIST(sizeHW=75, S=6, C=10, B=1)\n",
        "model_MNIST = model_MNIST.to(device)\n",
        "optimizer = torch.optim.Adam(params=model_MNIST.parameters(), lr=learning_rate, weight_decay=0.0005)\n",
        "loss_yolo = YoloLoss(lambd_coord=5, lambd_noobj=0.5, S=6, device=device)\n",
        "\n",
        "# print(optimizer)\n",
        "#summary(model_MNIST, input_size = (BATCH_SIZE,1,75,75))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9tSO0Qo7e-CX",
        "outputId": "30e9cb02-0115-4295-8901-373725937086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[START] : 2022-08-26 17:58:48 :\n",
            "[Training on] : CPU\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 1/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.09439\n",
            "xy_coord training loss for this batch : 0.00019\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.03031\n",
            "confidence without object training loss for this batch : 0.04939\n",
            "class proba training loss for this batch : 0.03779\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.08519\n",
            "xy_coord training loss for this batch : 0.00099\n",
            "wh_sizes training loss for this batch : 0.00031\n",
            "confidence with object training loss for this batch : 0.02470\n",
            "confidence without object training loss for this batch : 0.05698\n",
            "class proba training loss for this batch : 0.02549\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.10395\n",
            "xy_coord training loss for this batch : 0.00023\n",
            "wh_sizes training loss for this batch : 0.00018\n",
            "confidence with object training loss for this batch : 0.03329\n",
            "confidence without object training loss for this batch : 0.05603\n",
            "class proba training loss for this batch : 0.04058\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.10638\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.02419\n",
            "confidence without object training loss for this batch : 0.05187\n",
            "class proba training loss for this batch : 0.05551\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.10645\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.02743\n",
            "confidence without object training loss for this batch : 0.05348\n",
            "class proba training loss for this batch : 0.05138\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.13770\n",
            "xy_coord training loss for this batch : 0.00003\n",
            "wh_sizes training loss for this batch : 0.00020\n",
            "confidence with object training loss for this batch : 0.02019\n",
            "confidence without object training loss for this batch : 0.04441\n",
            "class proba training loss for this batch : 0.09417\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.07867\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.01098\n",
            "confidence without object training loss for this batch : 0.01979\n",
            "class proba training loss for this batch : 0.05674\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.12112\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.02806\n",
            "confidence without object training loss for this batch : 0.04989\n",
            "class proba training loss for this batch : 0.06707\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.08081\n",
            "xy_coord training loss for this batch : 0.00034\n",
            "wh_sizes training loss for this batch : 0.00050\n",
            "confidence with object training loss for this batch : 0.01680\n",
            "confidence without object training loss for this batch : 0.02968\n",
            "class proba training loss for this batch : 0.04498\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.05703\n",
            "xy_coord training loss for this batch : 0.00013\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.01417\n",
            "confidence without object training loss for this batch : 0.03282\n",
            "class proba training loss for this batch : 0.02498\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.08396\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.03066\n",
            "confidence without object training loss for this batch : 0.06265\n",
            "class proba training loss for this batch : 0.02106\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:57.650195\n",
            "Mean training loss for this epoch : 0.09038\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 2/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.09173\n",
            "xy_coord training loss for this batch : 0.00012\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.01858\n",
            "confidence without object training loss for this batch : 0.03295\n",
            "class proba training loss for this batch : 0.05538\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.06509\n",
            "xy_coord training loss for this batch : 0.00012\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.01541\n",
            "confidence without object training loss for this batch : 0.03136\n",
            "class proba training loss for this batch : 0.03270\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.07973\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.01547\n",
            "confidence without object training loss for this batch : 0.03445\n",
            "class proba training loss for this batch : 0.04607\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.07496\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.02685\n",
            "confidence without object training loss for this batch : 0.05310\n",
            "class proba training loss for this batch : 0.02053\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.08368\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00019\n",
            "confidence with object training loss for this batch : 0.01975\n",
            "confidence without object training loss for this batch : 0.03551\n",
            "class proba training loss for this batch : 0.04471\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.06606\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.01489\n",
            "confidence without object training loss for this batch : 0.03546\n",
            "class proba training loss for this batch : 0.03223\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.11111\n",
            "xy_coord training loss for this batch : 0.00018\n",
            "wh_sizes training loss for this batch : 0.00018\n",
            "confidence with object training loss for this batch : 0.02116\n",
            "confidence without object training loss for this batch : 0.03725\n",
            "class proba training loss for this batch : 0.06955\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.14527\n",
            "xy_coord training loss for this batch : 0.00013\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.05380\n",
            "confidence without object training loss for this batch : 0.08763\n",
            "class proba training loss for this batch : 0.04650\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.07322\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.02709\n",
            "confidence without object training loss for this batch : 0.04310\n",
            "class proba training loss for this batch : 0.02347\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.11145\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.04289\n",
            "confidence without object training loss for this batch : 0.06041\n",
            "class proba training loss for this batch : 0.03730\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.11547\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00016\n",
            "confidence with object training loss for this batch : 0.05234\n",
            "confidence without object training loss for this batch : 0.07041\n",
            "class proba training loss for this batch : 0.02666\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:56.459185\n",
            "Mean training loss for this epoch : 0.08923\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 3/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.08336\n",
            "xy_coord training loss for this batch : 0.00036\n",
            "wh_sizes training loss for this batch : 0.00025\n",
            "confidence with object training loss for this batch : 0.02421\n",
            "confidence without object training loss for this batch : 0.04690\n",
            "class proba training loss for this batch : 0.03268\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.06516\n",
            "xy_coord training loss for this batch : 0.00012\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.01404\n",
            "confidence without object training loss for this batch : 0.03006\n",
            "class proba training loss for this batch : 0.03482\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.08599\n",
            "xy_coord training loss for this batch : 0.00003\n",
            "wh_sizes training loss for this batch : 0.00020\n",
            "confidence with object training loss for this batch : 0.02775\n",
            "confidence without object training loss for this batch : 0.04570\n",
            "class proba training loss for this batch : 0.03421\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.06789\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.01728\n",
            "confidence without object training loss for this batch : 0.03030\n",
            "class proba training loss for this batch : 0.03451\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.09584\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00018\n",
            "confidence with object training loss for this batch : 0.02434\n",
            "confidence without object training loss for this batch : 0.03720\n",
            "class proba training loss for this batch : 0.05159\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.06190\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.01276\n",
            "confidence without object training loss for this batch : 0.02326\n",
            "class proba training loss for this batch : 0.03652\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.07418\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02107\n",
            "confidence without object training loss for this batch : 0.03494\n",
            "class proba training loss for this batch : 0.03481\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.06980\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.02522\n",
            "confidence without object training loss for this batch : 0.04467\n",
            "class proba training loss for this batch : 0.02124\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.09082\n",
            "xy_coord training loss for this batch : 0.00019\n",
            "wh_sizes training loss for this batch : 0.00020\n",
            "confidence with object training loss for this batch : 0.02565\n",
            "confidence without object training loss for this batch : 0.06633\n",
            "class proba training loss for this batch : 0.03006\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.06620\n",
            "xy_coord training loss for this batch : 0.00014\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.01700\n",
            "confidence without object training loss for this batch : 0.02717\n",
            "class proba training loss for this batch : 0.03439\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.09271\n",
            "xy_coord training loss for this batch : 0.00017\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.02175\n",
            "confidence without object training loss for this batch : 0.04452\n",
            "class proba training loss for this batch : 0.04726\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:56.631517\n",
            "Mean training loss for this epoch : 0.08772\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 4/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.08024\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00019\n",
            "confidence with object training loss for this batch : 0.02737\n",
            "confidence without object training loss for this batch : 0.04552\n",
            "class proba training loss for this batch : 0.02866\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.09471\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00018\n",
            "confidence with object training loss for this batch : 0.03293\n",
            "confidence without object training loss for this batch : 0.06195\n",
            "class proba training loss for this batch : 0.02950\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.10564\n",
            "xy_coord training loss for this batch : 0.00014\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.03292\n",
            "confidence without object training loss for this batch : 0.05261\n",
            "class proba training loss for this batch : 0.04511\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.07517\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.02474\n",
            "confidence without object training loss for this batch : 0.04318\n",
            "class proba training loss for this batch : 0.02789\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.10294\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.02586\n",
            "confidence without object training loss for this batch : 0.04466\n",
            "class proba training loss for this batch : 0.05376\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.08961\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02195\n",
            "confidence without object training loss for this batch : 0.03785\n",
            "class proba training loss for this batch : 0.04774\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.06810\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.01643\n",
            "confidence without object training loss for this batch : 0.02890\n",
            "class proba training loss for this batch : 0.03642\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.10339\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.03173\n",
            "confidence without object training loss for this batch : 0.04481\n",
            "class proba training loss for this batch : 0.04821\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.08301\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.02701\n",
            "confidence without object training loss for this batch : 0.04607\n",
            "class proba training loss for this batch : 0.03189\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.06550\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.01781\n",
            "confidence without object training loss for this batch : 0.04599\n",
            "class proba training loss for this batch : 0.02370\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.10133\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.02585\n",
            "confidence without object training loss for this batch : 0.04989\n",
            "class proba training loss for this batch : 0.04933\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:55.753316\n",
            "Mean training loss for this epoch : 0.08465\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 5/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.11983\n",
            "xy_coord training loss for this batch : 0.00015\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.03035\n",
            "confidence without object training loss for this batch : 0.04148\n",
            "class proba training loss for this batch : 0.06724\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.11224\n",
            "xy_coord training loss for this batch : 0.00027\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.03120\n",
            "confidence without object training loss for this batch : 0.05016\n",
            "class proba training loss for this batch : 0.05388\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.07145\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.02110\n",
            "confidence without object training loss for this batch : 0.04308\n",
            "class proba training loss for this batch : 0.02811\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.08510\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.01724\n",
            "confidence without object training loss for this batch : 0.03003\n",
            "class proba training loss for this batch : 0.05171\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.08496\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.02557\n",
            "confidence without object training loss for this batch : 0.04429\n",
            "class proba training loss for this batch : 0.03622\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.12778\n",
            "xy_coord training loss for this batch : 0.00004\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.02765\n",
            "confidence without object training loss for this batch : 0.05068\n",
            "class proba training loss for this batch : 0.07411\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.09183\n",
            "xy_coord training loss for this batch : 0.00016\n",
            "wh_sizes training loss for this batch : 0.00046\n",
            "confidence with object training loss for this batch : 0.02159\n",
            "confidence without object training loss for this batch : 0.03889\n",
            "class proba training loss for this batch : 0.04772\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.09356\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.02913\n",
            "confidence without object training loss for this batch : 0.05331\n",
            "class proba training loss for this batch : 0.03642\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.07143\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.02738\n",
            "confidence without object training loss for this batch : 0.04739\n",
            "class proba training loss for this batch : 0.01931\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.09531\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02921\n",
            "confidence without object training loss for this batch : 0.04613\n",
            "class proba training loss for this batch : 0.04205\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.09217\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.03564\n",
            "confidence without object training loss for this batch : 0.05594\n",
            "class proba training loss for this batch : 0.02745\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:55.334478\n",
            "Mean training loss for this epoch : 0.08431\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 6/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.07006\n",
            "xy_coord training loss for this batch : 0.00015\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.01923\n",
            "confidence without object training loss for this batch : 0.03845\n",
            "class proba training loss for this batch : 0.03015\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.09544\n",
            "xy_coord training loss for this batch : 0.00017\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.02448\n",
            "confidence without object training loss for this batch : 0.04319\n",
            "class proba training loss for this batch : 0.04785\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.08287\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.02044\n",
            "confidence without object training loss for this batch : 0.03612\n",
            "class proba training loss for this batch : 0.04363\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.07609\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00016\n",
            "confidence with object training loss for this batch : 0.02348\n",
            "confidence without object training loss for this batch : 0.03572\n",
            "class proba training loss for this batch : 0.03365\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.07497\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.03330\n",
            "confidence without object training loss for this batch : 0.04344\n",
            "class proba training loss for this batch : 0.01904\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.12991\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.04061\n",
            "confidence without object training loss for this batch : 0.06286\n",
            "class proba training loss for this batch : 0.05702\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.06424\n",
            "xy_coord training loss for this batch : 0.00015\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.01527\n",
            "confidence without object training loss for this batch : 0.03205\n",
            "class proba training loss for this batch : 0.03135\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.10622\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00023\n",
            "confidence with object training loss for this batch : 0.03240\n",
            "confidence without object training loss for this batch : 0.05534\n",
            "class proba training loss for this batch : 0.04450\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.05259\n",
            "xy_coord training loss for this batch : 0.00018\n",
            "wh_sizes training loss for this batch : 0.00023\n",
            "confidence with object training loss for this batch : 0.01570\n",
            "confidence without object training loss for this batch : 0.03024\n",
            "class proba training loss for this batch : 0.01975\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.08334\n",
            "xy_coord training loss for this batch : 0.00054\n",
            "wh_sizes training loss for this batch : 0.00021\n",
            "confidence with object training loss for this batch : 0.02755\n",
            "confidence without object training loss for this batch : 0.03871\n",
            "class proba training loss for this batch : 0.03268\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.12842\n",
            "xy_coord training loss for this batch : 0.00024\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.04882\n",
            "confidence without object training loss for this batch : 0.07335\n",
            "class proba training loss for this batch : 0.04102\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:56.721960\n",
            "Mean training loss for this epoch : 0.08369\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 7/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.05451\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00019\n",
            "confidence with object training loss for this batch : 0.01478\n",
            "confidence without object training loss for this batch : 0.02802\n",
            "class proba training loss for this batch : 0.02422\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.06715\n",
            "xy_coord training loss for this batch : 0.00014\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.02101\n",
            "confidence without object training loss for this batch : 0.03877\n",
            "class proba training loss for this batch : 0.02538\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.06385\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.01487\n",
            "confidence without object training loss for this batch : 0.03632\n",
            "class proba training loss for this batch : 0.02952\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.10680\n",
            "xy_coord training loss for this batch : 0.00013\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.03402\n",
            "confidence without object training loss for this batch : 0.05434\n",
            "class proba training loss for this batch : 0.04432\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.10260\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.04385\n",
            "confidence without object training loss for this batch : 0.05872\n",
            "class proba training loss for this batch : 0.02817\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.09076\n",
            "xy_coord training loss for this batch : 0.00018\n",
            "wh_sizes training loss for this batch : 0.00018\n",
            "confidence with object training loss for this batch : 0.02743\n",
            "confidence without object training loss for this batch : 0.05180\n",
            "class proba training loss for this batch : 0.03565\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.05871\n",
            "xy_coord training loss for this batch : 0.00004\n",
            "wh_sizes training loss for this batch : 0.00007\n",
            "confidence with object training loss for this batch : 0.01105\n",
            "confidence without object training loss for this batch : 0.02359\n",
            "class proba training loss for this batch : 0.03532\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.04066\n",
            "xy_coord training loss for this batch : 0.00003\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.01180\n",
            "confidence without object training loss for this batch : 0.02655\n",
            "class proba training loss for this batch : 0.01506\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.07510\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.02218\n",
            "confidence without object training loss for this batch : 0.03829\n",
            "class proba training loss for this batch : 0.03287\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.05745\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.01998\n",
            "confidence without object training loss for this batch : 0.03895\n",
            "class proba training loss for this batch : 0.01697\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.19073\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.02661\n",
            "confidence without object training loss for this batch : 0.04557\n",
            "class proba training loss for this batch : 0.14070\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:12.574158\n",
            "Mean training loss for this epoch : 0.08056\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 8/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.07927\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00007\n",
            "confidence with object training loss for this batch : 0.02852\n",
            "confidence without object training loss for this batch : 0.04449\n",
            "class proba training loss for this batch : 0.02766\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.10151\n",
            "xy_coord training loss for this batch : 0.00024\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.03401\n",
            "confidence without object training loss for this batch : 0.05782\n",
            "class proba training loss for this batch : 0.03673\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.07040\n",
            "xy_coord training loss for this batch : 0.00013\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.02990\n",
            "confidence without object training loss for this batch : 0.05196\n",
            "class proba training loss for this batch : 0.01314\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.07875\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.02615\n",
            "confidence without object training loss for this batch : 0.04449\n",
            "class proba training loss for this batch : 0.02938\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.09732\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02366\n",
            "confidence without object training loss for this batch : 0.04658\n",
            "class proba training loss for this batch : 0.04968\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.07584\n",
            "xy_coord training loss for this batch : 0.00020\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.02877\n",
            "confidence without object training loss for this batch : 0.04067\n",
            "class proba training loss for this batch : 0.02504\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.10806\n",
            "xy_coord training loss for this batch : 0.00024\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.04151\n",
            "confidence without object training loss for this batch : 0.06483\n",
            "class proba training loss for this batch : 0.03224\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.09691\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.03430\n",
            "confidence without object training loss for this batch : 0.05421\n",
            "class proba training loss for this batch : 0.03440\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.07912\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.03432\n",
            "confidence without object training loss for this batch : 0.04667\n",
            "class proba training loss for this batch : 0.02029\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.07137\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00005\n",
            "confidence with object training loss for this batch : 0.02058\n",
            "confidence without object training loss for this batch : 0.03914\n",
            "class proba training loss for this batch : 0.03063\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.05552\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.01215\n",
            "confidence without object training loss for this batch : 0.01576\n",
            "class proba training loss for this batch : 0.03446\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:06.819568\n",
            "Mean training loss for this epoch : 0.08130\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 9/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.06489\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.00946\n",
            "confidence without object training loss for this batch : 0.02048\n",
            "class proba training loss for this batch : 0.04447\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.05294\n",
            "xy_coord training loss for this batch : 0.00004\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.01566\n",
            "confidence without object training loss for this batch : 0.02475\n",
            "class proba training loss for this batch : 0.02422\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.07293\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02097\n",
            "confidence without object training loss for this batch : 0.03436\n",
            "class proba training loss for this batch : 0.03407\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.10113\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.02283\n",
            "confidence without object training loss for this batch : 0.03617\n",
            "class proba training loss for this batch : 0.05917\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.06490\n",
            "xy_coord training loss for this batch : 0.00026\n",
            "wh_sizes training loss for this batch : 0.00020\n",
            "confidence with object training loss for this batch : 0.02060\n",
            "confidence without object training loss for this batch : 0.04534\n",
            "class proba training loss for this batch : 0.01933\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.11825\n",
            "xy_coord training loss for this batch : 0.00014\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.02599\n",
            "confidence without object training loss for this batch : 0.05779\n",
            "class proba training loss for this batch : 0.06195\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.06326\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.01985\n",
            "confidence without object training loss for this batch : 0.02962\n",
            "class proba training loss for this batch : 0.02780\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.06217\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00016\n",
            "confidence with object training loss for this batch : 0.02094\n",
            "confidence without object training loss for this batch : 0.03773\n",
            "class proba training loss for this batch : 0.02132\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.13603\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.05199\n",
            "confidence without object training loss for this batch : 0.07714\n",
            "class proba training loss for this batch : 0.04419\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.11675\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.03062\n",
            "confidence without object training loss for this batch : 0.04582\n",
            "class proba training loss for this batch : 0.06231\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.08727\n",
            "xy_coord training loss for this batch : 0.00004\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.03553\n",
            "confidence without object training loss for this batch : 0.06989\n",
            "class proba training loss for this batch : 0.01582\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:59.223043\n",
            "Mean training loss for this epoch : 0.08019\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 10/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.10570\n",
            "xy_coord training loss for this batch : 0.00021\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.03178\n",
            "confidence without object training loss for this batch : 0.06125\n",
            "class proba training loss for this batch : 0.04160\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.06053\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.01647\n",
            "confidence without object training loss for this batch : 0.03675\n",
            "class proba training loss for this batch : 0.02472\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.05142\n",
            "xy_coord training loss for this batch : 0.00015\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.01232\n",
            "confidence without object training loss for this batch : 0.02203\n",
            "class proba training loss for this batch : 0.02656\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.14592\n",
            "xy_coord training loss for this batch : 0.00020\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.04142\n",
            "confidence without object training loss for this batch : 0.07727\n",
            "class proba training loss for this batch : 0.06407\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.07642\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02089\n",
            "confidence without object training loss for this batch : 0.03983\n",
            "class proba training loss for this batch : 0.03486\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.07278\n",
            "xy_coord training loss for this batch : 0.00012\n",
            "wh_sizes training loss for this batch : 0.00020\n",
            "confidence with object training loss for this batch : 0.02589\n",
            "confidence without object training loss for this batch : 0.04409\n",
            "class proba training loss for this batch : 0.02326\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.06789\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.02204\n",
            "confidence without object training loss for this batch : 0.03870\n",
            "class proba training loss for this batch : 0.02569\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.07532\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.02212\n",
            "confidence without object training loss for this batch : 0.03580\n",
            "class proba training loss for this batch : 0.03447\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.05659\n",
            "xy_coord training loss for this batch : 0.00014\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.01872\n",
            "confidence without object training loss for this batch : 0.02611\n",
            "class proba training loss for this batch : 0.02362\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.08370\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02106\n",
            "confidence without object training loss for this batch : 0.03717\n",
            "class proba training loss for this batch : 0.04328\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.08118\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.00537\n",
            "confidence without object training loss for this batch : 0.00688\n",
            "class proba training loss for this batch : 0.07170\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:01.382402\n",
            "Mean training loss for this epoch : 0.07840\n"
          ]
        }
      ],
      "source": [
        "delta_time = datetime.timedelta(hours=1)\n",
        "timezone = datetime.timezone(offset=delta_time)\n",
        "\n",
        "t = datetime.datetime.now(tz=timezone)\n",
        "str_t = '{:%Y-%m-%d %H:%M:%S}'.format(t)\n",
        "print(f\"[START] : {str_t} :\")\n",
        "print(f\"[Training on] : {str(device).upper()}\")\n",
        "\n",
        "EPOCHS = 10\n",
        "size_grid = 6\n",
        "batch_loss_list = []\n",
        "model_MNIST = model_MNIST.to(device)\n",
        "check = []\n",
        "\n",
        "for epoch in range(EPOCHS) : \n",
        "    begin_time = timer()\n",
        "    epochs_loss = 0.\n",
        "    \n",
        "    print(\"-\"*20)\n",
        "    str_t = '{:%Y-%m-%d %H:%M:%S}'.format(t)\n",
        "    print(\" \"*5 + f\"{str_t} : EPOCH {epoch+1}/{EPOCHS}\")\n",
        "    print(\"-\"*20)\n",
        "\n",
        "    model_MNIST.train()\n",
        "    for batch, (img, labels, bbox_true) in enumerate(training_dataset):\n",
        "        loss = 0\n",
        "        begin_batch_time = timer()\n",
        "        img, labels, bbox_true = img.to(device), labels.to(device), bbox_true.to(device)\n",
        "        \n",
        "        ### turn bbox into NxSxSx5 tensor\n",
        "        bbox_true_6x6 = bbox2Tensor(bbox_true).to(device)\n",
        "        \n",
        "        ### clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        ### compute predictions\n",
        "        bbox_preds, label_preds = model_MNIST(img)\n",
        "        \n",
        "        ### compute losses over each grid cell for each image in the batch\n",
        "        check_xy, losses, loss = loss_yolo(bbox_preds, bbox_true_6x6, label_preds, labels)\n",
        "        check.append(check_xy)\n",
        "    \n",
        "        ### compute gradients\n",
        "        loss.backward()\n",
        "        \n",
        "        ### Weight updates\n",
        "        optimizer.step()\n",
        "        \n",
        "        ######### print part #######################\n",
        "        current_loss = loss.item()\n",
        "        batch_loss_list.append(current_loss)\n",
        "        epochs_loss = epochs_loss + current_loss\n",
        "\n",
        "        if batch+1 <= len_training_ds//BATCH_SIZE:\n",
        "            current_training_sample = (batch+1)*BATCH_SIZE\n",
        "        else:\n",
        "            current_training_sample = (batch)*BATCH_SIZE + len_training_ds%BATCH_SIZE\n",
        "        \n",
        "        if (batch) == 0 or (batch+1)%100 == 0 or batch == len_training_ds//BATCH_SIZE:\n",
        "            print(f\" --- Image : {current_training_sample}/{len_training_ds}\",\\\n",
        "                    f\" : loss = {current_loss:.5f}\")\n",
        "            print(f\"xy_coord training loss for this batch : {torch.sum(losses['loss_xy']) / len(img):.5f}\")\n",
        "            print(f\"wh_sizes training loss for this batch : {torch.sum(losses['loss_wh']) / len(img):.5f}\")\n",
        "            print(f\"confidence with object training loss for this batch : {torch.sum(losses['loss_conf_obj']) / len(img):.5f}\")\n",
        "            print(f\"confidence without object training loss for this batch : {torch.sum(losses['loss_conf_noobj']) / len(img):.5f}\")\n",
        "            print(f\"class proba training loss for this batch : {torch.sum(losses['loss_class']) / len(img):.5f}\")\n",
        "            print('\\n')\n",
        "            if batch == (len_training_ds//BATCH_SIZE):\n",
        "                print(f\"Total elapsed time for training : {datetime.timedelta(seconds=timer()-begin_time)}\")\n",
        "                print(f\"Mean training loss for this epoch : {epochs_loss / len(training_dataset):.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "pS0A-3zLweFR"
      },
      "outputs": [],
      "source": [
        "torch.save(model_MNIST.state_dict(), \"yolo_mnist_model_Xepochs.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "7O8u0S3PweFR"
      },
      "outputs": [],
      "source": [
        "def relative2absolute(bbox_relative:torch.Tensor, SIZEHW=75, S=6)->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Turns bounding box relative to cell coordinates into absolute coordinates \n",
        "    (pixels). Used to calculate IoU. \n",
        "\n",
        "    Args:\n",
        "        bbox_relative : torch.Tensor of shape (N, 4)\n",
        "            Bounding box coordinates to convert.\n",
        "    Return:\n",
        "        bbox_absolute : torch.Tensor of shape (N, 4)\n",
        "    \"\"\"\n",
        "    CELL_SIZE = SIZEHW/S\n",
        "\n",
        "    cx_rcell, cy_rcell, rw, rh = bbox_relative[:,:4].permute(1,0)\n",
        "    \n",
        "    ### xc,yc centers relative to the frame coordinates\n",
        "    cx = cx_rcell * CELL_SIZE - (1/CELL_SIZE) * (cx_rcell/CELL_SIZE).to(torch.int32)\n",
        "    cy = cy_rcell * CELL_SIZE - (1/CELL_SIZE) * (cy_rcell/CELL_SIZE).to(torch.int32)\n",
        "\n",
        "    ### xc,yc centers absolute coordinates\n",
        "    cx_abs = SIZEHW * cx\n",
        "    cy_abs = SIZEHW * cy\n",
        "\n",
        "    ### x,y absolute positions \n",
        "    x_min = cx_abs - (SIZEHW * (rw/2))\n",
        "    y_min = cy_abs - (SIZEHW * (rh/2))\n",
        "    x_max = cx_abs + (SIZEHW * (rw/2))\n",
        "    y_max = cy_abs + (SIZEHW * (rh/2))\n",
        "\n",
        "    bbox_absolute = torch.stack((x_min, y_min, x_max, y_max), dim=-1)\n",
        "    return bbox_absolute\n",
        "\n",
        "def intersection_over_union(pred_box:torch.Tensor, true_box:torch.Tensor)->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Intersection over Union method.\n",
        "\n",
        "    Args:\n",
        "        pred_box : torch.Tensor of shape (N, 5)\n",
        "            Predicted bounding boxes of a batch, in a given cell.\n",
        "        true_box : torch.Tensor of shape (N, 5)\n",
        "            Ground truth bounding boxes of a batch, in a given cell.\n",
        "\n",
        "    Return:\n",
        "        iou : float\n",
        "            Number between 0 and 1 where 1 is a perfect overlap.\n",
        "    \"\"\"\n",
        "    ### Convert cell reltative coordinates to absolute coordinates\n",
        "    pred_box = relative2absolute(pred_box)\n",
        "    true_box = relative2absolute(true_box)   \n",
        "    xmin_pred, ymin_pred, xmax_pred, ymax_pred = pred_box.permute(1,0)\n",
        "    xmin_true, ymin_true, xmax_true, ymax_true = true_box.permute(1,0)\n",
        "\n",
        "    ### There is no object if all coordinates are zero\n",
        "    isObject = xmin_true + ymin_true + xmax_true + ymax_true\n",
        "    isObject = isObject.to(torch.bool)\n",
        "\n",
        "    smoothing_factor = 1e-10\n",
        "\n",
        "    ### x, y overlaps btw pred and groundtrue\n",
        "    xmin_overlap = torch.maximum(xmin_pred, xmin_true)\n",
        "    xmax_overlap = torch.minimum(xmax_pred, xmax_true)\n",
        "    ymin_overlap = torch.maximum(ymin_pred, ymin_true)\n",
        "    ymax_overlap = torch.minimum(ymax_pred, ymax_true)\n",
        "    \n",
        "    ### Pred and groundtrue areas\n",
        "    pred_box_area = (xmax_pred - xmin_pred) * (ymax_pred - ymin_pred)\n",
        "    true_box_area = (xmax_true - xmin_true) * (ymax_true - ymin_true)\n",
        "\n",
        "    ### Compute intersection area, union area and IoU\n",
        "    overlap_area = torch.maximum((xmax_overlap - xmin_overlap), torch.Tensor([0]).to(device)) * torch.maximum((ymax_overlap - ymin_overlap), torch.Tensor([0]).to(device))\n",
        "    union_area = (pred_box_area + true_box_area) - overlap_area\n",
        "    iou = (overlap_area + smoothing_factor) / (union_area + smoothing_factor)\n",
        "    \n",
        "    ### Set IoU to zero when there is no coordinates (i.e. no object)\n",
        "    iou = iou * isObject\n",
        "\n",
        "    return iou   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "OWc1Cr8yrSH8",
        "outputId": "bd79409e-b55c-42a0-fa11-f609b48225cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE BOX : 0.00018\n",
            "MSE confidence score : 0.21276\n",
            "class acc : 99.04%\n"
          ]
        }
      ],
      "source": [
        "S=6\n",
        "for (img, labels, bbox_true) in validation_dataset:\n",
        "    img, labels, bbox_true = img.to(device), labels.to(device), bbox_true.to(device)\n",
        "    model_MNIST.eval()\n",
        "    with torch.no_grad():\n",
        "        ### prediction\n",
        "        bbox_preds, label_preds = model_MNIST(img)\n",
        "\n",
        "        ### (N,4) -> (N, S, S, 5)\n",
        "        bbox_true_6x6 = bbox2Tensor(bbox_true).to(device)\n",
        "\n",
        "        ### keeping only cells (i,j) with an object \n",
        "        cells_with_obj = bbox_true_6x6.nonzero()[::5]\n",
        "        N, cells_i, cells_j, _ = cells_with_obj.permute(1,0)\n",
        "\n",
        "        ### MSE along bbox coordinates and sizes in the cells containing an object\n",
        "        mse_box = (1/len(img)) * torch.sum(torch.pow(bbox_true - bbox_preds[N, cells_i, cells_j,:4],2))\n",
        "        \n",
        "        ### confidence score accuracy : sum of the all grid confidence scores\n",
        "        ### pred confidence score is confidence score times IoU.\n",
        "        mse_confidence_score = torch.zeros(len(img))\n",
        "        for i in range(S):\n",
        "            for j in range(S):\n",
        "                iou = intersection_over_union(bbox_true_6x6[:,i,j], bbox_preds[:,i,j])\n",
        "                mse_confidence_score += torch.pow(bbox_true_6x6[:,i,j,-1] - bbox_preds[:,i,j,-1] * iou,2)\n",
        "        \n",
        "        mse_confidence_score = (1/(len(img))) * torch.sum(mse_confidence_score)\n",
        "\n",
        "        ### applied softmax to class predictions and compute accuracy\n",
        "        softmax_pred_classes = torch.softmax(label_preds[N, cells_i, cells_j], dim=1)\n",
        "        classes_acc = (1/len(img)) * torch.sum(torch.argmax(labels, dim=1) == torch.argmax(softmax_pred_classes, dim=1))\n",
        "\n",
        "print(f\"MSE BOX : {mse_box.item():.5f}\")\n",
        "print(f\"MSE confidence score : {mse_confidence_score.item():.5f}\")\n",
        "print(f\"class acc : {classes_acc.item()*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "yb1WSNT-xqSI"
      },
      "outputs": [],
      "source": [
        "im_width = 75\n",
        "im_height = 75\n",
        "use_normalized_coordinates = True\n",
        "\n",
        "def draw_ONE_bounding_box_on_image(image, ymin:int, xmin:int, ymax:int, xmax:int, \n",
        "                               color:str='red', thickness:int=1, display_str:bool=None, \n",
        "                               use_normalized_coordinates:bool=True):\n",
        "  \"\"\"Adds a bounding box to an image.\n",
        "  Bounding box coordinates can be specified in either absolute (pixel) or\n",
        "  normalized coordinates by setting the use_normalized_coordinates argument.\n",
        "  \n",
        "  Args:\n",
        "    image: a PIL.Image object.\n",
        "    ymin: ymin of bounding box.\n",
        "    xmin: xmin of bounding box.\n",
        "    ymax: ymax of bounding box.\n",
        "    xmax: xmax of bounding box.\n",
        "    color: color to draw bounding box. Default is red.\n",
        "    thickness: line thickness. Default value is 4.\n",
        "    display_str_list: string to display in box\n",
        "    use_normalized_coordinates: If True (default), treat coordinates\n",
        "      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n",
        "      coordinates as absolute.\n",
        "  \"\"\"\n",
        "  draw = PIL.ImageDraw.Draw(image)\n",
        "  im_width, im_height = image.size\n",
        "  \n",
        "  if use_normalized_coordinates:\n",
        "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
        "                                  ymin * im_height, ymax * im_height)\n",
        "  else:\n",
        "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
        "  draw.line([(left, top), (left, bottom), (right, bottom),\n",
        "             (right, top), (left, top)], width=thickness, fill=color)\n",
        "\n",
        "\n",
        "\n",
        "def draw_bounding_boxes_on_image(image, boxes:np.ndarray, color:list=[], \n",
        "                                 thickness:int=1, display_str_list:tuple=()):\n",
        "  \"\"\"Draws bounding boxes on image.\n",
        "\n",
        "  Args:\n",
        "    image: a PIL.Image object.\n",
        "    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
        "           The coordinates are in normalized format between [0, 1].\n",
        "    color: color to draw bounding box. Default is red.\n",
        "    thickness: line thickness. Default value is 4.\n",
        "    display_str_list: a list of strings for each bounding box.\n",
        "                           \n",
        "  Raises:\n",
        "    ValueError: if boxes is not a [N, 4] array\n",
        "  \"\"\"\n",
        "  boxes_shape = boxes.shape\n",
        "  if not boxes_shape:\n",
        "    return\n",
        "  if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n",
        "    raise ValueError('Input must be of size [N, 4]')\n",
        "  \n",
        "  for i in range(boxes_shape[0]):\n",
        "    draw_ONE_bounding_box_on_image(image, \n",
        "                                   boxes[i, 1], boxes[i, 0], \n",
        "                                   boxes[i, 3], boxes[i, 2], \n",
        "                                   color[i], thickness, display_str_list[i])\n",
        "\n",
        "\n",
        "def draw_bounding_boxes_on_image_array(image:np.ndarray, boxes:np.ndarray, color:list=[], \n",
        "                                       thickness:int=1, display_str_list:tuple=()):\n",
        "  \"\"\"Draws bounding boxes on image (numpy array).\n",
        "\n",
        "  Args:\n",
        "    image: a numpy array object.\n",
        "    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
        "           The coordinates are in normalized format between [0, 1].\n",
        "    color: color to draw bounding box. Default is red.\n",
        "    thickness: line thickness. Default value is 4.\n",
        "    display_str_list_list: a list of strings for each bounding box.\n",
        "  Raises:\n",
        "    ValueError: if boxes is not a [N, 4] array\n",
        "  \"\"\"\n",
        "  image_pil = PIL.Image.fromarray(image)\n",
        "  rgbimg = PIL.Image.new(\"RGBA\", image_pil.size)\n",
        "  rgbimg.paste(image_pil)\n",
        "  draw_bounding_boxes_on_image(rgbimg, boxes, color, thickness, display_str_list)\n",
        "  return np.array(rgbimg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "iAUtKFv24Rhq"
      },
      "outputs": [],
      "source": [
        "############### Matplotlib config\n",
        "plt.rc('image', cmap='gray')\n",
        "plt.rc('grid', linewidth=0)\n",
        "plt.rc('xtick', top=False, bottom=False, labelsize='large')\n",
        "plt.rc('ytick', left=False, right=False, labelsize='large')\n",
        "plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')\n",
        "plt.rc('text', color='a8151a')\n",
        "plt.rc('figure', facecolor='F0F0F0')# Matplotlib fonts\n",
        "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), \"mpl-data/fonts/ttf\")\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "rEEBQaok5TFu"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "def display_digits_with_boxes(digits, predictions, labels, pred_bboxes, bboxes, title):\n",
        "  \"\"\"Utility to display a row of digits with their predictions.\n",
        "\n",
        "  Args:\n",
        "    digits : np.ndarray of shape (N,75,75,1)\n",
        "        Raw image with normalized pixel values (from 0 to 1)\n",
        "    predictions : np.ndarray of shape (N,)\n",
        "        Predicted label with the same shape as labels\n",
        "    labels : np.ndarray of shape (N,)\n",
        "        Labels of the digits (from 0 to 9)\n",
        "    pred_bboxes : np.ndarray of shape (N, 4) ??\n",
        "        Predicted bboxes locations\n",
        "    bboxes : np.ndarray of shape (N, 4)\n",
        "        Ground true bboxe locations\n",
        "    iou : list of shape (N,)\n",
        "        IoU of each bboxes\n",
        "    title : str\n",
        "        Figure's title\n",
        "  \"\"\"\n",
        "  iou_threshold = 0.6\n",
        "  n = 10\n",
        "  indexes = np.random.choice(len(predictions), size=n)\n",
        "  n_digits = digits[indexes].numpy()\n",
        "  n_predictions6x6 = predictions[indexes]\n",
        "  n_labels = labels[indexes]\n",
        "  n_labels = torch.argmax(n_labels, dim=1).numpy()\n",
        "\n",
        "  if (len(pred_bboxes) > 0):\n",
        "    # If multiple bboxes predicted\n",
        "    n_pred_bboxes6x6 = pred_bboxes[indexes,:]\n",
        "\n",
        "  if (len(bboxes) > 0):\n",
        "    # If multiple ground truth bboxes\n",
        "    n_bboxes_rel = bboxes[indexes,:]\n",
        "\n",
        "  # Rescale pixel values to un-normed values (from 0 -black- to 255 -white-)\n",
        "  n_digits = n_digits * 255.0\n",
        "  n_digits = n_digits.reshape(n, 75, 75)\n",
        "\n",
        "  n_bboxes = relative2absolute(torch.as_tensor(n_bboxes_rel))\n",
        "  n_bboxes = np.asarray(n_bboxes_rel)/75\n",
        "\n",
        "\n",
        "  # Set plot config\n",
        "  fig = plt.figure(figsize=(20, 4))\n",
        "  plt.title(title)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  \n",
        "\n",
        "  for i in range(n):\n",
        "    bboxes_to_plot = []\n",
        "    for cell_i in range(6):\n",
        "      for cell_j in range(6):\n",
        "        n_pred_bboxes = n_pred_bboxes6x6[:, cell_i, cell_j, :4]\n",
        "        \n",
        "        n_iou = intersection_over_union(n_pred_bboxes, n_bboxes_rel)\n",
        "        \n",
        "        softmax_pred_classes = torch.softmax(n_predictions6x6[:, cell_i, cell_j], dim=1)\n",
        "        n_predictions = torch.argmax(softmax_pred_classes, dim=1).numpy()\n",
        "\n",
        "        n_pred_bboxes = relative2absolute(n_pred_bboxes)\n",
        "        n_pred_bboxes = np.asarray(n_pred_bboxes)\n",
        "        \n",
        "        if (len(pred_bboxes) > i):\n",
        "          bboxes_to_plot.append(n_pred_bboxes[i])\n",
        "        \n",
        "    if (len(bboxes) > i):\n",
        "      bboxes_to_plot.append(n_bboxes[i])\n",
        "    \n",
        "    print(len(bboxes_to_plot))\n",
        "    ax = fig.add_subplot(1, n, i+1)\n",
        "    img_to_draw = draw_bounding_boxes_on_image_array(image=n_digits[i], boxes=np.asarray(bboxes_to_plot), color=['red', 'green'], display_str_list=[\"true\", \"pred\"])\n",
        "    plt.xlabel(n_predictions[i])\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    \n",
        "    if n_predictions[i] != n_labels[i]:\n",
        "      ax.xaxis.label.set_color('red')\n",
        "    \n",
        "    plt.imshow(img_to_draw)\n",
        "    \n",
        "    # if len(iou) > i :\n",
        "    color = \"black\"\n",
        "    if (n_iou[i] < iou_threshold):\n",
        "      color = \"red\"\n",
        "    ax.text(0.2, -0.3, \"iou: %s\" %(n_iou[i]), color=color, transform=ax.transAxes)\n",
        "        \n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "def dataset_to_numpy_util(training_dataset, validation_dataset, N):\n",
        "  \"\"\"\n",
        "  Pull a batch from the datasets. This code is not very nice.\n",
        "  \n",
        "  Args:\n",
        "    training_dataset : torch.utils.data.Dataset\n",
        "        Dataset from the torch.utils.data.Dataset Pytorch class, returning the \n",
        "        training digits, labels and bboxes coordinates as batches such as : \n",
        "            - training digits : torch.Tensor of shape (batch_size, 1, 75, 75)\n",
        "            - labels : torch.Tensor of shape (batch_size, 10)\n",
        "            - bboxes coordinates : torch.Tensor of shape (batch_size, 4)\n",
        "    validation_dataset : torch.utils.data.Dataset \n",
        "        Dataset from the torch.utils.data.Dataset Pytorch class, returning the \n",
        "        whole validation digits, labels and bboxes coordinates.\n",
        "    N : int\n",
        "        Size of the training sample to extract from the training dataset\n",
        "\n",
        "  Returns:\n",
        "    N_train_ds_digits : np.ndarray of shape (N, 1, 75, 75)\n",
        "    N_train_ds_labels : np.ndarray of shape (N, 10)\n",
        "    N_train_ds_bboxes : np.ndarray of shape (N, 4)\n",
        "    validation_digits : np.ndarray of shape (len(validation_dataset), 1, 75, 75)\n",
        "    validation_labels : np.ndarray of shape (len(validation_dataset), 10)\n",
        "    validation_bboxes : np.ndarray of shape (len(validation_dataset), 4)\n",
        "  \"\"\"\n",
        "  ### get N training digits, labels and bboxes from one batch\n",
        "  ### turning the bboxes coordinates into ndarrays\n",
        "  one_batch_train_ds_digits, one_batch_train_ds_labels, one_batch_train_ds_bboxes = next(iter(training_dataset))\n",
        "  N_train_ds_digits = one_batch_train_ds_digits[:N].numpy()\n",
        "  N_train_ds_labels = one_batch_train_ds_labels[:N].numpy()\n",
        "  N_train_ds_bboxes = one_batch_train_ds_bboxes[:N].numpy()\n",
        "  \n",
        "  ### get the whole validation digits, labels and bboxes\n",
        "  ### turning the bboxes coordinates into ndarrays\n",
        "  for validation_digits, validation_labels, validation_bboxes in validation_dataset:\n",
        "      validation_digits = validation_digits.numpy()\n",
        "      validation_labels = validation_labels.numpy()\n",
        "      validation_bboxes = validation_bboxes.numpy()\n",
        "      break\n",
        "\n",
        "  # turning one hot encoding labels into the corresponding digit\n",
        "  validation_labels = np.argmax(validation_labels, axis=1)\n",
        "  N_train_ds_labels = np.argmax(N_train_ds_labels, axis=1)\n",
        "\n",
        "  return (N_train_ds_digits, N_train_ds_labels, N_train_ds_bboxes,\n",
        "          validation_digits, validation_labels, validation_bboxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "NDK68abLxos6",
        "outputId": "2200125f-2447-4f7d-8d90-88819290f473"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb Cellule 19\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# (training_digits, training_labels, training_bboxes,\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#  validation_digits, validation_labels, validation_bboxes) = dataset_to_numpy_util(training_dataset, validation_dataset, 10)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#print(\"Number of predictions where iou > threshold(%s): %s\" % (iou_threshold, (iou >= iou_threshold).sum()))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#print(\"Number of predictions where iou < threshold(%s): %s\" % (iou_threshold, (iou < iou_threshold).sum()))\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m display_digits_with_boxes(img, label_preds, labels, bbox_preds, bbox_true, \u001b[39m\"\u001b[39;49m\u001b[39mvalidation digits and their labels\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "\u001b[1;32m/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb Cellule 19\u001b[0m in \u001b[0;36mdisplay_digits_with_boxes\u001b[0;34m(digits, predictions, labels, pred_bboxes, bboxes, title)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb#X25sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(bboxes_to_plot))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb#X25sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m ax \u001b[39m=\u001b[39m fig\u001b[39m.\u001b[39madd_subplot(\u001b[39m1\u001b[39m, n, i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb#X25sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m img_to_draw \u001b[39m=\u001b[39m draw_bounding_boxes_on_image_array(image\u001b[39m=\u001b[39;49mn_digits[i], boxes\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49masarray(bboxes_to_plot), color\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mred\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mgreen\u001b[39;49m\u001b[39m'\u001b[39;49m], display_str_list\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mtrue\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mpred\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb#X25sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(n_predictions[i])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb#X25sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m plt\u001b[39m.\u001b[39mxticks([])\n",
            "\u001b[1;32m/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb Cellule 19\u001b[0m in \u001b[0;36mdraw_bounding_boxes_on_image_array\u001b[0;34m(image, boxes, color, thickness, display_str_list)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb#X25sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m rgbimg \u001b[39m=\u001b[39m PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mnew(\u001b[39m\"\u001b[39m\u001b[39mRGBA\u001b[39m\u001b[39m\"\u001b[39m, image_pil\u001b[39m.\u001b[39msize)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb#X25sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m rgbimg\u001b[39m.\u001b[39mpaste(image_pil)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb#X25sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m draw_bounding_boxes_on_image(rgbimg, boxes, color, thickness, display_str_list)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb#X25sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(rgbimg)\n",
            "\u001b[1;32m/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb Cellule 19\u001b[0m in \u001b[0;36mdraw_bounding_boxes_on_image\u001b[0;34m(image, boxes, color, thickness, display_str_list)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb#X25sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInput must be of size [N, 4]\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb#X25sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(boxes_shape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb#X25sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m   draw_ONE_bounding_box_on_image(image, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb#X25sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m                                  boxes[i, \u001b[39m1\u001b[39m], boxes[i, \u001b[39m0\u001b[39m], \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb#X25sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m                                  boxes[i, \u001b[39m3\u001b[39m], boxes[i, \u001b[39m2\u001b[39m], \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/thierryksstentini/Documents/Python_divers/GitHub/Food_Recognition/YOLO_MNIST_Localization.ipynb#X25sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m                                  color[i], thickness, display_str_list[i])\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1440x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# (training_digits, training_labels, training_bboxes,\n",
        "#  validation_digits, validation_labels, validation_bboxes) = dataset_to_numpy_util(training_dataset, validation_dataset, 10)\n",
        "\n",
        "# validation_bboxes = relative2absolute(torch.as_tensor(validation_bboxes))\n",
        "# validation_bboxes = np.asarray(validation_bboxes)/75\n",
        "# iou = intersection_over_union(predicted_bboxes.numpy(), validation_bboxes)\n",
        "\n",
        "\n",
        "\n",
        "#print(\"Number of predictions where iou > threshold(%s): %s\" % (iou_threshold, (iou >= iou_threshold).sum()))\n",
        "#print(\"Number of predictions where iou < threshold(%s): %s\" % (iou_threshold, (iou < iou_threshold).sum()))\n",
        "\n",
        "display_digits_with_boxes(img, label_preds, labels, bbox_preds, bbox_true, \"validation digits and their labels\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "euSLQ6z_8aL_"
      },
      "outputs": [],
      "source": [
        "for cell_i in range(6):\n",
        "    for cell_j in range(6):\n",
        "        n_pred_bboxes = bbox_preds[:10, cell_i, cell_j, :4]\n",
        "        n_pred_bboxes = relative2absolute(n_pred_bboxes)\n",
        "        n_pred_bboxes = np.asarray(n_pred_bboxes)/75"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0388, 0.0400, 0.3629, 0.3629],\n",
              "        [0.0559, 0.0276, 0.3532, 0.3532],\n",
              "        [0.0309, 0.0077, 0.3204, 0.3204],\n",
              "        [0.0466, 0.0114, 0.3050, 0.3050],\n",
              "        [0.0376, 0.0226, 0.3511, 0.3511],\n",
              "        [0.0388, 0.0421, 0.3618, 0.3618],\n",
              "        [0.0398, 0.0221, 0.3188, 0.3188],\n",
              "        [0.0346, 0.0249, 0.3216, 0.3216],\n",
              "        [0.0392, 0.0220, 0.3167, 0.3167],\n",
              "        [0.0385, 0.0179, 0.3486, 0.3486]])"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bbox_preds[50:60, 2, 3, :4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "YOLO_MNIST_Localization.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "bfc2e043f4a41bbce066f7921047bdcfe8769546c7198e4df6482fd90de274f1"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04f6d46789754cfd9f0a441baa32af45": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06ff67cf46e64c3e8ba324f03a84c5ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7a121501cfd438895e7bd2d85e56251",
              "IPY_MODEL_c1c21fc51a224d60b314af87cbde69d4",
              "IPY_MODEL_97522edebd964d2aa9671dac5a5f381d"
            ],
            "layout": "IPY_MODEL_216234dec1484887a68608c520b3f093"
          }
        },
        "0963b6c3e45e4e5dabfd2e0d9b6cca38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09ed36dea11c480d8e0f38478998b828": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b60a12468347494d9c4fe04cd86af58e",
            "placeholder": "​",
            "style": "IPY_MODEL_f68b6dd7bde64b71afb10a929c410ed9",
            "value": "100%"
          }
        },
        "15b3c78cbd51469ca2b870b993b6fca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "201eec9906304c4e95854aa2fca10439": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5d88a84b1a24e98b3ee1d1b469defff",
            "max": 9912422,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc8b573ffafd427495a1ccf3b818e815",
            "value": 9912422
          }
        },
        "216234dec1484887a68608c520b3f093": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24e840cc10c54af0a0d40c6eb27beda1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30d2ecca2c46436198f644e8d6dc7f24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46612f5ff5024571917d3275717a5ae6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ee695bbe8e543a2b6cc48b97ed63b3e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54e7c876fe4d4be482733d96848b5a51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58c90db1a5df4c0bb4d6c6cc8cd98dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd7117704fa347838c6b19067ebca54c",
            "placeholder": "​",
            "style": "IPY_MODEL_15b3c78cbd51469ca2b870b993b6fca0",
            "value": " 1648877/1648877 [00:00&lt;00:00, 5302356.85it/s]"
          }
        },
        "5f502d93a2fc46398195151da51c0083": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c99c88b52334f53a1a3c8e89f5c3d7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74c88f9633d541b0a60149df3d9a606c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d20b57ac5b44fbd91e7cee432070247": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dc6465a02ed4763b527f143ad9e254c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "809cb638d85e48999892dfd8cfc815a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a64e70ecfbe46558b9865815d283596",
              "IPY_MODEL_ae6fd6d93a5647a28ffd9af5e26942e6",
              "IPY_MODEL_aad87ec81bb34aadac8c84ae9b505c68"
            ],
            "layout": "IPY_MODEL_cdfd8330d4964e9fa41bfe4fd1639def"
          }
        },
        "83fbb691d40b4d02b029bcbdb2f85590": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85daafeb4a1a4b55bc6b63ba3f088bd5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c60136a50bd4bf6ad83c4d26f8d8aa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97522edebd964d2aa9671dac5a5f381d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dc6465a02ed4763b527f143ad9e254c",
            "placeholder": "​",
            "style": "IPY_MODEL_7d20b57ac5b44fbd91e7cee432070247",
            "value": " 28881/28881 [00:00&lt;00:00, 810071.71it/s]"
          }
        },
        "9811f18275dd46f397a0dc445bcbd90a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c63e5aa3160c49d28f9a6df08abb2558",
              "IPY_MODEL_201eec9906304c4e95854aa2fca10439",
              "IPY_MODEL_da28120108424e88bdb8599bd0f8d4f7"
            ],
            "layout": "IPY_MODEL_83fbb691d40b4d02b029bcbdb2f85590"
          }
        },
        "9a64e70ecfbe46558b9865815d283596": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9418919bc204bb6af14a33815d1bd5f",
            "placeholder": "​",
            "style": "IPY_MODEL_24e840cc10c54af0a0d40c6eb27beda1",
            "value": "100%"
          }
        },
        "a4fef10f26744768b57986d380689412": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a66241a6cd464e339bef5fb2fceab412": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aad87ec81bb34aadac8c84ae9b505c68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b34a8acab2614688bef300d8be847cf6",
            "placeholder": "​",
            "style": "IPY_MODEL_0963b6c3e45e4e5dabfd2e0d9b6cca38",
            "value": " 4542/4542 [00:00&lt;00:00, 136321.56it/s]"
          }
        },
        "ae6fd6d93a5647a28ffd9af5e26942e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ee695bbe8e543a2b6cc48b97ed63b3e",
            "max": 4542,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54e7c876fe4d4be482733d96848b5a51",
            "value": 4542
          }
        },
        "b34a8acab2614688bef300d8be847cf6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b60a12468347494d9c4fe04cd86af58e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7a121501cfd438895e7bd2d85e56251": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74c88f9633d541b0a60149df3d9a606c",
            "placeholder": "​",
            "style": "IPY_MODEL_a4fef10f26744768b57986d380689412",
            "value": "100%"
          }
        },
        "c136bc57bec64d38b2505a0c60e6f64b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1c21fc51a224d60b314af87cbde69d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04f6d46789754cfd9f0a441baa32af45",
            "max": 28881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f502d93a2fc46398195151da51c0083",
            "value": 28881
          }
        },
        "c63e5aa3160c49d28f9a6df08abb2558": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30d2ecca2c46436198f644e8d6dc7f24",
            "placeholder": "​",
            "style": "IPY_MODEL_6c99c88b52334f53a1a3c8e89f5c3d7f",
            "value": "100%"
          }
        },
        "c9418919bc204bb6af14a33815d1bd5f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc8b573ffafd427495a1ccf3b818e815": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cdfd8330d4964e9fa41bfe4fd1639def": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5d88a84b1a24e98b3ee1d1b469defff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da28120108424e88bdb8599bd0f8d4f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46612f5ff5024571917d3275717a5ae6",
            "placeholder": "​",
            "style": "IPY_MODEL_8c60136a50bd4bf6ad83c4d26f8d8aa7",
            "value": " 9912422/9912422 [00:00&lt;00:00, 20048947.36it/s]"
          }
        },
        "dd7117704fa347838c6b19067ebca54c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e18767ed9f8944b5a8091d5f39e986d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c136bc57bec64d38b2505a0c60e6f64b",
            "max": 1648877,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a66241a6cd464e339bef5fb2fceab412",
            "value": 1648877
          }
        },
        "f68b6dd7bde64b71afb10a929c410ed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f85da4cd1a64460bb2580b69d8fa066d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09ed36dea11c480d8e0f38478998b828",
              "IPY_MODEL_e18767ed9f8944b5a8091d5f39e986d9",
              "IPY_MODEL_58c90db1a5df4c0bb4d6c6cc8cd98dce"
            ],
            "layout": "IPY_MODEL_85daafeb4a1a4b55bc6b63ba3f088bd5"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
