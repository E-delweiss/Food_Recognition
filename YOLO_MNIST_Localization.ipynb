{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThOpaque/Food_Recognition/blob/main/YOLO_MNIST_Localization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAHeh1BRufX4",
        "outputId": "dc6de471-93f4-4d41-d781-9574240fd3fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchinfo in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (1.7.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: torchmetrics in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (0.9.3)\n",
            "Requirement already satisfied: packaging in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from torchmetrics) (1.23.1)\n",
            "Requirement already satisfied: torch>=1.3.1 in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from torchmetrics) (1.13.0.dev20220730)\n",
            "Requirement already satisfied: typing-extensions in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from torch>=1.3.1->torchmetrics) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "import os, time, datetime\n",
        "from timeit import default_timer as timer\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "%pip install torchinfo;\n",
        "%pip install torchmetrics;\n",
        "from torchmetrics import MeanSquaredError;\n",
        "from torchinfo import summary;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOraK1TX7XZB",
        "outputId": "3001e719-ddf8-47a6-fcf4-149184181740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------------\n",
            "Execute notebook on - mps -\n",
            "------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Choosing device between CPU or GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "elif torch.has_mps:\n",
        "    device=torch.device('mps')\n",
        "else:\n",
        "    device=torch.device('cpu')\n",
        "    \n",
        "print(\"\\n------------------------------------\")\n",
        "print(f\"Execute notebook on - {device} -\")\n",
        "print(\"------------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "M7VztqFE71JZ"
      },
      "outputs": [],
      "source": [
        "class my_mnist_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root:str, split:str=None, download:bool=False, S=6, sizeHW=75):\n",
        "        assert split, \"You have to specify the split.\"\n",
        "        \n",
        "        if split == \"train\":\n",
        "            train = True\n",
        "        elif split == \"test\":\n",
        "            train = False\n",
        "        \n",
        "        self.dataset = torchvision.datasets.MNIST(root=root, train=train, download=download)\n",
        "        \n",
        "        self.cell_size = sizeHW / S\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def _numpy_pad_to_bounding_box(self, image, offset_height=0, offset_width=0, target_height=0, target_width=0):\n",
        "        assert image.shape[:-1][0] <= target_height-offset_height, \"height must be <= target - offset\"\n",
        "        assert image.shape[:-1][1] <= target_width-offset_width, \"width must be <= target - offset\"\n",
        "        \n",
        "        target_array = np.zeros((target_height, target_width, image.shape[-1]))\n",
        "\n",
        "        for k in range(image.shape[0]):\n",
        "            target_array[offset_height+k][offset_width:image.shape[1]+offset_width] = image[k]\n",
        "        \n",
        "        return target_array\n",
        "\n",
        "    def _transform_pasting75(self, image, label):\n",
        "        ### xmin, ymin of digit\n",
        "        xmin = torch.randint(0, 48, (1,))\n",
        "        ymin = torch.randint(0, 48, (1,))\n",
        "        \n",
        "        image = torchvision.transforms.ToTensor()(image)\n",
        "        image = torch.reshape(image, (28,28,1,))\n",
        "        image = torch.from_numpy(self._numpy_pad_to_bounding_box(image, ymin, xmin, 75, 75))\n",
        "        image = image.permute(2, 0, 1) #(C,H,W)\n",
        "        image = image.to(torch.float)\n",
        "        \n",
        "        xmin, ymin = xmin.to(torch.float), ymin.to(torch.float)\n",
        "\n",
        "        xmax_bbox, ymax_bbox = (xmin + 28), (ymin + 28)\n",
        "        xmin_bbox, ymin_bbox = xmin, ymin\n",
        "        w_bbox = xmax_bbox-xmin_bbox\n",
        "        h_bbox = ymax_bbox-ymin_bbox\n",
        "\n",
        "        rw = w_bbox / 75\n",
        "        rh = h_bbox / 75\n",
        "        cx = (xmin + (w_bbox/2))/75\n",
        "        cy = (ymin + (h_bbox/2))/75\n",
        "\n",
        "        cx_rcell = cx % self.cell_size / self.cell_size\n",
        "        cy_rcell = cy % self.cell_size / self.cell_size\n",
        "\n",
        "\n",
        "        label_one_hot = F.one_hot(torch.as_tensor(label, dtype=torch.int64), 10)\n",
        "        bbox_coord = torch.Tensor([cx_rcell, cy_rcell, rw, rh])\n",
        "\n",
        "        return image, label_one_hot, bbox_coord\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        image, one_hot_label, bbox_coord = self._transform_pasting75(self.dataset[idx][0], self.dataset[idx][1])\n",
        "        \n",
        "        return image, one_hot_label.to(torch.float), bbox_coord\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "yQNznLfO8jOx"
      },
      "outputs": [],
      "source": [
        "def get_training_dataset(BATCH_SIZE=64):\n",
        "    \"\"\"\n",
        "    Loads and maps the training split of the dataset using the custom dataset class. \n",
        "    \"\"\"\n",
        "    dataset = my_mnist_dataset(root=\"data\", split=\"train\", download=True)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    return dataloader, len(dataset)\n",
        "\n",
        "def get_validation_dataset(BATCH_SIZE = None):\n",
        "    \"\"\"\n",
        "    Loads and maps the validation split of the datasetusing the custom dataset class. \n",
        "    \"\"\"\n",
        "    dataset = my_mnist_dataset(root=\"data\", split=\"test\", download=True)\n",
        "    if BATCH_SIZE is None:\n",
        "        BATCH_SIZE = len(dataset)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    return dataloader, len(dataset)\n",
        "\n",
        "# instantiate the datasets\n",
        "training_dataset, len_training_ds = get_training_dataset()\n",
        "validation_dataset, len_validation_ds = get_validation_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wfgUx1srt90c"
      },
      "outputs": [],
      "source": [
        "class CNNBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(CNNBlock, self).__init__()\n",
        "        self.conv = torch.nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "        self.bn = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.l_relu = torch.nn.LeakyReLU(0.1)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        x = self.conv(input)\n",
        "        x = self.bn(x)\n",
        "        return self.l_relu(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3VjCkTEce-CT"
      },
      "outputs": [],
      "source": [
        "class YoloMNIST(torch.nn.Module):\n",
        "    def __init__(self, sizeHW, S, C, B):\n",
        "        super(YoloMNIST, self).__init__()\n",
        "        self.S, self.C, self.B = S, C, B\n",
        "        self.sizeHW = sizeHW\n",
        "        self.cell_size = self.sizeHW / self.S\n",
        "\n",
        "        self.seq = torch.nn.Sequential()        \n",
        "        self.seq.add_module(f\"conv_1\", CNNBlock(1, 32, stride=2, kernel_size=7, padding=2))\n",
        "        self.seq.add_module(f\"maxpool_1\", torch.nn.MaxPool2d(2))\n",
        "        self.seq.add_module(f\"conv_3\", CNNBlock(32, 128, stride=1, kernel_size=3, padding=0))\n",
        "        self.seq.add_module(f\"maxpool_2\", torch.nn.MaxPool2d(2))\n",
        "        self.seq.add_module(f\"conv_5\", CNNBlock(128, 64, stride=1, kernel_size=1, padding=0))\n",
        "        self.seq.add_module(f\"conv_4\", CNNBlock(64, 128, stride=1, kernel_size=3, padding=0))\n",
        "        self.seq.add_module(f\"conv_6\", CNNBlock(128, 128, stride=1, kernel_size=3, padding=1))\n",
        "        \n",
        "        self.fcs = self._create_fcs()\n",
        "\n",
        "    def _size_output(self, sizeHW:int, kernel:int, stride:int, padding:int=0, isMaxPool:bool=False)->int:\n",
        "        \"\"\"\n",
        "        Output size (width/height) of convolutional or maxpool layers.\n",
        "\n",
        "        Args:\n",
        "            sizeHW : int\n",
        "                Image size (we suppose this is a square image)\n",
        "            kernel : int\n",
        "                Size of a square kernel\n",
        "            stride : int\n",
        "                Stride of convolution layer\n",
        "            padding : int\n",
        "                Padding of convolution layer\n",
        "            isMaxPool : Bool, default is False.\n",
        "                Specify if it is a Maxpool layer (True) or not (False). \n",
        "\n",
        "        Return:\n",
        "            output_size : int\n",
        "                Image output size after a convolutional or MaxPool layer.\n",
        "        \"\"\" \n",
        "        if isMaxPool == True:\n",
        "            output_size = int(sizeHW/2)\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "        if padding == 'same':\n",
        "            output_size = sizeHW\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "        else:\n",
        "            output_size = (sizeHW + 2 * padding - (kernel-1)-1)/stride\n",
        "            output_size = int(output_size + 1)\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "\n",
        "    def _create_fcs(self):\n",
        "        output = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(128 * self.S * self.S, 4096),\n",
        "            torch.nn.LeakyReLU(0.1),\n",
        "            torch.nn.Linear(4096, self.S * self.S * (self.C + self.B*5))\n",
        "        )\n",
        "        return output\n",
        "    \n",
        "\n",
        "    def forward(self, input:torch.Tensor)->tuple:\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            input : torch.Tensor of shape (N, C, H, W)\n",
        "                Batch of images.\n",
        "\n",
        "        Return:\n",
        "            box_coord : torch.Tensor of shape (N, 6, 6, 5)\n",
        "                Contains xc_rcell, yc_rcell, rw, rh and the confidence number c\n",
        "                over 6x6 grid cells.\n",
        "            classifier : torch.Tensor of shape (N, 6, 6, 10)\n",
        "                Contains the one-hot encoding of each digit number over\n",
        "                6x6 grid cells.\n",
        "        \"\"\"     \n",
        "        x = self.seq(input)\n",
        "        x = self.fcs(x)\n",
        "        x = x.view(x.size(0), self.S, self.S, self.B * 5 + self.C)\n",
        "        box_coord = x[:,:,:,0:5]\n",
        "        classifier = x[:,:,:,5:]\n",
        "        return box_coord, classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "id": "T_jOg_i_p2_c"
      },
      "outputs": [],
      "source": [
        "class YoloLoss(torch.nn.Module):\n",
        "    def __init__(self, lambd_coord:int, lambd_noobj:float, device:torch.device, S:int=6):\n",
        "        super(YoloLoss, self).__init__()\n",
        "        self.LAMBD_COORD = lambd_coord\n",
        "        self.LAMBD_NOOBJ = lambd_noobj\n",
        "        self.S = S\n",
        "        self.device = device\n",
        "\n",
        "    def _coordloss(self, pred_coord_rcell, true_coord_rcell):\n",
        "        \"\"\"\n",
        "        Args : \n",
        "            pred_coord_rcell : torch.Tensor of shape (N, 2)\n",
        "            true_coord_rcell : torch.Tensor of shape (N, 2)\n",
        "        Returns :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        xc_hat, yc_hat = pred_coord_rcell.permute(1,0)\n",
        "        xc, yc = true_coord_rcell.permute(1,0)\n",
        "\n",
        "        squared_error = torch.pow(xc - xc_hat,2) + torch.pow(yc - yc_hat,2)\n",
        "        return squared_error\n",
        "\n",
        "    def _sizeloss(self, pred_size, true_size):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_size : torch.Tensor of shape (N, 2)\n",
        "            true_size : torch.Tensor of shape (N, 2)\n",
        "        Returns : \n",
        "            root_squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        rw_hat, rh_hat = pred_size.permute(1,0)\n",
        "        rw, rh = true_size.permute(1,0)\n",
        "\n",
        "        #sizes can't be negative\n",
        "        rw_hat = rw_hat.clip(min=0)\n",
        "        rh_hat = rh_hat.clip(min=0)\n",
        "\n",
        "        root_squared_error_w = torch.pow(torch.sqrt(rw) - torch.sqrt(rw_hat),2)\n",
        "        root_squared_error_h = torch.pow(torch.sqrt(rh) - torch.sqrt(rh_hat),2)\n",
        "        root_squared_error = root_squared_error_w + root_squared_error_h\n",
        "        return root_squared_error\n",
        "\n",
        "    def _confidenceloss(self, pred_c, true_c):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_c : torch.Tensor of shape (N)\n",
        "            true_c : torch.Tensor of shape (N)\n",
        "        Return :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        squared_error = torch.pow(true_c - pred_c, 2)\n",
        "        return squared_error\n",
        "\n",
        "    def _classloss(self, pred_class, true_class):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_class : torch.Tensor of shape (N, 10)\n",
        "            true_class : torch.Tensor of shape (N, 10)\n",
        "        Returns :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        squared_error = torch.pow(true_class - pred_class, 2)\n",
        "        return torch.sum(squared_error, dim=1)\n",
        "\n",
        "    def forward(self, pred_box:torch.Tensor, true_box:torch.Tensor, pred_class:torch.Tensor, true_class:torch.Tensor):\n",
        "        \"\"\"\n",
        "        Grid forward pass.\n",
        "\n",
        "        Args:\n",
        "            pred_box : torch.Tensor of shape (N, S, S, 5)\n",
        "                Batch predicted outputs containing xc_rcell, yc_rcell, rw, rh,\n",
        "                and confident number c for each grid cell.\n",
        "            true_box : torch.Tensor of shape (N, S, S, 5)\n",
        "                Groundtrue batch containing bbox values for each cell and\n",
        "                c indicate if there is an object to detect or not (1/0).\n",
        "            pred_class : torch.Tensor of shape (N, S, S, 10)\n",
        "                Probability of each digit class in each grid cell\n",
        "            true_class : torch.Tensor of shape (N, 10)\n",
        "                one-hot vect of each digit\n",
        "\n",
        "        Return:\n",
        "            loss : float\n",
        "                The batch loss value of the grid\n",
        "        \"\"\"\n",
        "        BATCH_SIZE = len(pred_box)\n",
        "\n",
        "        ### Initialization of the losses\n",
        "        losses_list = ['loss_xy', 'loss_wh', 'loss_conf_obj', 'loss_conf_noobj', 'loss_class']\n",
        "        losses = {key : torch.zeros(BATCH_SIZE).to(self.device) for key in losses_list}\n",
        "        \n",
        "        ### Compute the losses for all images in the batch\n",
        "        for i in range(self.S):\n",
        "            for j in range(self.S):\n",
        "                ### Intersection over Union\n",
        "                IoU = self._intersection_over_union(pred_box[:,i,j], true_box[:,i,j])\n",
        "\n",
        "                ### bbox coordinates\n",
        "                xy_hat = pred_box[:,i,j,:2]\n",
        "                xy = true_box[:,i,j,:2]\n",
        "                wh_hat = pred_box[:,i,j,2:4]\n",
        "                wh = true_box[:,i,j,2:4]\n",
        "                \n",
        "                ### confidence numbers\n",
        "                pred_c = pred_box[:,i,j,4]# * IoU\n",
        "                true_c = true_box[:,i,j,4]\n",
        "\n",
        "                ### objects to detect\n",
        "                isObject = true_c.to(torch.bool)\n",
        "                isNoObject = torch.logical_not(true_c) #(~bool) doesn't work on MPS device\n",
        "\n",
        "                ### sum the losses over the grid\n",
        "                losses['loss_xy'] += isObject * self._coordloss(xy_hat, xy)\n",
        "                losses['loss_wh'] += isObject * self._sizeloss(wh_hat, wh)\n",
        "                losses['loss_conf_obj'] += isObject * self._confidenceloss(pred_c, true_c)\n",
        "                losses['loss_conf_noobj'] += isNoObject * self._confidenceloss(pred_c, true_c)\n",
        "                losses['loss_class'] += isObject * self._classloss(pred_class[:,i,j], true_class)\n",
        "\n",
        "        ### Yolo_v1 loss over the batch, shape : (BATCH_SIZE)\n",
        "        loss = self.LAMBD_COORD * losses['loss_xy'] \\\n",
        "                + self.LAMBD_COORD * losses['loss_wh'] \\\n",
        "                + losses['loss_conf_obj'] \\\n",
        "                + self.LAMBD_NOOBJ * losses['loss_conf_noobj'] \\\n",
        "                + losses['loss_class']\n",
        "        \n",
        "        loss = torch.sum(loss) / BATCH_SIZE\n",
        "\n",
        "        return losses, loss\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "OdIlkN61mXJj"
      },
      "outputs": [],
      "source": [
        "def bbox2Tensor(bbox:torch.Tensor, S:int=6, sizeHW:int=75, device=torch.device('cpu'))->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Constructs en Tensor and puts bbox values in the corresponding i,j grid cell.\n",
        "\n",
        "    Args :\n",
        "        bbox : torch.Tensor of shape (N,4)\n",
        "            Contains bbox values xc_rcell, yc_rcell, rw and rh.\n",
        "        S : int, default is 6\n",
        "            Size of the grid.\n",
        "        sizeHW : int, default is 75\n",
        "            Size of the image.\n",
        "\n",
        "    Return :\n",
        "        bbox_t : torch.Tensor of shape (N, S, S, 5)\n",
        "            Tensor containing all 4 bbox values in the corresponding i,j grid\n",
        "            cell position i.e. in the i,j position where an object should be\n",
        "            detected.\n",
        "    \"\"\"\n",
        "    N = len(bbox)\n",
        "    bbox_t = torch.zeros(N,S,S,5).to(device)\n",
        "    cell_size = sizeHW/S\n",
        "\n",
        "    xc_rcell, yc_rcell, rw, rh = bbox.permute(1,0).to(device)\n",
        "    xc = xc_rcell * cell_size - (1/cell_size) * (xc_rcell/cell_size).to(torch.int32)\n",
        "    yc = yc_rcell * cell_size - (1/cell_size) * (yc_rcell/cell_size).to(torch.int32)\n",
        "\n",
        "    N_range = torch.arange(N)\n",
        "    lines = (yc * S).to(torch.long)\n",
        "    columns = (xc * S).to(torch.long)\n",
        "    bbox_t[N_range, lines, columns] = torch.stack((xc_rcell, yc_rcell, rw, rh, torch.ones(N))).permute(1,0)\n",
        "    \n",
        "    return bbox_t.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "mHUOkfWPe-CW"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.0001\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "model_MNIST = YoloMNIST(sizeHW=75, S=6, C=10, B=1)\n",
        "model_MNIST = model_MNIST.to(device)\n",
        "optimizer = torch.optim.Adam(params=model_MNIST.parameters(), lr=learning_rate, weight_decay=0.0005)\n",
        "loss_yolo = YoloLoss(lambd_coord=5, lambd_noobj=0.5, S=6, sizeHW=75, device=device)\n",
        "\n",
        "# print(optimizer)\n",
        "#summary(model_MNIST, input_size = (BATCH_SIZE,1,75,75))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "9tSO0Qo7e-CX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[START] : 2022-08-25 18:50:21 :\n",
            "[Training on] : CPU\n",
            "--------------------\n",
            "     2022-08-25 18:50:21 : EPOCH 1/3\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 5.51166\n",
            "xy_coord training loss for this batch : 0.05202\n",
            "wh_sizes training loss for this batch : 0.45522\n",
            "confidence with object training loss for this batch : 1.20043\n",
            "confidence without object training loss for this batch : 1.12338\n",
            "class proba training loss for this batch : 1.21335\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 1.06861\n",
            "xy_coord training loss for this batch : 0.00255\n",
            "wh_sizes training loss for this batch : 0.00385\n",
            "confidence with object training loss for this batch : 0.07596\n",
            "confidence without object training loss for this batch : 0.21245\n",
            "class proba training loss for this batch : 0.85442\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 1.01707\n",
            "xy_coord training loss for this batch : 0.00245\n",
            "wh_sizes training loss for this batch : 0.00243\n",
            "confidence with object training loss for this batch : 0.05722\n",
            "confidence without object training loss for this batch : 0.16733\n",
            "class proba training loss for this batch : 0.85180\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.84756\n",
            "xy_coord training loss for this batch : 0.00233\n",
            "wh_sizes training loss for this batch : 0.00201\n",
            "confidence with object training loss for this batch : 0.05992\n",
            "confidence without object training loss for this batch : 0.18375\n",
            "class proba training loss for this batch : 0.67407\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.70653\n",
            "xy_coord training loss for this batch : 0.00323\n",
            "wh_sizes training loss for this batch : 0.00167\n",
            "confidence with object training loss for this batch : 0.06483\n",
            "confidence without object training loss for this batch : 0.14680\n",
            "class proba training loss for this batch : 0.54377\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.65482\n",
            "xy_coord training loss for this batch : 0.00229\n",
            "wh_sizes training loss for this batch : 0.00208\n",
            "confidence with object training loss for this batch : 0.05109\n",
            "confidence without object training loss for this batch : 0.14109\n",
            "class proba training loss for this batch : 0.51135\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.62638\n",
            "xy_coord training loss for this batch : 0.00181\n",
            "wh_sizes training loss for this batch : 0.00154\n",
            "confidence with object training loss for this batch : 0.05868\n",
            "confidence without object training loss for this batch : 0.16164\n",
            "class proba training loss for this batch : 0.47011\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.48214\n",
            "xy_coord training loss for this batch : 0.00206\n",
            "wh_sizes training loss for this batch : 0.00126\n",
            "confidence with object training loss for this batch : 0.05565\n",
            "confidence without object training loss for this batch : 0.14310\n",
            "class proba training loss for this batch : 0.33835\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.48896\n",
            "xy_coord training loss for this batch : 0.00205\n",
            "wh_sizes training loss for this batch : 0.00118\n",
            "confidence with object training loss for this batch : 0.05860\n",
            "confidence without object training loss for this batch : 0.14183\n",
            "class proba training loss for this batch : 0.34327\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.42017\n",
            "xy_coord training loss for this batch : 0.00167\n",
            "wh_sizes training loss for this batch : 0.00157\n",
            "confidence with object training loss for this batch : 0.05702\n",
            "confidence without object training loss for this batch : 0.12059\n",
            "class proba training loss for this batch : 0.28670\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.37054\n",
            "xy_coord training loss for this batch : 0.00188\n",
            "wh_sizes training loss for this batch : 0.00173\n",
            "confidence with object training loss for this batch : 0.04680\n",
            "confidence without object training loss for this batch : 0.10384\n",
            "class proba training loss for this batch : 0.25378\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:19.876407\n",
            "Mean training loss for this epoch : 0.77085\n",
            "--------------------\n",
            "     2022-08-25 18:50:21 : EPOCH 2/3\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.35636\n",
            "xy_coord training loss for this batch : 0.00159\n",
            "wh_sizes training loss for this batch : 0.00145\n",
            "confidence with object training loss for this batch : 0.03554\n",
            "confidence without object training loss for this batch : 0.10185\n",
            "class proba training loss for this batch : 0.25470\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.35975\n",
            "xy_coord training loss for this batch : 0.00175\n",
            "wh_sizes training loss for this batch : 0.00101\n",
            "confidence with object training loss for this batch : 0.03837\n",
            "confidence without object training loss for this batch : 0.08729\n",
            "class proba training loss for this batch : 0.26397\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.32352\n",
            "xy_coord training loss for this batch : 0.00131\n",
            "wh_sizes training loss for this batch : 0.00111\n",
            "confidence with object training loss for this batch : 0.05082\n",
            "confidence without object training loss for this batch : 0.10483\n",
            "class proba training loss for this batch : 0.20820\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.33323\n",
            "xy_coord training loss for this batch : 0.00145\n",
            "wh_sizes training loss for this batch : 0.00081\n",
            "confidence with object training loss for this batch : 0.05426\n",
            "confidence without object training loss for this batch : 0.11901\n",
            "class proba training loss for this batch : 0.20820\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.29494\n",
            "xy_coord training loss for this batch : 0.00111\n",
            "wh_sizes training loss for this batch : 0.00083\n",
            "confidence with object training loss for this batch : 0.02809\n",
            "confidence without object training loss for this batch : 0.08617\n",
            "class proba training loss for this batch : 0.21407\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.23544\n",
            "xy_coord training loss for this batch : 0.00082\n",
            "wh_sizes training loss for this batch : 0.00087\n",
            "confidence with object training loss for this batch : 0.04439\n",
            "confidence without object training loss for this batch : 0.09621\n",
            "class proba training loss for this batch : 0.13446\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.26659\n",
            "xy_coord training loss for this batch : 0.00065\n",
            "wh_sizes training loss for this batch : 0.00088\n",
            "confidence with object training loss for this batch : 0.03848\n",
            "confidence without object training loss for this batch : 0.08570\n",
            "class proba training loss for this batch : 0.17762\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.29695\n",
            "xy_coord training loss for this batch : 0.00099\n",
            "wh_sizes training loss for this batch : 0.00070\n",
            "confidence with object training loss for this batch : 0.06111\n",
            "confidence without object training loss for this batch : 0.11112\n",
            "class proba training loss for this batch : 0.17185\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.26951\n",
            "xy_coord training loss for this batch : 0.00101\n",
            "wh_sizes training loss for this batch : 0.00071\n",
            "confidence with object training loss for this batch : 0.05147\n",
            "confidence without object training loss for this batch : 0.11809\n",
            "class proba training loss for this batch : 0.15038\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.24018\n",
            "xy_coord training loss for this batch : 0.00074\n",
            "wh_sizes training loss for this batch : 0.00070\n",
            "confidence with object training loss for this batch : 0.03943\n",
            "confidence without object training loss for this batch : 0.08889\n",
            "class proba training loss for this batch : 0.14911\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.26007\n",
            "xy_coord training loss for this batch : 0.00084\n",
            "wh_sizes training loss for this batch : 0.00064\n",
            "confidence with object training loss for this batch : 0.05849\n",
            "confidence without object training loss for this batch : 0.09615\n",
            "class proba training loss for this batch : 0.14606\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:17.054675\n",
            "Mean training loss for this epoch : 0.30563\n",
            "--------------------\n",
            "     2022-08-25 18:50:21 : EPOCH 3/3\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.21165\n",
            "xy_coord training loss for this batch : 0.00073\n",
            "wh_sizes training loss for this batch : 0.00060\n",
            "confidence with object training loss for this batch : 0.03174\n",
            "confidence without object training loss for this batch : 0.08035\n",
            "class proba training loss for this batch : 0.13307\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.20642\n",
            "xy_coord training loss for this batch : 0.00066\n",
            "wh_sizes training loss for this batch : 0.00050\n",
            "confidence with object training loss for this batch : 0.03324\n",
            "confidence without object training loss for this batch : 0.06504\n",
            "class proba training loss for this batch : 0.13486\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.19127\n",
            "xy_coord training loss for this batch : 0.00059\n",
            "wh_sizes training loss for this batch : 0.00063\n",
            "confidence with object training loss for this batch : 0.03404\n",
            "confidence without object training loss for this batch : 0.07022\n",
            "class proba training loss for this batch : 0.11604\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.25650\n",
            "xy_coord training loss for this batch : 0.00064\n",
            "wh_sizes training loss for this batch : 0.00054\n",
            "confidence with object training loss for this batch : 0.03949\n",
            "confidence without object training loss for this batch : 0.08927\n",
            "class proba training loss for this batch : 0.16644\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.22176\n",
            "xy_coord training loss for this batch : 0.00066\n",
            "wh_sizes training loss for this batch : 0.00040\n",
            "confidence with object training loss for this batch : 0.03411\n",
            "confidence without object training loss for this batch : 0.06778\n",
            "class proba training loss for this batch : 0.14848\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.19102\n",
            "xy_coord training loss for this batch : 0.00046\n",
            "wh_sizes training loss for this batch : 0.00046\n",
            "confidence with object training loss for this batch : 0.03147\n",
            "confidence without object training loss for this batch : 0.06129\n",
            "class proba training loss for this batch : 0.12432\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.19446\n",
            "xy_coord training loss for this batch : 0.00054\n",
            "wh_sizes training loss for this batch : 0.00059\n",
            "confidence with object training loss for this batch : 0.04341\n",
            "confidence without object training loss for this batch : 0.06797\n",
            "class proba training loss for this batch : 0.11141\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.20157\n",
            "xy_coord training loss for this batch : 0.00033\n",
            "wh_sizes training loss for this batch : 0.00052\n",
            "confidence with object training loss for this batch : 0.04034\n",
            "confidence without object training loss for this batch : 0.07865\n",
            "class proba training loss for this batch : 0.11768\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.18245\n",
            "xy_coord training loss for this batch : 0.00039\n",
            "wh_sizes training loss for this batch : 0.00058\n",
            "confidence with object training loss for this batch : 0.03089\n",
            "confidence without object training loss for this batch : 0.07045\n",
            "class proba training loss for this batch : 0.11152\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.21350\n",
            "xy_coord training loss for this batch : 0.00036\n",
            "wh_sizes training loss for this batch : 0.00047\n",
            "confidence with object training loss for this batch : 0.03894\n",
            "confidence without object training loss for this batch : 0.08531\n",
            "class proba training loss for this batch : 0.12772\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.14128\n",
            "xy_coord training loss for this batch : 0.00040\n",
            "wh_sizes training loss for this batch : 0.00039\n",
            "confidence with object training loss for this batch : 0.03471\n",
            "confidence without object training loss for this batch : 0.07456\n",
            "class proba training loss for this batch : 0.06533\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:16.098383\n",
            "Mean training loss for this epoch : 0.21776\n"
          ]
        }
      ],
      "source": [
        "delta_time = datetime.timedelta(hours=1)\n",
        "timezone = datetime.timezone(offset=delta_time)\n",
        "\n",
        "t = datetime.datetime.now(tz=timezone)\n",
        "str_t = '{:%Y-%m-%d %H:%M:%S}'.format(t)\n",
        "print(f\"[START] : {str_t} :\")\n",
        "print(f\"[Training on] : {str(device).upper()}\")\n",
        "\n",
        "EPOCHS = 3\n",
        "size_grid = 6\n",
        "batch_loss_list = []\n",
        "model_MNIST = model_MNIST.to(device)\n",
        "\n",
        "for epoch in range(EPOCHS) : \n",
        "    begin_time = timer()\n",
        "    epochs_loss = 0.\n",
        "    \n",
        "    print(\"-\"*20)\n",
        "    str_t = '{:%Y-%m-%d %H:%M:%S}'.format(t)\n",
        "    print(\" \"*5 + f\"{str_t} : EPOCH {epoch+1}/{EPOCHS}\")\n",
        "    print(\"-\"*20)\n",
        "\n",
        "    model_MNIST.train()\n",
        "    for batch, (img, labels, bbox_true) in enumerate(training_dataset):\n",
        "        loss = 0\n",
        "        begin_batch_time = timer()\n",
        "        img, labels, bbox_true = img.to(device), labels.to(device), bbox_true.to(device)\n",
        "        \n",
        "        ### turn bbox into NxSxSx5 tensor\n",
        "        bbox_true_6x6 = bbox2Tensor(bbox_true).to(device)\n",
        "        \n",
        "        ### clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        ### compute predictions\n",
        "        bbox_preds, label_preds = model_MNIST(img)\n",
        "        \n",
        "        ### compute losses over each grid cell for each image in the batch\n",
        "        losses, loss = loss_yolo(bbox_preds, bbox_true_6x6, label_preds, labels)\n",
        "    \n",
        "        ### compute gradients\n",
        "        loss.backward()\n",
        "        \n",
        "        ### Weight updates\n",
        "        optimizer.step()\n",
        "        \n",
        "        ######### print part #######################\n",
        "        current_loss = loss.item()\n",
        "        batch_loss_list.append(current_loss)\n",
        "        epochs_loss = epochs_loss + current_loss\n",
        "\n",
        "        if batch+1 <= len_training_ds//BATCH_SIZE:\n",
        "            current_training_sample = (batch+1)*BATCH_SIZE\n",
        "        else:\n",
        "            current_training_sample = (batch)*BATCH_SIZE + len_training_ds%BATCH_SIZE\n",
        "        \n",
        "        if (batch) == 0 or (batch+1)%100 == 0 or batch == len_training_ds//BATCH_SIZE:\n",
        "            print(f\" --- Image : {current_training_sample}/{len_training_ds}\",\\\n",
        "                    f\" : loss = {current_loss:.5f}\")\n",
        "            print(f\"xy_coord training loss for this batch : {torch.sum(losses['loss_xy']) / len(img):.5f}\")\n",
        "            print(f\"wh_sizes training loss for this batch : {torch.sum(losses['loss_wh']) / len(img):.5f}\")\n",
        "            print(f\"confidence with object training loss for this batch : {torch.sum(losses['loss_conf_obj']) / len(img):.5f}\")\n",
        "            print(f\"confidence without object training loss for this batch : {torch.sum(losses['loss_conf_noobj']) / len(img):.5f}\")\n",
        "            print(f\"class proba training loss for this batch : {torch.sum(losses['loss_class']) / len(img):.5f}\")\n",
        "            print('\\n')\n",
        "            if batch == (len_training_ds//BATCH_SIZE):\n",
        "                print(f\"Total elapsed time for training : {datetime.timedelta(seconds=timer()-begin_time)}\")\n",
        "                print(f\"Mean training loss for this epoch : {epochs_loss / len(training_dataset):.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model_MNIST.state_dict(), \"yolo_mnist_model_3epochs.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {},
      "outputs": [],
      "source": [
        "def relative2absolute(bbox_relative:torch.Tensor, SIZEHW=75, S=6)->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Turns bounding box relative to cell coordinates into absolute coordinates \n",
        "    (pixels). Used to calculate IoU. \n",
        "\n",
        "    Args:\n",
        "        bbox_relative : torch.Tensor of shape (N, 4)\n",
        "            Bounding box coordinates to convert.\n",
        "    Return:\n",
        "        bbox_absolute : torch.Tensor of shape (N, 4)\n",
        "    \"\"\"\n",
        "    CELL_SIZE = SIZEHW/S\n",
        "\n",
        "    cx_rcell, cy_rcell, rw, rh = bbox_relative[:,:4].permute(1,0)\n",
        "    \n",
        "    ### xc,yc centers relative to the frame coordinates\n",
        "    cx = cx_rcell * CELL_SIZE - (1/CELL_SIZE) * (cx_rcell/CELL_SIZE).to(torch.int32)\n",
        "    cy = cy_rcell * CELL_SIZE - (1/CELL_SIZE) * (cy_rcell/CELL_SIZE).to(torch.int32)\n",
        "\n",
        "    ### xc,yc centers absolute coordinates\n",
        "    cx_abs = SIZEHW * cx\n",
        "    cy_abs = SIZEHW * cy\n",
        "\n",
        "    ### x,y absolute positions \n",
        "    x_min = cx_abs - (SIZEHW * (rw/2))\n",
        "    y_min = cy_abs - (SIZEHW * (rh/2))\n",
        "    x_max = cx_abs + (SIZEHW * (rw/2))\n",
        "    y_max = cy_abs + (SIZEHW * (rh/2))\n",
        "\n",
        "    bbox_absolute = torch.stack((x_min, y_min, x_max, y_max), dim=-1)\n",
        "    return bbox_absolute\n",
        "\n",
        "def intersection_over_union(pred_box:torch.Tensor, true_box:torch.Tensor)->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Intersection over Union method.\n",
        "\n",
        "    Args:\n",
        "        pred_box : torch.Tensor of shape (N, 5)\n",
        "            Predicted bounding boxes of a batch, in a given cell.\n",
        "        true_box : torch.Tensor of shape (N, 5)\n",
        "            Ground truth bounding boxes of a batch, in a given cell.\n",
        "\n",
        "    Return:\n",
        "        iou : float\n",
        "            Number between 0 and 1 where 1 is a perfect overlap.\n",
        "    \"\"\"\n",
        "    ### Convert cell reltative coordinates to absolute coordinates\n",
        "    pred_box = relative2absolute(pred_box)\n",
        "    true_box = relative2absolute(true_box)   \n",
        "    xmin_pred, ymin_pred, xmax_pred, ymax_pred = pred_box.permute(1,0)\n",
        "    xmin_true, ymin_true, xmax_true, ymax_true = true_box.permute(1,0)\n",
        "\n",
        "    ### There is no object if all coordinates are zero\n",
        "    isObject = xmin_true + ymin_true + xmax_true + ymax_true\n",
        "    isObject = isObject.to(torch.bool)\n",
        "\n",
        "    smoothing_factor = 1e-10\n",
        "\n",
        "    ### x, y overlaps btw pred and groundtrue\n",
        "    xmin_overlap = torch.maximum(xmin_pred, xmin_true)\n",
        "    xmax_overlap = torch.minimum(xmax_pred, xmax_true)\n",
        "    ymin_overlap = torch.maximum(ymin_pred, ymin_true)\n",
        "    ymax_overlap = torch.minimum(ymax_pred, ymax_true)\n",
        "    \n",
        "    ### Pred and groundtrue areas\n",
        "    pred_box_area = (xmax_pred - xmin_pred) * (ymax_pred - ymin_pred)\n",
        "    true_box_area = (xmax_true - xmin_true) * (ymax_true - ymin_true)\n",
        "\n",
        "    ### Compute intersection area, union area and IoU\n",
        "    overlap_area = torch.maximum((xmax_overlap - xmin_overlap), torch.Tensor([0]).to(device)) * torch.maximum((ymax_overlap - ymin_overlap), torch.Tensor([0]).to(device))\n",
        "    union_area = (pred_box_area + true_box_area) - overlap_area\n",
        "    iou = (overlap_area + smoothing_factor) / (union_area + smoothing_factor)\n",
        "    \n",
        "    ### Set IoU to zero when there is no coordinates (i.e. no object)\n",
        "    iou = iou * isObject\n",
        "\n",
        "    return iou   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "OWc1Cr8yrSH8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE BOX :  tensor(0.0010)\n",
            "MSE confidence score :  tensor(0.5613)\n",
            "class acc :  tensor(97.1700) %\n"
          ]
        }
      ],
      "source": [
        "S=6\n",
        "for (img, labels, bbox_true) in validation_dataset:\n",
        "    img, labels, bbox_true = img.to(device), labels.to(device), bbox_true.to(device)\n",
        "    model_MNIST.eval()\n",
        "    with torch.no_grad():\n",
        "        ### prediction\n",
        "        bbox_preds, label_preds = model_MNIST(img)\n",
        "\n",
        "        ### (N,4) -> (N, S, S, 5)\n",
        "        bbox_true_6x6 = bbox2Tensor(bbox_true).to(device)\n",
        "\n",
        "        ### keeping only cells (i,j) with an object \n",
        "        cells_with_obj = bbox_true_6x6.nonzero()[::5]\n",
        "        N, cells_i, cells_j, _ = cells_with_obj.permute(1,0)\n",
        "\n",
        "        ### MSE along bbox coordinates and sizes in the cells containing an object\n",
        "        mse_box = (1/len(img)) * torch.sum(torch.pow(bbox_true - bbox_preds[N, cells_i, cells_j,:4],2))\n",
        "        \n",
        "        ### confidence score accuracy : sum of the all grid confidence scores\n",
        "        ### pred confidence score is confidence score times IoU.\n",
        "        mse_confidence_score = torch.zeros(len(img))\n",
        "        for i in range(S):\n",
        "            for j in range(S):\n",
        "                iou = intersection_over_union(bbox_true_6x6[:,i,j], bbox_preds[:,i,j])\n",
        "                mse_confidence_score += torch.pow(bbox_true_6x6[:,i,j,-1] - bbox_preds[:,i,j,-1] * iou,2)\n",
        "        \n",
        "        mse_confidence_score = (1/(len(img))) * torch.sum(mse_confidence_score)\n",
        "\n",
        "        ### applied softmax to class predictions and compute accuracy\n",
        "        softmax_pred_classes = torch.softmax(label_preds[N, cells_i, cells_j], dim=1)\n",
        "        classes_acc = (1/len(img)) * torch.sum(torch.argmax(labels, dim=1) == torch.argmax(softmax_pred_classes, dim=1))\n",
        "\n",
        "print(\"MSE BOX : \", mse_box)\n",
        "print(\"MSE confidence score : \", mse_confidence_score)\n",
        "print(\"class acc : \", classes_acc*100,\"%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "YOLO_MNIST_Localization.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "bfc2e043f4a41bbce066f7921047bdcfe8769546c7198e4df6482fd90de274f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
