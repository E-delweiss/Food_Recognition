{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLaonuMWt4L2+1Uov/C0sx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "This notebook aims to help the reader to walk through the YoloV1 code for MNIST localization. It will not explain all the code in detail but will focus on the most important parts.\n",
        "\n",
        "I won't go into YoloV1 details so I give you some articles that help me to understand the all process :\n",
        "* YoloV1 [paper](https://arxiv.org/pdf/1506.02640.pdf)\n",
        "* GitHub [repo](https://github.com/zzzheng/pytorch-yolo-v1) of @zzzheng\n",
        "* Medium [article](https://medium.com/mlearning-ai/object-detection-explained-yolo-v1-fb4bcd3d87a1)\n",
        "* @Hackernoon [article](https://hackernoon.com/understanding-yolo-f5a74bbc7967)\n",
        "* Excellent [web page](https://pjreddie.com/darknet/yolo/) about Yolo, Darknet models, and other stuffs from @Joseph Chet Redmon\n",
        "\n",
        "### Terminology :\n",
        "* B : number of bounding boxes generated by the model (here 1)\n",
        "* C : number of classes (here 10)\n",
        "* S : number of grid cells in one direction (here 6)\n",
        "* \\*r or \\*r_img : indicate a value *relatives to the image size*\n",
        "* \\*cr_img : indicate a **center** value *relatives to the image size*\n",
        "* \\*r_cell : indicate a value *relatives to a cell*\n",
        "* \\*cr_cell : indicate a **center** value *relatives to a cell*"
      ],
      "metadata": {
        "id": "_b-_FzR6f5UV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "I use the MNIST dataset from http://yann.lecun.com/exdb/mnist/ available in the `torchvision` framework. This dataset is composed of 60k training and 10k validation 28x28 greyscale images. \n",
        "\n",
        "To create my dataset I proceed through those steps (see [MNIST_dataset.py](https://github.com/ThOpaque/Food_Recognition/blob/main/WarmingUp_with_MNIST/MNIST_dataset.py) for the full code) :\n",
        "* Randomly paste the 28x28 digits into a 75x75 black background\n",
        "* Retrieve the xmin, ymin (top left coordinates), the width and height\n",
        "* Encode those coordinates *relative to the cell* for xmin, ymin and *relative to the image* for the width and height\n",
        "* Include those coordinates in a (N,S,S,B+1) tensor\n",
        "* One-hot encoded the digit labels and include them in a (N,S,S,C) tensor\n",
        "\n",
        "At the end, a SxS grid is obtained for each image. See below, for `S=6` and the digit `0` : its bounding box size is 28x28 (0.3733x0.3733 relatively to the image) and its center is in the cell `(2,1)`, preciselly, at the coordinates `(0.60, 0.32)` *respectively to this cell*.  \n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "#####################################################\n",
        "```python\n",
        "def _encode(self, box, label):    \n",
        "    ...\n",
        "    ### Object grid location\n",
        "    i = (xcr_img / self.cell_size).ceil() - 1.0\n",
        "    j = (ycr_img / self.cell_size).ceil() - 1.0\n",
        "    i, j = int(i), int(j)\n",
        "\n",
        "    ### x & y of the cell left-top corner\n",
        "    x0 = i * self.cell_size\n",
        "    y0 = j * self.cell_size\n",
        "    \n",
        "    ### x & y of the box on the cell, normalized from 0.0 to 1.0.\n",
        "    xcr_cell = (xcr - x0) / self.cell_size\n",
        "    ycr_cell = (ycr - y0) / self.cell_size\n",
        "\n",
        "    ### 4 coords + 1 conf + 10 classes\n",
        "    one_hot_label = F.one_hot(torch.as_tensor(label, dtype=torch.int64), self.C)\n",
        "\n",
        "    box_target = torch.zeros(self.S, self.S, self.B+4)\n",
        "    box_target[j, i, :5] = torch.Tensor([xcr_cell, ycr_cell, wr, hr, 1.])\n",
        "\n",
        "    return box_target, one_hot_label\n",
        "```\n",
        "#####################################################"
      ],
      "metadata": {
        "id": "dcex7lXZivf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Darknet-like model\n",
        "I first started to create the same model from the Yolo paper. \n",
        "\n",
        "[IMG YOLO ARCHITECTURE]\n",
        "\n",
        "But of course, to achieve my purpuse on MNIST, I do not need a such complex model. So, I created a [Darknet-like](https://github.com/ThOpaque/Food_Recognition/blob/main/WarmingUp_with_MNIST/Darknet_like.py) one, by keeping the same global topology :\n",
        "- Using CNN Blocks which contains a convolutional layer, a batch normalization layer and a LeakyReLU activation\n",
        "- CNN Blocks are followed by a MaxPool layer except the last ones before fully connected layers\n",
        "- LeakyReLU activation between the two fully connected layers\n",
        "- The last fully connected layer outputs a tensor of size (N, S, S, C+B+4+1)\n",
        "\n",
        "To make the code cleaner, I choose to dissociate the \"classification output\" (used to classify the digits) and the \"regression output\" (used to predict the bounding box coordinates) in the forward. \n",
        "\n",
        "### Darknet-like model\n",
        "```python\n",
        "class YoloMNIST(torch.nn.Module):\n",
        "    def __init__(self, sizeHW, S, C, B):\n",
        "        ...\n",
        "        self.seq = torch.nn.Sequential()        \n",
        "        self.seq.add_module(f\"conv_1\", CNNBlock(1, 32, stride=2, kernel_size=7, padding=2))\n",
        "        self.seq.add_module(f\"maxpool_1\", torch.nn.MaxPool2d(2))\n",
        "        self.seq.add_module(f\"conv_2\", CNNBlock(32, 128, stride=1, kernel_size=3, padding=0))\n",
        "        self.seq.add_module(f\"maxpool_2\", torch.nn.MaxPool2d(2))\n",
        "        self.seq.add_module(f\"conv_3\", CNNBlock(128, 64, stride=1, kernel_size=1, padding=0))\n",
        "        self.seq.add_module(f\"conv_4\", CNNBlock(64, 128, stride=1, kernel_size=3, padding=0))\n",
        "        self.seq.add_module(f\"conv_5\", CNNBlock(128, 128, stride=1, kernel_size=3, padding=1))\n",
        "        self.fcs = self._create_fcs()\n",
        "\n",
        "    def _create_fcs(self):\n",
        "        output = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(128 * self.S * self.S, 4096),\n",
        "            torch.nn.LeakyReLU(0.1),\n",
        "            torch.nn.Linear(4096, self.S * self.S * (self.C + self.B*5)))\n",
        "        return output\n",
        "\n",
        "    def forward(self, input:torch.Tensor)->tuple:\n",
        "        x = self.seq(input)\n",
        "        x = self.fcs(x)\n",
        "        x = x.view(x.size(0), self.S, self.S, self.B * 5 + self.C)\n",
        "        box_coord = x[:,:,:,0:5]\n",
        "        classifier = x[:,:,:,5:]\n",
        "        return box_coord, classifier\n",
        "```\n"
      ],
      "metadata": {
        "id": "xjACRqpCYEMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yolo Loss\n",
        "From the Yolo paper, I recall the global formula of the Yolo loss. Again, I recommand you to look at the references I listed above.\n",
        " "
      ],
      "metadata": {
        "id": "Tm3O1FVd2Ebu"
      }
    }
  ]
}