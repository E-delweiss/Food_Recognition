{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThOpaque/Food_Recognition/blob/main/WarmingUp_with_MNIST/YOLO_MNIST_Localization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAHeh1BRufX4",
        "outputId": "4cef43db-1d01-46ed-9300-5b6200fec7c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.7.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.9.3)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "import os, time, datetime\n",
        "from timeit import default_timer as timer\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "%pip install torchinfo;\n",
        "%pip install torchmetrics;\n",
        "from torchmetrics import MeanSquaredError;\n",
        "from torchinfo import summary;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOraK1TX7XZB",
        "outputId": "259d5789-4ab7-48b9-f310-4505121f5f11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------------------------\n",
            "Execute notebook on - cpu -\n",
            "------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Choosing device between CPU or GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "# elif torch.has_mps:\n",
        "#     device=torch.device('mps')\n",
        "else:\n",
        "    device=torch.device('cpu')\n",
        "    \n",
        "print(\"\\n------------------------------------\")\n",
        "print(f\"Execute notebook on - {device} -\")\n",
        "print(\"------------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7VztqFE71JZ"
      },
      "outputs": [],
      "source": [
        "class my_mnist_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root:str, split:str=\"train\", download:bool=False, S=6, sizeHW=75):\n",
        "        if split == \"test\":\n",
        "            train = False\n",
        "        else:\n",
        "            train = True\n",
        "        \n",
        "        self.dataset = torchvision.datasets.MNIST(root=root, train=train, download=download)\n",
        "        \n",
        "        self.cell_size = sizeHW / S\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def _numpy_pad_to_bounding_box(self, image, offset_height=0, offset_width=0, target_height=0, target_width=0):\n",
        "        assert image.shape[:-1][0] <= target_height-offset_height, \"height must be <= target - offset\"\n",
        "        assert image.shape[:-1][1] <= target_width-offset_width, \"width must be <= target - offset\"\n",
        "        \n",
        "        target_array = np.zeros((target_height, target_width, image.shape[-1]))\n",
        "\n",
        "        for k in range(image.shape[0]):\n",
        "            target_array[offset_height+k][offset_width:image.shape[1]+offset_width] = image[k]\n",
        "        \n",
        "        return target_array\n",
        "\n",
        "    def _transform_pasting75(self, image, label):\n",
        "        ### xmin, ymin of digit\n",
        "        xmin = torch.randint(0, 48, (1,))\n",
        "        ymin = torch.randint(0, 48, (1,))\n",
        "        \n",
        "        image = torchvision.transforms.ToTensor()(image)\n",
        "        image = torch.reshape(image, (28,28,1,))\n",
        "        image = torch.from_numpy(self._numpy_pad_to_bounding_box(image, ymin, xmin, 75, 75))\n",
        "        image = image.permute(2, 0, 1) #(C,H,W)\n",
        "        image = image.to(torch.float)\n",
        "        \n",
        "        xmin, ymin = xmin.to(torch.float), ymin.to(torch.float)\n",
        "\n",
        "        xmax_bbox, ymax_bbox = (xmin + 28), (ymin + 28)\n",
        "        xmin_bbox, ymin_bbox = xmin, ymin\n",
        "        w_bbox = xmax_bbox-xmin_bbox\n",
        "        h_bbox = ymax_bbox-ymin_bbox\n",
        "\n",
        "        rw = w_bbox / 75\n",
        "        rh = h_bbox / 75\n",
        "        cx = (xmin + (w_bbox/2))/75\n",
        "        cy = (ymin + (h_bbox/2))/75\n",
        "\n",
        "        cx_rcell = cx % self.cell_size / self.cell_size\n",
        "        cy_rcell = cy % self.cell_size / self.cell_size\n",
        "\n",
        "\n",
        "        label_one_hot = F.one_hot(torch.as_tensor(label, dtype=torch.int64), 10)\n",
        "        bbox_coord = torch.Tensor([cx_rcell, cy_rcell, rw, rh])\n",
        "\n",
        "        return image, label_one_hot, bbox_coord\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        image, one_hot_label, bbox_coord = self._transform_pasting75(self.dataset[idx][0], self.dataset[idx][1])\n",
        "        \n",
        "        return image, one_hot_label.to(torch.float), bbox_coord\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423,
          "referenced_widgets": [
            "1082b52862584f47ac371b7c37e45f32",
            "536017ec05ad45b28cacb6abf83795cc",
            "2debc45cf0ee4ecdbb84ade1ab39615b",
            "1c871c8171144624a9c857c8e4354a7a",
            "db27cee5aa524f5c89ee9ef3c50a0e60",
            "2f398dda6e744f01904c796703da58cd",
            "dee265ee475f44329263cd7b22ee1637",
            "628dc217877c4f539974877fa3d87696",
            "8635c9299fef4220aab7ee1765e2709c",
            "2dfc26754d20455ba0da75e33218a36e",
            "393d0d522b4743e9a04a4e3d708f529f",
            "4104399bac454257be472b40203ef926",
            "3664bfa18d95441bbf8a0875cc2e8f56",
            "b7243342e8e9473e8695e687276e9eac",
            "183da87c5a514626bff112af3cbbcffa",
            "d5481277c0294251a3b141983ec2950c",
            "15636d229dbf4a54b9f11faf54658dba",
            "498e99190ffc4810b5150c07da2e1b3d",
            "08133968c3a24449b92b80f7eb61e4d9",
            "88f47dba11ff48009ad3440061de6ee2",
            "05564490bce74bedaf40cbc368909fd4",
            "889032b33cd548cabbce84d692a18a9a",
            "ec5a25f7484a4bb0b6c404a183f78343",
            "deb656adb8114376b067f10b37f6232b",
            "ed55481101c446ac900f073ed6272749",
            "0fc8eb581ed9457aacb56dc01a9dc9e6",
            "aa921fc6bdef41049de138c4678ef731",
            "65d9876405d94ac09981657509957cc0",
            "6b5cf599b6f14f59abbd59cdfeb2757c",
            "5b0a6f743129427d8be2e358ac90c848",
            "60149f89d4684950b8de058c6479e2ec",
            "57f9abfdefd74ab6b4806bc500e7f8c1",
            "54ecc4bec948431d8202cafaedffe8f6",
            "858bd330763a4c71b0c9287f2e5ad46e",
            "f78eed65f4724831a6dfcf5272b25993",
            "6ce32bcdc7714b84ac70f7a48835cc49",
            "26b940b2d66a4cb59306b2a3abb8b6b5",
            "cd8c25d30c8e46239415fb04da6f9bb5",
            "d1b608ab95554c9aaf779b61ca7c5cbf",
            "699a5bbfd9ed494ca22f5067871ed5b1",
            "b183fa1ae8ec4e9ebb00943f16fd4752",
            "adaba7d4206e4a68a02229712d9de299",
            "85c6e81be5de4a2a92f92fa28606274e",
            "8b77517829854981acb8f1bf7eb9307c"
          ]
        },
        "id": "yQNznLfO8jOx",
        "outputId": "9c0bccfa-f9b5-4373-a538-51e594faed4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/9912422 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1082b52862584f47ac371b7c37e45f32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/28881 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4104399bac454257be472b40203ef926"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1648877 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec5a25f7484a4bb0b6c404a183f78343"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4542 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "858bd330763a4c71b0c9287f2e5ad46e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def get_training_dataset(BATCH_SIZE=64):\n",
        "    \"\"\"\n",
        "    Loads and maps the training split of the dataset using the custom dataset class. \n",
        "    \"\"\"\n",
        "    dataset = my_mnist_dataset(root=\"data\", split=\"train\", download=True)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    return dataloader, len(dataset)\n",
        "\n",
        "def get_validation_dataset(BATCH_SIZE = None):\n",
        "    \"\"\"\n",
        "    Loads and maps the validation split of the datasetusing the custom dataset class. \n",
        "    \"\"\"\n",
        "    dataset = my_mnist_dataset(root=\"data\", split=\"test\", download=True)\n",
        "    if BATCH_SIZE is None:\n",
        "        BATCH_SIZE = len(dataset)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    return dataloader, len(dataset)\n",
        "\n",
        "# instantiate the datasets\n",
        "training_dataset, len_training_ds = get_training_dataset()\n",
        "validation_dataset, len_validation_ds = get_validation_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfgUx1srt90c"
      },
      "outputs": [],
      "source": [
        "class CNNBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(CNNBlock, self).__init__()\n",
        "        self.conv = torch.nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "        self.bn = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.l_relu = torch.nn.LeakyReLU(0.1)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        x = self.conv(input)\n",
        "        x = self.bn(x)\n",
        "        return self.l_relu(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VjCkTEce-CT"
      },
      "outputs": [],
      "source": [
        "class YoloMNIST(torch.nn.Module):\n",
        "    def __init__(self, sizeHW, S, C, B):\n",
        "        super(YoloMNIST, self).__init__()\n",
        "        self.S, self.C, self.B = S, C, B\n",
        "        self.sizeHW = sizeHW\n",
        "        self.cell_size = self.sizeHW / self.S\n",
        "\n",
        "        self.seq = torch.nn.Sequential()        \n",
        "        self.seq.add_module(f\"conv_1\", CNNBlock(1, 32, stride=2, kernel_size=7, padding=2))\n",
        "        self.seq.add_module(f\"maxpool_1\", torch.nn.MaxPool2d(2))\n",
        "        self.seq.add_module(f\"conv_3\", CNNBlock(32, 128, stride=1, kernel_size=3, padding=0))\n",
        "        self.seq.add_module(f\"maxpool_2\", torch.nn.MaxPool2d(2))\n",
        "        self.seq.add_module(f\"conv_5\", CNNBlock(128, 64, stride=1, kernel_size=1, padding=0))\n",
        "        self.seq.add_module(f\"conv_4\", CNNBlock(64, 128, stride=1, kernel_size=3, padding=0))\n",
        "        self.seq.add_module(f\"conv_6\", CNNBlock(128, 128, stride=1, kernel_size=3, padding=1))\n",
        "        \n",
        "        self.fcs = self._create_fcs()\n",
        "\n",
        "    def _size_output(self, sizeHW:int, kernel:int, stride:int, padding:int=0, isMaxPool:bool=False)->int:\n",
        "        \"\"\"\n",
        "        Output size (width/height) of convolutional or maxpool layers.\n",
        "\n",
        "        Args:\n",
        "            sizeHW : int\n",
        "                Image size (we suppose this is a square image)\n",
        "            kernel : int\n",
        "                Size of a square kernel\n",
        "            stride : int\n",
        "                Stride of convolution layer\n",
        "            padding : int\n",
        "                Padding of convolution layer\n",
        "            isMaxPool : Bool, default is False.\n",
        "                Specify if it is a Maxpool layer (True) or not (False). \n",
        "\n",
        "        Return:\n",
        "            output_size : int\n",
        "                Image output size after a convolutional or MaxPool layer.\n",
        "        \"\"\" \n",
        "        if isMaxPool == True:\n",
        "            output_size = int(sizeHW/2)\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "        if padding == 'same':\n",
        "            output_size = sizeHW\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "        else:\n",
        "            output_size = (sizeHW + 2 * padding - (kernel-1)-1)/stride\n",
        "            output_size = int(output_size + 1)\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "\n",
        "    def _create_fcs(self):\n",
        "        output = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(128 * self.S * self.S, 4096),\n",
        "            torch.nn.LeakyReLU(0.1),\n",
        "            torch.nn.Linear(4096, self.S * self.S * (self.C + self.B*5))\n",
        "        )\n",
        "        return output\n",
        "    \n",
        "\n",
        "    def forward(self, input:torch.Tensor)->tuple:\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            input : torch.Tensor of shape (N, C, H, W)\n",
        "                Batch of images.\n",
        "\n",
        "        Return:\n",
        "            box_coord : torch.Tensor of shape (N, 6, 6, 5)\n",
        "                Contains xc_rcell, yc_rcell, rw, rh and the confidence number c\n",
        "                over 6x6 grid cells.\n",
        "            classifier : torch.Tensor of shape (N, 6, 6, 10)\n",
        "                Contains the one-hot encoding of each digit number over\n",
        "                6x6 grid cells.\n",
        "        \"\"\"     \n",
        "        x = self.seq(input)\n",
        "        x = self.fcs(x)\n",
        "        x = x.view(x.size(0), self.S, self.S, self.B * 5 + self.C)\n",
        "        box_coord = x[:,:,:,0:5]\n",
        "        classifier = x[:,:,:,5:]\n",
        "        return box_coord, classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_jOg_i_p2_c"
      },
      "outputs": [],
      "source": [
        "class YoloLoss(torch.nn.Module):\n",
        "    def __init__(self, lambd_coord:int, lambd_noobj:float, device:torch.device, S:int=6):\n",
        "        super(YoloLoss, self).__init__()\n",
        "        self.LAMBD_COORD = lambd_coord\n",
        "        self.LAMBD_NOOBJ = lambd_noobj\n",
        "        self.S = S\n",
        "        self.device = device\n",
        "\n",
        "    def _coordloss(self, pred_coord_rcell, true_coord_rcell):\n",
        "        \"\"\"\n",
        "        Args : \n",
        "            pred_coord_rcell : torch.Tensor of shape (N, 2)\n",
        "            true_coord_rcell : torch.Tensor of shape (N, 2)\n",
        "        Returns :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        xc_hat, yc_hat = pred_coord_rcell.permute(1,0)\n",
        "        xc, yc = true_coord_rcell.permute(1,0)\n",
        "\n",
        "        squared_error = torch.pow(xc - xc_hat,2) + torch.pow(yc - yc_hat,2)\n",
        "        return squared_error\n",
        "\n",
        "    def _sizeloss(self, pred_size, true_size):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_size : torch.Tensor of shape (N, 2)\n",
        "            true_size : torch.Tensor of shape (N, 2)\n",
        "        Returns : \n",
        "            root_squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        rw_hat, rh_hat = pred_size.permute(1,0)\n",
        "        rw, rh = true_size.permute(1,0)\n",
        "\n",
        "        #sizes can't be negative\n",
        "        rw_hat = rw_hat.clip(min=0)\n",
        "        rh_hat = rh_hat.clip(min=0)\n",
        "\n",
        "        root_squared_error_w = torch.pow(torch.sqrt(rw) - torch.sqrt(rw_hat),2)\n",
        "        root_squared_error_h = torch.pow(torch.sqrt(rh) - torch.sqrt(rh_hat),2)\n",
        "        root_squared_error = root_squared_error_w + root_squared_error_h\n",
        "        return root_squared_error\n",
        "\n",
        "    def _confidenceloss(self, pred_c, true_c):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_c : torch.Tensor of shape (N)\n",
        "            true_c : torch.Tensor of shape (N)\n",
        "        Return :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        squared_error = torch.pow(true_c - pred_c, 2)\n",
        "        return squared_error\n",
        "\n",
        "    def _classloss(self, pred_class, true_class):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_class : torch.Tensor of shape (N, 10)\n",
        "            true_class : torch.Tensor of shape (N, 10)\n",
        "        Returns :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        squared_error = torch.pow(true_class - pred_class, 2)\n",
        "        return torch.sum(squared_error, dim=1)\n",
        "\n",
        "    def forward(self, pred_box:torch.Tensor, true_box:torch.Tensor, pred_class:torch.Tensor, true_class:torch.Tensor):\n",
        "        \"\"\"\n",
        "        Grid forward pass.\n",
        "\n",
        "        Args:\n",
        "            pred_box : torch.Tensor of shape (N, S, S, 5)\n",
        "                Batch predicted outputs containing xc_rcell, yc_rcell, rw, rh,\n",
        "                and confident number c for each grid cell.\n",
        "            true_box : torch.Tensor of shape (N, S, S, 5)\n",
        "                Groundtrue batch containing bbox values for each cell and\n",
        "                c indicate if there is an object to detect or not (1/0).\n",
        "            pred_class : torch.Tensor of shape (N, S, S, 10)\n",
        "                Probability of each digit class in each grid cell\n",
        "            true_class : torch.Tensor of shape (N, 10)\n",
        "                one-hot vect of each digit\n",
        "\n",
        "        Return:\n",
        "            loss : float\n",
        "                The batch loss value of the grid\n",
        "        \"\"\"\n",
        "        BATCH_SIZE = len(pred_box)\n",
        "\n",
        "        ### Initialization of the losses\n",
        "        losses_list = ['loss_xy', 'loss_wh', 'loss_conf_obj', 'loss_conf_noobj', 'loss_class','isObject']\n",
        "        losses = {key : torch.zeros(BATCH_SIZE).to(self.device) for key in losses_list}\n",
        "        check_loss = []\n",
        "        ### Compute the losses for all images in the batch\n",
        "        for i in range(self.S):\n",
        "            for j in range(self.S):\n",
        "                ### Intersection over Union\n",
        "                #IoU = self._intersection_over_union(pred_box[:,i,j], true_box[:,i,j])\n",
        "\n",
        "                ### bbox coordinates\n",
        "                xy_hat = pred_box[:,i,j,:2]\n",
        "                xy = true_box[:,i,j,:2]\n",
        "                wh_hat = pred_box[:,i,j,2:4]\n",
        "                wh = true_box[:,i,j,2:4]\n",
        "                \n",
        "                ### confidence numbers\n",
        "                pred_c = pred_box[:,i,j,4]# * IoU\n",
        "                true_c = true_box[:,i,j,4]\n",
        "\n",
        "                ### objects to detect\n",
        "                isObject = true_c.to(torch.bool)\n",
        "                isNoObject = torch.logical_not(true_c) #(~bool) doesn't work on MPS device\n",
        "\n",
        "                ### sum the losses over the grid\n",
        "                losses['isObject'] += isObject\n",
        "                losses['loss_xy'] += isObject * self._coordloss(xy_hat, xy)\n",
        "                check_loss.append(losses['loss_xy'])\n",
        "                losses['loss_wh'] += isObject * self._sizeloss(wh_hat, wh)\n",
        "                losses['loss_conf_obj'] += isObject * self._confidenceloss(pred_c, true_c)\n",
        "                losses['loss_conf_noobj'] += isNoObject * self._confidenceloss(pred_c, true_c)\n",
        "                losses['loss_class'] += isObject * self._classloss(pred_class[:,i,j], true_class)\n",
        "        \n",
        "\n",
        "\n",
        "        ### Yolo_v1 loss over the batch, shape : (BATCH_SIZE)\n",
        "        loss = self.LAMBD_COORD * losses['loss_xy'] \\\n",
        "                + self.LAMBD_COORD * losses['loss_wh'] \\\n",
        "                + losses['loss_conf_obj'] \\\n",
        "                + self.LAMBD_NOOBJ * losses['loss_conf_noobj'] \\\n",
        "                + losses['loss_class']\n",
        "\n",
        "\n",
        "        assert torch.isnan(torch.sum(losses['loss_conf_obj']))==False, \"La loss {} est devenu nan\".format('loss_conf_obj')\n",
        "        assert torch.isnan(torch.sum(losses['loss_conf_noobj']))==False, \"La loss {} est devenu nan\".format('loss_conf_noobj')\n",
        "        assert torch.isnan(torch.sum(losses['loss_class']))==False, \"La loss {} est devenu nan\".format('loss_class')\n",
        "        assert torch.isnan(torch.sum(losses['isObject']))==False, \"La loss {} est devenu nan\".format('isObject')\n",
        "        assert torch.isnan(torch.sum(losses['loss_wh']))==False, \"La loss {} est devenu nan\".format('loss_wh')\n",
        "        assert torch.isnan(torch.sum(losses['loss_xy']))==False, \"La loss {} est devenu nan\".format('loss_xy')\n",
        "\n",
        "\n",
        "        loss = torch.sum(loss) / BATCH_SIZE\n",
        "\n",
        "        return check_loss, losses, loss\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdIlkN61mXJj"
      },
      "outputs": [],
      "source": [
        "def bbox2Tensor(bbox:torch.Tensor, S:int=6, sizeHW:int=75, device=torch.device('cpu'))->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Constructs en Tensor and puts bbox values in the corresponding i,j grid cell.\n",
        "\n",
        "    Args :\n",
        "        bbox : torch.Tensor of shape (N,4)\n",
        "            Contains bbox values xc_rcell, yc_rcell, rw and rh.\n",
        "        S : int, default is 6\n",
        "            Size of the grid.\n",
        "        sizeHW : int, default is 75\n",
        "            Size of the image.\n",
        "\n",
        "    Return :\n",
        "        bbox_t : torch.Tensor of shape (N, S, S, 5)\n",
        "            Tensor containing all 4 bbox values in the corresponding i,j grid\n",
        "            cell position i.e. in the i,j position where an object should be\n",
        "            detected.\n",
        "    \"\"\"\n",
        "    assert bbox.shape[-1] == 4, \"True bbox should be of size (N,S,S,4).\"\n",
        "\n",
        "    N = len(bbox)\n",
        "    bbox_t = torch.zeros(N,S,S,5).to(device)\n",
        "    cell_size = sizeHW/S\n",
        "\n",
        "    xc_rcell, yc_rcell, rw, rh = bbox.permute(1,0).to(device)\n",
        "    xc = xc_rcell * cell_size - (1/cell_size) * (xc_rcell/cell_size).to(torch.int32)\n",
        "    yc = yc_rcell * cell_size - (1/cell_size) * (yc_rcell/cell_size).to(torch.int32)\n",
        "\n",
        "    N_range = torch.arange(N)\n",
        "    lines = (yc * S).to(torch.long)\n",
        "    columns = (xc * S).to(torch.long)\n",
        "    bbox_t[N_range, lines, columns] = torch.stack((xc_rcell, yc_rcell, rw, rh, torch.ones(N))).permute(1,0)\n",
        "    \n",
        "    return bbox_t.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHUOkfWPe-CW"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "model_MNIST = YoloMNIST(sizeHW=75, S=6, C=10, B=1)\n",
        "model_MNIST = model_MNIST.to(device)\n",
        "optimizer = torch.optim.Adam(params=model_MNIST.parameters(), lr=learning_rate, weight_decay=0.0005)\n",
        "loss_yolo = YoloLoss(lambd_coord=5, lambd_noobj=0.5, S=6, device=device)\n",
        "\n",
        "# print(optimizer)\n",
        "#summary(model_MNIST, input_size = (BATCH_SIZE,1,75,75))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9tSO0Qo7e-CX",
        "outputId": "30e9cb02-0115-4295-8901-373725937086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[START] : 2022-08-26 17:58:48 :\n",
            "[Training on] : CPU\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 1/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.09439\n",
            "xy_coord training loss for this batch : 0.00019\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.03031\n",
            "confidence without object training loss for this batch : 0.04939\n",
            "class proba training loss for this batch : 0.03779\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.08519\n",
            "xy_coord training loss for this batch : 0.00099\n",
            "wh_sizes training loss for this batch : 0.00031\n",
            "confidence with object training loss for this batch : 0.02470\n",
            "confidence without object training loss for this batch : 0.05698\n",
            "class proba training loss for this batch : 0.02549\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.10395\n",
            "xy_coord training loss for this batch : 0.00023\n",
            "wh_sizes training loss for this batch : 0.00018\n",
            "confidence with object training loss for this batch : 0.03329\n",
            "confidence without object training loss for this batch : 0.05603\n",
            "class proba training loss for this batch : 0.04058\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.10638\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.02419\n",
            "confidence without object training loss for this batch : 0.05187\n",
            "class proba training loss for this batch : 0.05551\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.10645\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.02743\n",
            "confidence without object training loss for this batch : 0.05348\n",
            "class proba training loss for this batch : 0.05138\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.13770\n",
            "xy_coord training loss for this batch : 0.00003\n",
            "wh_sizes training loss for this batch : 0.00020\n",
            "confidence with object training loss for this batch : 0.02019\n",
            "confidence without object training loss for this batch : 0.04441\n",
            "class proba training loss for this batch : 0.09417\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.07867\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.01098\n",
            "confidence without object training loss for this batch : 0.01979\n",
            "class proba training loss for this batch : 0.05674\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.12112\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.02806\n",
            "confidence without object training loss for this batch : 0.04989\n",
            "class proba training loss for this batch : 0.06707\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.08081\n",
            "xy_coord training loss for this batch : 0.00034\n",
            "wh_sizes training loss for this batch : 0.00050\n",
            "confidence with object training loss for this batch : 0.01680\n",
            "confidence without object training loss for this batch : 0.02968\n",
            "class proba training loss for this batch : 0.04498\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.05703\n",
            "xy_coord training loss for this batch : 0.00013\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.01417\n",
            "confidence without object training loss for this batch : 0.03282\n",
            "class proba training loss for this batch : 0.02498\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.08396\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.03066\n",
            "confidence without object training loss for this batch : 0.06265\n",
            "class proba training loss for this batch : 0.02106\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:57.650195\n",
            "Mean training loss for this epoch : 0.09038\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 2/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.09173\n",
            "xy_coord training loss for this batch : 0.00012\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.01858\n",
            "confidence without object training loss for this batch : 0.03295\n",
            "class proba training loss for this batch : 0.05538\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.06509\n",
            "xy_coord training loss for this batch : 0.00012\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.01541\n",
            "confidence without object training loss for this batch : 0.03136\n",
            "class proba training loss for this batch : 0.03270\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.07973\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.01547\n",
            "confidence without object training loss for this batch : 0.03445\n",
            "class proba training loss for this batch : 0.04607\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.07496\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.02685\n",
            "confidence without object training loss for this batch : 0.05310\n",
            "class proba training loss for this batch : 0.02053\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.08368\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00019\n",
            "confidence with object training loss for this batch : 0.01975\n",
            "confidence without object training loss for this batch : 0.03551\n",
            "class proba training loss for this batch : 0.04471\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.06606\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.01489\n",
            "confidence without object training loss for this batch : 0.03546\n",
            "class proba training loss for this batch : 0.03223\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.11111\n",
            "xy_coord training loss for this batch : 0.00018\n",
            "wh_sizes training loss for this batch : 0.00018\n",
            "confidence with object training loss for this batch : 0.02116\n",
            "confidence without object training loss for this batch : 0.03725\n",
            "class proba training loss for this batch : 0.06955\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.14527\n",
            "xy_coord training loss for this batch : 0.00013\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.05380\n",
            "confidence without object training loss for this batch : 0.08763\n",
            "class proba training loss for this batch : 0.04650\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.07322\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.02709\n",
            "confidence without object training loss for this batch : 0.04310\n",
            "class proba training loss for this batch : 0.02347\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.11145\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.04289\n",
            "confidence without object training loss for this batch : 0.06041\n",
            "class proba training loss for this batch : 0.03730\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.11547\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00016\n",
            "confidence with object training loss for this batch : 0.05234\n",
            "confidence without object training loss for this batch : 0.07041\n",
            "class proba training loss for this batch : 0.02666\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:56.459185\n",
            "Mean training loss for this epoch : 0.08923\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 3/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.08336\n",
            "xy_coord training loss for this batch : 0.00036\n",
            "wh_sizes training loss for this batch : 0.00025\n",
            "confidence with object training loss for this batch : 0.02421\n",
            "confidence without object training loss for this batch : 0.04690\n",
            "class proba training loss for this batch : 0.03268\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.06516\n",
            "xy_coord training loss for this batch : 0.00012\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.01404\n",
            "confidence without object training loss for this batch : 0.03006\n",
            "class proba training loss for this batch : 0.03482\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.08599\n",
            "xy_coord training loss for this batch : 0.00003\n",
            "wh_sizes training loss for this batch : 0.00020\n",
            "confidence with object training loss for this batch : 0.02775\n",
            "confidence without object training loss for this batch : 0.04570\n",
            "class proba training loss for this batch : 0.03421\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.06789\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.01728\n",
            "confidence without object training loss for this batch : 0.03030\n",
            "class proba training loss for this batch : 0.03451\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.09584\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00018\n",
            "confidence with object training loss for this batch : 0.02434\n",
            "confidence without object training loss for this batch : 0.03720\n",
            "class proba training loss for this batch : 0.05159\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.06190\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.01276\n",
            "confidence without object training loss for this batch : 0.02326\n",
            "class proba training loss for this batch : 0.03652\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.07418\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02107\n",
            "confidence without object training loss for this batch : 0.03494\n",
            "class proba training loss for this batch : 0.03481\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.06980\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.02522\n",
            "confidence without object training loss for this batch : 0.04467\n",
            "class proba training loss for this batch : 0.02124\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.09082\n",
            "xy_coord training loss for this batch : 0.00019\n",
            "wh_sizes training loss for this batch : 0.00020\n",
            "confidence with object training loss for this batch : 0.02565\n",
            "confidence without object training loss for this batch : 0.06633\n",
            "class proba training loss for this batch : 0.03006\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.06620\n",
            "xy_coord training loss for this batch : 0.00014\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.01700\n",
            "confidence without object training loss for this batch : 0.02717\n",
            "class proba training loss for this batch : 0.03439\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.09271\n",
            "xy_coord training loss for this batch : 0.00017\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.02175\n",
            "confidence without object training loss for this batch : 0.04452\n",
            "class proba training loss for this batch : 0.04726\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:56.631517\n",
            "Mean training loss for this epoch : 0.08772\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 4/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.08024\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00019\n",
            "confidence with object training loss for this batch : 0.02737\n",
            "confidence without object training loss for this batch : 0.04552\n",
            "class proba training loss for this batch : 0.02866\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.09471\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00018\n",
            "confidence with object training loss for this batch : 0.03293\n",
            "confidence without object training loss for this batch : 0.06195\n",
            "class proba training loss for this batch : 0.02950\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.10564\n",
            "xy_coord training loss for this batch : 0.00014\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.03292\n",
            "confidence without object training loss for this batch : 0.05261\n",
            "class proba training loss for this batch : 0.04511\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.07517\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.02474\n",
            "confidence without object training loss for this batch : 0.04318\n",
            "class proba training loss for this batch : 0.02789\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.10294\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.02586\n",
            "confidence without object training loss for this batch : 0.04466\n",
            "class proba training loss for this batch : 0.05376\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.08961\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02195\n",
            "confidence without object training loss for this batch : 0.03785\n",
            "class proba training loss for this batch : 0.04774\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.06810\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.01643\n",
            "confidence without object training loss for this batch : 0.02890\n",
            "class proba training loss for this batch : 0.03642\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.10339\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.03173\n",
            "confidence without object training loss for this batch : 0.04481\n",
            "class proba training loss for this batch : 0.04821\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.08301\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.02701\n",
            "confidence without object training loss for this batch : 0.04607\n",
            "class proba training loss for this batch : 0.03189\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.06550\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.01781\n",
            "confidence without object training loss for this batch : 0.04599\n",
            "class proba training loss for this batch : 0.02370\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.10133\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.02585\n",
            "confidence without object training loss for this batch : 0.04989\n",
            "class proba training loss for this batch : 0.04933\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:55.753316\n",
            "Mean training loss for this epoch : 0.08465\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 5/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.11983\n",
            "xy_coord training loss for this batch : 0.00015\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.03035\n",
            "confidence without object training loss for this batch : 0.04148\n",
            "class proba training loss for this batch : 0.06724\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.11224\n",
            "xy_coord training loss for this batch : 0.00027\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.03120\n",
            "confidence without object training loss for this batch : 0.05016\n",
            "class proba training loss for this batch : 0.05388\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.07145\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.02110\n",
            "confidence without object training loss for this batch : 0.04308\n",
            "class proba training loss for this batch : 0.02811\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.08510\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.01724\n",
            "confidence without object training loss for this batch : 0.03003\n",
            "class proba training loss for this batch : 0.05171\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.08496\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.02557\n",
            "confidence without object training loss for this batch : 0.04429\n",
            "class proba training loss for this batch : 0.03622\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.12778\n",
            "xy_coord training loss for this batch : 0.00004\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.02765\n",
            "confidence without object training loss for this batch : 0.05068\n",
            "class proba training loss for this batch : 0.07411\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.09183\n",
            "xy_coord training loss for this batch : 0.00016\n",
            "wh_sizes training loss for this batch : 0.00046\n",
            "confidence with object training loss for this batch : 0.02159\n",
            "confidence without object training loss for this batch : 0.03889\n",
            "class proba training loss for this batch : 0.04772\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.09356\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.02913\n",
            "confidence without object training loss for this batch : 0.05331\n",
            "class proba training loss for this batch : 0.03642\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.07143\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.02738\n",
            "confidence without object training loss for this batch : 0.04739\n",
            "class proba training loss for this batch : 0.01931\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.09531\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02921\n",
            "confidence without object training loss for this batch : 0.04613\n",
            "class proba training loss for this batch : 0.04205\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.09217\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.03564\n",
            "confidence without object training loss for this batch : 0.05594\n",
            "class proba training loss for this batch : 0.02745\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:55.334478\n",
            "Mean training loss for this epoch : 0.08431\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 6/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.07006\n",
            "xy_coord training loss for this batch : 0.00015\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.01923\n",
            "confidence without object training loss for this batch : 0.03845\n",
            "class proba training loss for this batch : 0.03015\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.09544\n",
            "xy_coord training loss for this batch : 0.00017\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.02448\n",
            "confidence without object training loss for this batch : 0.04319\n",
            "class proba training loss for this batch : 0.04785\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.08287\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.02044\n",
            "confidence without object training loss for this batch : 0.03612\n",
            "class proba training loss for this batch : 0.04363\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.07609\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00016\n",
            "confidence with object training loss for this batch : 0.02348\n",
            "confidence without object training loss for this batch : 0.03572\n",
            "class proba training loss for this batch : 0.03365\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.07497\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.03330\n",
            "confidence without object training loss for this batch : 0.04344\n",
            "class proba training loss for this batch : 0.01904\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.12991\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.04061\n",
            "confidence without object training loss for this batch : 0.06286\n",
            "class proba training loss for this batch : 0.05702\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.06424\n",
            "xy_coord training loss for this batch : 0.00015\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.01527\n",
            "confidence without object training loss for this batch : 0.03205\n",
            "class proba training loss for this batch : 0.03135\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.10622\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00023\n",
            "confidence with object training loss for this batch : 0.03240\n",
            "confidence without object training loss for this batch : 0.05534\n",
            "class proba training loss for this batch : 0.04450\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.05259\n",
            "xy_coord training loss for this batch : 0.00018\n",
            "wh_sizes training loss for this batch : 0.00023\n",
            "confidence with object training loss for this batch : 0.01570\n",
            "confidence without object training loss for this batch : 0.03024\n",
            "class proba training loss for this batch : 0.01975\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.08334\n",
            "xy_coord training loss for this batch : 0.00054\n",
            "wh_sizes training loss for this batch : 0.00021\n",
            "confidence with object training loss for this batch : 0.02755\n",
            "confidence without object training loss for this batch : 0.03871\n",
            "class proba training loss for this batch : 0.03268\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.12842\n",
            "xy_coord training loss for this batch : 0.00024\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.04882\n",
            "confidence without object training loss for this batch : 0.07335\n",
            "class proba training loss for this batch : 0.04102\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:56.721960\n",
            "Mean training loss for this epoch : 0.08369\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 7/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.05451\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00019\n",
            "confidence with object training loss for this batch : 0.01478\n",
            "confidence without object training loss for this batch : 0.02802\n",
            "class proba training loss for this batch : 0.02422\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.06715\n",
            "xy_coord training loss for this batch : 0.00014\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.02101\n",
            "confidence without object training loss for this batch : 0.03877\n",
            "class proba training loss for this batch : 0.02538\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.06385\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.01487\n",
            "confidence without object training loss for this batch : 0.03632\n",
            "class proba training loss for this batch : 0.02952\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.10680\n",
            "xy_coord training loss for this batch : 0.00013\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.03402\n",
            "confidence without object training loss for this batch : 0.05434\n",
            "class proba training loss for this batch : 0.04432\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.10260\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.04385\n",
            "confidence without object training loss for this batch : 0.05872\n",
            "class proba training loss for this batch : 0.02817\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.09076\n",
            "xy_coord training loss for this batch : 0.00018\n",
            "wh_sizes training loss for this batch : 0.00018\n",
            "confidence with object training loss for this batch : 0.02743\n",
            "confidence without object training loss for this batch : 0.05180\n",
            "class proba training loss for this batch : 0.03565\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.05871\n",
            "xy_coord training loss for this batch : 0.00004\n",
            "wh_sizes training loss for this batch : 0.00007\n",
            "confidence with object training loss for this batch : 0.01105\n",
            "confidence without object training loss for this batch : 0.02359\n",
            "class proba training loss for this batch : 0.03532\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.04066\n",
            "xy_coord training loss for this batch : 0.00003\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.01180\n",
            "confidence without object training loss for this batch : 0.02655\n",
            "class proba training loss for this batch : 0.01506\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.07510\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.02218\n",
            "confidence without object training loss for this batch : 0.03829\n",
            "class proba training loss for this batch : 0.03287\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.05745\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.01998\n",
            "confidence without object training loss for this batch : 0.03895\n",
            "class proba training loss for this batch : 0.01697\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.19073\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.02661\n",
            "confidence without object training loss for this batch : 0.04557\n",
            "class proba training loss for this batch : 0.14070\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:12.574158\n",
            "Mean training loss for this epoch : 0.08056\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 8/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.07927\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00007\n",
            "confidence with object training loss for this batch : 0.02852\n",
            "confidence without object training loss for this batch : 0.04449\n",
            "class proba training loss for this batch : 0.02766\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.10151\n",
            "xy_coord training loss for this batch : 0.00024\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.03401\n",
            "confidence without object training loss for this batch : 0.05782\n",
            "class proba training loss for this batch : 0.03673\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.07040\n",
            "xy_coord training loss for this batch : 0.00013\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.02990\n",
            "confidence without object training loss for this batch : 0.05196\n",
            "class proba training loss for this batch : 0.01314\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.07875\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.02615\n",
            "confidence without object training loss for this batch : 0.04449\n",
            "class proba training loss for this batch : 0.02938\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.09732\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02366\n",
            "confidence without object training loss for this batch : 0.04658\n",
            "class proba training loss for this batch : 0.04968\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.07584\n",
            "xy_coord training loss for this batch : 0.00020\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.02877\n",
            "confidence without object training loss for this batch : 0.04067\n",
            "class proba training loss for this batch : 0.02504\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.10806\n",
            "xy_coord training loss for this batch : 0.00024\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.04151\n",
            "confidence without object training loss for this batch : 0.06483\n",
            "class proba training loss for this batch : 0.03224\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.09691\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.03430\n",
            "confidence without object training loss for this batch : 0.05421\n",
            "class proba training loss for this batch : 0.03440\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.07912\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.03432\n",
            "confidence without object training loss for this batch : 0.04667\n",
            "class proba training loss for this batch : 0.02029\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.07137\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00005\n",
            "confidence with object training loss for this batch : 0.02058\n",
            "confidence without object training loss for this batch : 0.03914\n",
            "class proba training loss for this batch : 0.03063\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.05552\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.01215\n",
            "confidence without object training loss for this batch : 0.01576\n",
            "class proba training loss for this batch : 0.03446\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:06.819568\n",
            "Mean training loss for this epoch : 0.08130\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 9/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.06489\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.00946\n",
            "confidence without object training loss for this batch : 0.02048\n",
            "class proba training loss for this batch : 0.04447\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.05294\n",
            "xy_coord training loss for this batch : 0.00004\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.01566\n",
            "confidence without object training loss for this batch : 0.02475\n",
            "class proba training loss for this batch : 0.02422\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.07293\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02097\n",
            "confidence without object training loss for this batch : 0.03436\n",
            "class proba training loss for this batch : 0.03407\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.10113\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.02283\n",
            "confidence without object training loss for this batch : 0.03617\n",
            "class proba training loss for this batch : 0.05917\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.06490\n",
            "xy_coord training loss for this batch : 0.00026\n",
            "wh_sizes training loss for this batch : 0.00020\n",
            "confidence with object training loss for this batch : 0.02060\n",
            "confidence without object training loss for this batch : 0.04534\n",
            "class proba training loss for this batch : 0.01933\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.11825\n",
            "xy_coord training loss for this batch : 0.00014\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.02599\n",
            "confidence without object training loss for this batch : 0.05779\n",
            "class proba training loss for this batch : 0.06195\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.06326\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.01985\n",
            "confidence without object training loss for this batch : 0.02962\n",
            "class proba training loss for this batch : 0.02780\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.06217\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00016\n",
            "confidence with object training loss for this batch : 0.02094\n",
            "confidence without object training loss for this batch : 0.03773\n",
            "class proba training loss for this batch : 0.02132\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.13603\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.05199\n",
            "confidence without object training loss for this batch : 0.07714\n",
            "class proba training loss for this batch : 0.04419\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.11675\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.03062\n",
            "confidence without object training loss for this batch : 0.04582\n",
            "class proba training loss for this batch : 0.06231\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.08727\n",
            "xy_coord training loss for this batch : 0.00004\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.03553\n",
            "confidence without object training loss for this batch : 0.06989\n",
            "class proba training loss for this batch : 0.01582\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:59.223043\n",
            "Mean training loss for this epoch : 0.08019\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 10/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.10570\n",
            "xy_coord training loss for this batch : 0.00021\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.03178\n",
            "confidence without object training loss for this batch : 0.06125\n",
            "class proba training loss for this batch : 0.04160\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.06053\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.01647\n",
            "confidence without object training loss for this batch : 0.03675\n",
            "class proba training loss for this batch : 0.02472\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.05142\n",
            "xy_coord training loss for this batch : 0.00015\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.01232\n",
            "confidence without object training loss for this batch : 0.02203\n",
            "class proba training loss for this batch : 0.02656\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.14592\n",
            "xy_coord training loss for this batch : 0.00020\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.04142\n",
            "confidence without object training loss for this batch : 0.07727\n",
            "class proba training loss for this batch : 0.06407\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.07642\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02089\n",
            "confidence without object training loss for this batch : 0.03983\n",
            "class proba training loss for this batch : 0.03486\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.07278\n",
            "xy_coord training loss for this batch : 0.00012\n",
            "wh_sizes training loss for this batch : 0.00020\n",
            "confidence with object training loss for this batch : 0.02589\n",
            "confidence without object training loss for this batch : 0.04409\n",
            "class proba training loss for this batch : 0.02326\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.06789\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.02204\n",
            "confidence without object training loss for this batch : 0.03870\n",
            "class proba training loss for this batch : 0.02569\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.07532\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.02212\n",
            "confidence without object training loss for this batch : 0.03580\n",
            "class proba training loss for this batch : 0.03447\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.05659\n",
            "xy_coord training loss for this batch : 0.00014\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.01872\n",
            "confidence without object training loss for this batch : 0.02611\n",
            "class proba training loss for this batch : 0.02362\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.08370\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02106\n",
            "confidence without object training loss for this batch : 0.03717\n",
            "class proba training loss for this batch : 0.04328\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.08118\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.00537\n",
            "confidence without object training loss for this batch : 0.00688\n",
            "class proba training loss for this batch : 0.07170\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:01.382402\n",
            "Mean training loss for this epoch : 0.07840\n"
          ]
        }
      ],
      "source": [
        "delta_time = datetime.timedelta(hours=1)\n",
        "timezone = datetime.timezone(offset=delta_time)\n",
        "\n",
        "t = datetime.datetime.now(tz=timezone)\n",
        "str_t = '{:%Y-%m-%d %H:%M:%S}'.format(t)\n",
        "print(f\"[START] : {str_t} :\")\n",
        "print(f\"[Training on] : {str(device).upper()}\")\n",
        "\n",
        "EPOCHS = 10\n",
        "size_grid = 6\n",
        "batch_loss_list = []\n",
        "model_MNIST = model_MNIST.to(device)\n",
        "check = []\n",
        "\n",
        "for epoch in range(EPOCHS) : \n",
        "    begin_time = timer()\n",
        "    epochs_loss = 0.\n",
        "    \n",
        "    print(\"-\"*20)\n",
        "    str_t = '{:%Y-%m-%d %H:%M:%S}'.format(t)\n",
        "    print(\" \"*5 + f\"{str_t} : EPOCH {epoch+1}/{EPOCHS}\")\n",
        "    print(\"-\"*20)\n",
        "\n",
        "    model_MNIST.train()\n",
        "    for batch, (img, labels, bbox_true) in enumerate(training_dataset):\n",
        "        loss = 0\n",
        "        begin_batch_time = timer()\n",
        "        img, labels, bbox_true = img.to(device), labels.to(device), bbox_true.to(device)\n",
        "        \n",
        "        ### turn bbox into NxSxSx5 tensor\n",
        "        bbox_true_6x6 = bbox2Tensor(bbox_true).to(device)\n",
        "        \n",
        "        ### clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        ### compute predictions\n",
        "        bbox_preds, label_preds = model_MNIST(img)\n",
        "        \n",
        "        ### compute losses over each grid cell for each image in the batch\n",
        "        check_xy, losses, loss = loss_yolo(bbox_preds, bbox_true_6x6, label_preds, labels)\n",
        "        check.append(check_xy)\n",
        "    \n",
        "        ### compute gradients\n",
        "        loss.backward()\n",
        "        \n",
        "        ### Weight updates\n",
        "        optimizer.step()\n",
        "        \n",
        "        ######### print part #######################\n",
        "        current_loss = loss.item()\n",
        "        batch_loss_list.append(current_loss)\n",
        "        epochs_loss = epochs_loss + current_loss\n",
        "\n",
        "        if batch+1 <= len_training_ds//BATCH_SIZE:\n",
        "            current_training_sample = (batch+1)*BATCH_SIZE\n",
        "        else:\n",
        "            current_training_sample = (batch)*BATCH_SIZE + len_training_ds%BATCH_SIZE\n",
        "        \n",
        "        if (batch) == 0 or (batch+1)%100 == 0 or batch == len_training_ds//BATCH_SIZE:\n",
        "            print(f\" --- Image : {current_training_sample}/{len_training_ds}\",\\\n",
        "                    f\" : loss = {current_loss:.5f}\")\n",
        "            print(f\"xy_coord training loss for this batch : {torch.sum(losses['loss_xy']) / len(img):.5f}\")\n",
        "            print(f\"wh_sizes training loss for this batch : {torch.sum(losses['loss_wh']) / len(img):.5f}\")\n",
        "            print(f\"confidence with object training loss for this batch : {torch.sum(losses['loss_conf_obj']) / len(img):.5f}\")\n",
        "            print(f\"confidence without object training loss for this batch : {torch.sum(losses['loss_conf_noobj']) / len(img):.5f}\")\n",
        "            print(f\"class proba training loss for this batch : {torch.sum(losses['loss_class']) / len(img):.5f}\")\n",
        "            print('\\n')\n",
        "            if batch == (len_training_ds//BATCH_SIZE):\n",
        "                print(f\"Total elapsed time for training : {datetime.timedelta(seconds=timer()-begin_time)}\")\n",
        "                print(f\"Mean training loss for this epoch : {epochs_loss / len(training_dataset):.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS0A-3zLweFR",
        "outputId": "4b665641-a6c6-4245-99f4-83e59f174f69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.save(model_MNIST.state_dict(), \"yolo_mnist_model_Xepochs.pt\")\n",
        "model_MNIST.load_state_dict(torch.load(\"../yolo_mnist_model_Xepochs.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "id": "7O8u0S3PweFR"
      },
      "outputs": [],
      "source": [
        "def relative2absolute(bbox_relative:torch.Tensor, SIZEHW=75, S=6)->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Turns bounding box relative to cell coordinates into absolute coordinates \n",
        "    (pixels). Used to calculate IoU. \n",
        "\n",
        "    Args:\n",
        "        bbox_relative : torch.Tensor of shape (N, 4)\n",
        "            Bounding box coordinates to convert.\n",
        "    Return:\n",
        "        bbox_absolute : torch.Tensor of shape (N, 4)\n",
        "    \"\"\"\n",
        "    assert len(bbox_relative.shape)==2, \"Bbox should be of size (N,4) or (N,5).\"\n",
        "    assert bbox_relative.max() <= 1., \"Bbox input should be relative to a grid cell, not absolute to the image.\"\n",
        "    \n",
        "    CELL_SIZE = SIZEHW/S\n",
        "\n",
        "    cx_rcell, cy_rcell, rw, rh = bbox_relative[:,:4].permute(1,0)\n",
        "    \n",
        "    ### xc,yc centers relative to the frame coordinates\n",
        "    cx = cx_rcell * CELL_SIZE - (1/CELL_SIZE) * (cx_rcell/CELL_SIZE).to(torch.int32)\n",
        "    cy = cy_rcell * CELL_SIZE - (1/CELL_SIZE) * (cy_rcell/CELL_SIZE).to(torch.int32)\n",
        "\n",
        "    ### xc,yc centers absolute coordinates\n",
        "    cx_abs = SIZEHW * cx\n",
        "    cy_abs = SIZEHW * cy\n",
        "\n",
        "    ### x,y absolute positions \n",
        "    x_min = cx_abs - (SIZEHW * (rw/2))\n",
        "    y_min = cy_abs - (SIZEHW * (rh/2))\n",
        "    x_max = cx_abs + (SIZEHW * (rw/2))\n",
        "    y_max = cy_abs + (SIZEHW * (rh/2))\n",
        "\n",
        "    bbox_absolute = torch.stack((x_min, y_min, x_max, y_max), dim=-1)\n",
        "    return bbox_absolute\n",
        "\n",
        "def intersection_over_union(bboxes1:torch.Tensor, bboxes2:torch.Tensor)->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Intersection over Union method.\n",
        "\n",
        "    Args:\n",
        "        bboxes1 : torch.Tensor of shape (N, 5)\n",
        "            Bounding boxes of a batch, in a given cell.\n",
        "        bboxes2 : torch.Tensor of shape (N, 5)\n",
        "            Bounding boxes of a batch, in a given cell.\n",
        "\n",
        "    Return:\n",
        "        iou : torch.Tensor of shape (N,)\n",
        "            Batch of floats between 0 and 1 where 1 is a perfect overlap.\n",
        "    \"\"\"\n",
        "    assert bboxes1.shape[-1] >= 4 and bboxes2.shape[-1] >= 4, \"All bbox should be of shape (N,4) or (N,5).\"\n",
        "\n",
        "    ### Convert cell reltative coordinates to absolute coordinates\n",
        "    bboxes1 = relative2absolute(bboxes1)\n",
        "    bboxes2 = relative2absolute(bboxes2)   \n",
        "    xmin1, ymin1, xmax1, ymax1 = bboxes1.permute(1,0)\n",
        "    xmin2, ymin2, xmax2, ymax2 = bboxes2.permute(1,0)\n",
        "\n",
        "    ### There is no object if all coordinates are zero\n",
        "    isObject = xmin2 + ymin2 + xmax2 + ymax2\n",
        "    isObject = isObject.to(torch.bool)\n",
        "\n",
        "    smoothing_factor = 1e-10\n",
        "\n",
        "    ### x, y overlaps btw pred and groundtrue\n",
        "    xmin_overlap = torch.maximum(xmin1, xmin2)\n",
        "    xmax_overlap = torch.minimum(xmax1, xmax2)\n",
        "    ymin_overlap = torch.maximum(ymin1, ymin2)\n",
        "    ymax_overlap = torch.minimum(ymax1, ymax2)\n",
        "    \n",
        "    ### Pred and groundtrue areas\n",
        "    box1_area = (xmax1 - xmin1) * (ymax1 - ymin1)\n",
        "    box2_area = (xmax2 - xmin2) * (ymax2 - ymin2)\n",
        "\n",
        "    ### Compute intersection area, union area and IoU\n",
        "    overlap_area = torch.maximum((xmax_overlap - xmin_overlap), torch.Tensor([0]).to(device)) * torch.maximum((ymax_overlap - ymin_overlap), torch.Tensor([0]).to(device))\n",
        "    union_area = (box1_area + box2_area) - overlap_area\n",
        "    iou = (overlap_area + smoothing_factor) / (union_area + smoothing_factor)\n",
        "    \n",
        "    ### Set IoU to zero when there is no coordinates (i.e. no object)\n",
        "    iou = iou * isObject\n",
        "\n",
        "    return iou   \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def non_max_suppression(bbox, labels, iou_threshold):\n",
        "    \"\"\"\n",
        "    - Get the highest pc number\n",
        "    - Discard all other bbox with a HIGH IOU (bc if the iou is low, it means  this pc number doesn't stand for the current object)\n",
        "\n",
        "    Args :\n",
        "        bbox : torch.Tensor of shape (N, S, S, 5)\n",
        "            Predicted bounding boxes with x, y, w, h and confident pc number\n",
        "        labels: torch.Tensor of shape (N, S, S, 10)\n",
        "            Predicted label\n",
        "        iou_threshold : float\n",
        "            iou from which the bboxes with low pc will be discard\n",
        "        \n",
        "    \"\"\"\n",
        "    def find_indices_max(tens):\n",
        "        \"\"\"\n",
        "        input tens : torch.Tensor of shape (N, S, S)\n",
        "        output batch_indices : torch.Tensor of shape (N,2)\n",
        "        \"\"\"\n",
        "        S = tens.shape[1]\n",
        "\n",
        "        # Reshape to (N, S*S)\n",
        "        tens_reshape = torch.reshape(tens, (tens.shape[0], -1))\n",
        "        indices = torch.argmax(tens_reshape, dim=1)\n",
        "\n",
        "        col_indices = (indices / S).to(torch.int32)\n",
        "        row_indices = indices % S\n",
        "\n",
        "        batch_indices = torch.stack((col_indices, row_indices)).T\n",
        "        return batch_indices\n",
        "    \n",
        "    assert 0. < iou_threshold < 1, \"iou_threshold should be a float between greeter than 0 and lower than 1.\"\n",
        "\n",
        "    N = len(bbox)\n",
        "    S = bbox.shape[1]\n",
        "    labels_prob = torch.softmax(labels, dim=-1)\n",
        "    bbox[:,:,:,4] = bbox[:,:,:,4] * torch.max(labels_prob, dim=-1)[0]\n",
        "\n",
        "    bbox_maxconf = torch.zeros(N,4)\n",
        "    \n",
        "    ### 1) finding indices i,j of the max confidence number of each image \n",
        "    ### in the batch\n",
        "    m = find_indices_max(bbox)\n",
        "    return m, bbox\n",
        "\n",
        "    ### Getting bboxes with the highest pc number for each image\n",
        "    ### Shape : (N, 4)\n",
        "    bbox_maxconf = bbox[range(N), m[:,0], m[:,1], :4]\n",
        "\n",
        "    ### Removing bboxes with the highest pc numbers\n",
        "    bbox[range(N), m[:,0], m[:,1]] = torch.Tensor([0])\n",
        "    for cell_i in range(S):\n",
        "        for cell_j in range(S):\n",
        "            iou = intersection_over_union(bbox[:,cell_i, cell_j], bbox_maxconf)\n",
        "            iou_bool = iou >= iou_threshold\n",
        "            \n",
        "            ### iou to shape (N,4)\n",
        "            iou_bool = iou_bool.unsqueeze(1).repeat((1, bbox.shape[-1]))\n",
        "\n",
        "            bbox[:,cell_i, cell_j] = bbox[:,cell_i, cell_j].masked_fill(iou_bool, 0)\n",
        "\n",
        "\n",
        "    return bbox\n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "DLyoeZO4QYHB"
      },
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_test = torch.rand(64,6,6,5)\n",
        "b_test = torch.rand(64,6,6,10)\n",
        "m, bbox = non_max_suppression(a_test, b_test, 0.7)"
      ],
      "metadata": {
        "id": "etfARfnjZGST"
      },
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "S = bbox.shape[1]\n",
        "\n",
        "# Reshape to (N, S*S)\n",
        "tens_reshape = torch.reshape(tens, (tens.shape[0], -1))\n",
        "indices = torch.argmax(tens_reshape, dim=1)\n",
        "\n",
        "col_indices = (indices / S).to(torch.int32)\n",
        "row_indices = indices % S\n",
        "\n",
        "batch_indices = torch.stack((col_indices, row_indices)).T"
      ],
      "metadata": {
        "id": "5fClilX0ZIFr",
        "outputId": "d295f50a-4906-4f5d-a737-825d3aef4f30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 6, 6, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 283
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bbox.shape[0]"
      ],
      "metadata": {
        "id": "JX0uHev8bOsr",
        "outputId": "c679feff-f044-400a-df51-56696f42ab6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 289
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "row_indices"
      ],
      "metadata": {
        "id": "w-Q0Mms4brOc",
        "outputId": "ea276032-5821-4f7d-ddeb-dbe67275fd61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 306,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 5,  3,  6, 25, 28, 26, 28, 17, 23, 23, 10, 16, 21,  3, 25,  8, 26,  8,\n",
              "         3, 12, 27, 22, 27,  6, 13, 10,  2, 22,  6, 23, 13, 16,  3, 16,  2, 27,\n",
              "        28, 21,  8, 20,  8, 16, 27, 27, 20, 16,  6, 28, 18, 16, 26, 13, 18, 28,\n",
              "        13,  2,  8, 12,  8, 11, 11, 10,  5,  8])"
            ]
          },
          "metadata": {},
          "execution_count": 306
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWc1Cr8yrSH8",
        "outputId": "bd79409e-b55c-42a0-fa11-f609b48225cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE BOX : 0.00018\n",
            "MSE confidence score : 0.21671\n",
            "class acc : 98.98%\n"
          ]
        }
      ],
      "source": [
        "S=6\n",
        "for (img, labels, bbox_true) in validation_dataset:\n",
        "    img, labels, bbox_true = img.to(device), labels.to(device), bbox_true.to(device)\n",
        "    model_MNIST.eval()\n",
        "    with torch.no_grad():\n",
        "        ### prediction\n",
        "        bbox_preds, label_preds = model_MNIST(img)\n",
        "\n",
        "        ### (N,4) -> (N, S, S, 5)\n",
        "        bbox_true_6x6 = bbox2Tensor(bbox_true).to(device)\n",
        "\n",
        "        ### keeping only cells (i,j) with an object \n",
        "        cells_with_obj = bbox_true_6x6.nonzero()[::5]\n",
        "        N, cells_i, cells_j, _ = cells_with_obj.permute(1,0)\n",
        "\n",
        "        ### MSE along bbox coordinates and sizes in the cells containing an object\n",
        "        mse_box = (1/len(img)) * torch.sum(torch.pow(bbox_true - bbox_preds[N, cells_i, cells_j,:4],2))\n",
        "        \n",
        "        ### confidence score accuracy : sum of the all grid confidence scores\n",
        "        ### pred confidence score is confidence score times IoU.\n",
        "        mse_confidence_score = torch.zeros(len(img))\n",
        "        for i in range(S):\n",
        "            for j in range(S):\n",
        "                iou = intersection_over_union(bbox_true_6x6[:,i,j], bbox_preds[:,i,j])\n",
        "                mse_confidence_score += torch.pow(bbox_true_6x6[:,i,j,-1] - bbox_preds[:,i,j,-1] * iou,2)\n",
        "        \n",
        "        mse_confidence_score = (1/(len(img))) * torch.sum(mse_confidence_score)\n",
        "\n",
        "        ### applied softmax to class predictions and compute accuracy\n",
        "        softmax_pred_classes = torch.softmax(label_preds[N, cells_i, cells_j], dim=1)\n",
        "        classes_acc = (1/len(img)) * torch.sum(torch.argmax(labels, dim=1) == torch.argmax(softmax_pred_classes, dim=1))\n",
        "\n",
        "print(f\"MSE BOX : {mse_box.item():.5f}\")\n",
        "print(f\"MSE confidence score : {mse_confidence_score.item():.5f}\")\n",
        "print(f\"class acc : {classes_acc.item()*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAUtKFv24Rhq"
      },
      "outputs": [],
      "source": [
        "############### Matplotlib config\n",
        "plt.rc('image', cmap='gray')\n",
        "plt.rc('grid', linewidth=0)\n",
        "plt.rc('xtick', top=False, bottom=False, labelsize='large')\n",
        "plt.rc('ytick', left=False, right=False, labelsize='large')\n",
        "plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')\n",
        "plt.rc('text', color='a8151a')\n",
        "plt.rc('figure', facecolor='F0F0F0')# Matplotlib fonts\n",
        "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), \"mpl-data/fonts/ttf\")\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZjSvwrpPt3C"
      },
      "outputs": [],
      "source": [
        "def draw_ONE_bounding_box_on_image(image, ymin:int, xmin:int, ymax:int, xmax:int, \n",
        "                               color:str='red', thickness:int=1, display_str:bool=None):\n",
        "  \"\"\"Adds a bounding box to an image.\n",
        "  Bounding box coordinates can be specified in either absolute (pixel) or\n",
        "  normalized coordinates by setting the use_normalized_coordinates argument.\n",
        "  \n",
        "  Args:\n",
        "    image: a PIL.Image object.\n",
        "    ymin: ymin of bounding box.\n",
        "    xmin: xmin of bounding box.\n",
        "    ymax: ymax of bounding box.\n",
        "    xmax: xmax of bounding box.\n",
        "    color: color to draw bounding box. Default is red.\n",
        "    thickness: line thickness. Default value is 4.\n",
        "    display_str_list: string to display in box\n",
        "    use_normalized_coordinates: If True (default), treat coordinates\n",
        "      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n",
        "      coordinates as absolute.\n",
        "  \"\"\"\n",
        "  draw = PIL.ImageDraw.Draw(image)\n",
        "  im_width, im_height = image.size\n",
        "  \n",
        "  left, right, top, bottom = xmin, xmax, ymin, ymax\n",
        "  \n",
        "  draw.line([(left, top), (left, bottom), (right, bottom), (right, top), (left, top)], width=thickness, fill=color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTE3VutBPt3D"
      },
      "outputs": [],
      "source": [
        "def draw_bounding_boxes_on_image(image, boxes_dict:dict, color_list:list=[], \n",
        "                                 thickness:int=1, display_str_list:tuple=()):\n",
        "  \"\"\"Draws bounding boxes on image.\n",
        "\n",
        "  Args:\n",
        "    image: PIL.Image.\n",
        "    boxes: numpy array of shape (N,4)\n",
        "      Contains (ymin, xmin, ymax, xmax). The coordinates are absolute.\n",
        "    color: list, default is empty\n",
        "      Color to draw bounding box.\n",
        "    thickness: int, default value is 4\n",
        "      Line thickness.\n",
        "    display_str_list: tuple\n",
        "      A list of strings for each bounding box.\n",
        "                           \n",
        "  Raises:\n",
        "    ValueError: if boxes is not a [N, 4] array\n",
        "  \"\"\"\n",
        "  for key in boxes_dict.keys():\n",
        "    if key == \"true_bbox\":\n",
        "      color = color_list[0]\n",
        "      thickness = 2\n",
        "    else : \n",
        "      color = color_list[1]\n",
        "      thickness = 1\n",
        "\n",
        "    boxes = np.asarray(boxes_dict[key])\n",
        "    boxes_shape = boxes.shape\n",
        "    if not boxes_shape:\n",
        "      return\n",
        "    if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n",
        "      raise ValueError('Input must be of size [N, 4]')\n",
        "    \n",
        "\n",
        "    for i in range(boxes_shape[0]):\n",
        "      draw_ONE_bounding_box_on_image(image, \n",
        "                                    # boxes[i, 1], boxes[i, 0], \n",
        "                                    # boxes[i, 3], boxes[i, 2], \n",
        "                                    boxes[i, 1], boxes[i, 0], \n",
        "                                    boxes[i, 3], boxes[i, 2], \n",
        "                                    color=color, thickness=thickness)\n",
        "                                    #, thickness, display_str_list[i])\n",
        "    \n",
        "                              "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMV5UzxQPt3D"
      },
      "outputs": [],
      "source": [
        "def draw_bounding_boxes_on_image_array(image:np.ndarray, boxes:dict, color:list=[], \n",
        "                                       thickness:int=1, display_str_list:tuple=()):\n",
        "  \"\"\"Draws bounding boxes on image (numpy array).\n",
        "\n",
        "  Args:\n",
        "    image: a numpy array object.\n",
        "    ####boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
        "           The coordinates are in normalized format between [0, 1].######\n",
        "    color: color to draw bounding box. Default is red.\n",
        "    thickness: line thickness. Default value is 4.\n",
        "    display_str_list_list: a list of strings for each bounding box.\n",
        "  Raises:\n",
        "    ValueError: if boxes is not a [N, 4] array\n",
        "  \"\"\"\n",
        "  image_pil = PIL.Image.fromarray(image)\n",
        "  rgbimg = PIL.Image.new(\"RGBA\", image_pil.size)\n",
        "  rgbimg.paste(image_pil)\n",
        "  draw_bounding_boxes_on_image(rgbimg, boxes, color, thickness, display_str_list)\n",
        "  return np.array(rgbimg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GOOWyP5Pt3E"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "def display_digits_with_boxes(digits, predictions, labels, pred_bboxes, bboxes, title, nb_sample=10):\n",
        "  \"\"\"Utility to display a row of digits with their predictions.\n",
        "\n",
        "  Args:\n",
        "    digits : np.ndarray of shape (N,75,75,1)\n",
        "        Raw image with normalized pixel values (from 0 to 1)\n",
        "    predictions : np.ndarray of shape (N,)\n",
        "        Predicted label with the same shape as labels\n",
        "    labels : np.ndarray of shape (N,)\n",
        "        Labels of the digits (from 0 to 9)\n",
        "    pred_bboxes : np.ndarray of shape (N, 4) ??\n",
        "        Predicted bboxes locations\n",
        "    bboxes : np.ndarray of shape (N, 4)\n",
        "        Ground true bboxe locations\n",
        "    iou : list of shape (N,)\n",
        "        IoU of each bboxes\n",
        "    title : str\n",
        "        Figure's title\n",
        "  \"\"\"\n",
        "  iou_threshold = 0.6\n",
        "  nb_sample = 10\n",
        "  indexes = np.random.choice(len(predictions), size=nb_sample)\n",
        "  \n",
        "  n_digits = digits[indexes].numpy()\n",
        "  # Rescale pixel values to un-normed values (from 0 -black- to 255 -white-)\n",
        "  n_digits = n_digits * 255.0\n",
        "  n_digits = n_digits.reshape(nb_sample, 75, 75)\n",
        "  \n",
        "  n_predictions = predictions[indexes]\n",
        "  # Argmax of one-hot vectors. Shape : (N,S,S,10) -> (N,S,S)\n",
        "  n_predictions = torch.argmax(torch.softmax(n_predictions, dim=-1), dim=-1).numpy()\n",
        "  \n",
        "  ### shape : (N, S, S, 5)\n",
        "  n_pred_bboxes = pred_bboxes[indexes]\n",
        "\n",
        "  ### shape : (N, 4)\n",
        "  n_bboxes_rel = bboxes[indexes]\n",
        "  n_bboxes = relative2absolute(torch.as_tensor(n_bboxes_rel)).numpy()\n",
        "  # n_bboxes = n_bboxes_rel/75\n",
        "\n",
        "  # Set plot config\n",
        "  fig = plt.figure(figsize=(20, 4))\n",
        "  plt.title(title)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  \n",
        "  bboxes_to_plot = {\"true_bbox\":[], \"pred_bbox\":[]}\n",
        "  for i in range(nb_sample):\n",
        "    bboxes_to_plot[\"pred_bbox\"] = []\n",
        "    bboxes_to_plot[\"true_bbox\"] = []\n",
        "    \n",
        "    for cell_i in range(6):\n",
        "      for cell_j in range(6):\n",
        "        n_pred_bboxes_ij = n_pred_bboxes[:, cell_i, cell_j, :4]\n",
        "        \n",
        "        # n_iou = intersection_over_union(n_pred_bboxes_ij, n_bboxes_rel)\n",
        "        \n",
        "        # n_predictions_ij = n_predictions[:, cell_i, cell_j]\n",
        "\n",
        "        n_pred_bboxes_ij = relative2absolute(n_pred_bboxes_ij).numpy()\n",
        "        \n",
        "        bboxes_to_plot[\"pred_bbox\"].append(n_pred_bboxes_ij[i])\n",
        "    \n",
        "    bboxes_to_plot[\"true_bbox\"].append(n_bboxes[i])\n",
        "    \n",
        "    ax = fig.add_subplot(1, nb_sample, i+1)\n",
        "    img_to_draw = draw_bounding_boxes_on_image_array(image=n_digits[i], boxes = bboxes_to_plot, color=[\"white\", \"red\"])#, display_str_list=[\"true\", \"pred\"])\n",
        "    # img_to_draw = draw_bounding_boxes_on_image_array(image=n_digits[i], boxes = np.asarray(bboxes_to_plot[\"pred_bbox\"]), color=\"red\")#, display_str_list=[\"true\", \"pred\"])\n",
        "\n",
        "# plt.xlabel(n_predictions[i])\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "# if n_predictions[i] != n_labels[i]:\n",
        "#   ax.xaxis.label.set_color('red')\n",
        "\n",
        "    plt.imshow(img_to_draw)\n",
        " \n",
        "# if len(iou) > i :\n",
        "# color = \"black\"\n",
        "# if (n_iou[i] < iou_threshold):\n",
        "#   color = \"red\"\n",
        "# ax.text(0.2, -0.3, \"iou: %s\" %(n_iou[i]), color=color, transform=ax.transAxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ouE3mxQPt3E",
        "outputId": "82f85c80-d9c8-4ffd-b651-e8c568936d59"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABGoAAAD3CAYAAABFALKIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAACTX0lEQVR4nO3dd3gc1dn38e+suixLsmXLRe69YWwM2AbTQnNs00IxEBIc4EkB8lACPCSEAAklARJCAgTy0kOJE0jANNNNscGYYox7wVUukqssq2vn/ePsaGdXW6WVtLJ+n+vyJWt39sxoZ+bMmXvOuY9l2zYiIiIiIiIiItL2PG29ASIiIiIiIiIiYihQIyIiIiIiIiKSJBSoERERERERERFJEgrUiIiIiIiIiIgkCQVqRERERERERESShAI1IiIiIiIiIiJJQoEaEREROSjNLiwqn11YNKitt0NEREQkHpZt2229DSIiItLBzC4ssoGhM0uK17peuxUYMrOk+KImlDcPeGZmSfGjYd5/Etgys6T4103aYBEREZFWoh41IiIi0uHNLixKbettEBEREQFQo0RERESSzuzCouOBZ4D7gP8D6oFfzSwpfiLEsncAxwCTZhcW/Rl4cmZJ8ZVOrx3gO8D3AXt2YdHVwPszS4pPm11YtAH4m++94bMLizoBhwN/AkYBG4GrZpYUz2u5v1REREQkkHrUiIiISLLqCeQBRcClwIOzC4u6BC80s6T4JuAj4MqZJcU5M0uKrwx6/+/As8DdvvdPc719ATAdyAd6AK8BtwNdgeuAF2cXFnVP9B8mIiIiEo4CNSIiIpKsaoHfziwprp1ZUvw6UA4MT/A6/jKzpHjzzJLiSuAi4PWZJcWvzywp9s4sKX4b+ByYluB1ioiIiISloU8iIiLSFuqBtKDX0jDBGceumSXFda7fK4CcBG/HZtf/+wPnzi4scve4SQPeT/A6RURERMJSoEZERETawiZgALDC9dpAYHUTy4s2jWW4992vbwb+MbOk+H+auA0iIiIizaZAjYiIiLSF2cCvZxcWfQNsxST8PQ2Y3MTydgCDmvE+mOTFi2YXFp0KvIPpTTMJWDuzpHhLE7dLREREJC7KUSMiIiJt4bfAAuBjYA9wN/D9mSXFS5tY3v3AObMLi/bMLiz6S4j3HwNGzS4s2ju7sOilUAXMLCneDJwB/AooxfSwuR61l0RERKQVWbYdraewiIiIiIiIiIi0Bj0hEhERERERERFJEgrUiIiIiIiIiIgkCQVqRERERERERESShAI1IiIiIiIiIiJJIuL03LbRWtsiIiIiIiIiInLQ83g8O4Huod6LFqihtra2RTZKRERERERERKQjysjI2BjuPQ19EhERERERERFJEgrUiIiIiIiIiIgkCQVqRERERERERESShAI1IiIiIiIiIiJJQoEaEREREREREZEkoUCNiIiIiIiIiEiSUKBGRERERERERCRJKFAjIiIiIiIiIpIkFKgREREREREREUkSCtSIiIiIiIiIiCQJBWpERERERERERJKEAjUiIiIiIiIiIklCgRoRERERERERkSShQI2IiIiIiIiISJJQoEZEREREREREJEkoUCMiIiIiIiIikiQUqBERERERERERSRIK1IiIiIiIiIiIJAkFakREREREREREkoQCNSIiIiIiIiIiSUKBGhERERERERGRJKFAjYiIiIiIiIhIklCgRkREREREREQkSShQIyIiIiIiIiKSJBSoERERERERERFJEgrUiIiIiIiIiIgkCQVqRERERERERESShAI1IiIiIiIiIiJJQoEaEREREREREZEkoUCNiIiIiIiIiEiSUKBGRERERERERCRJKFAjIiIiIiIiIpIkFKgREREREREREUkSCtSIiIiIiIiIiCQJBWpERERERERERJKEAjUiIiIiIiIiIklCgRoRERERERERkSShQI2IiIiIiIiISJJQoEZEREREREREJEkoUCMiIiIiIiIikiQUqBERERERERERSRIK1IiIiIiIiIiIJAkFakREREREREREkoQCNSIiIiIiIiIiSUKBGhERERERERGRJKFAjYiIiIiIiIhIklCgRkREREREREQkSShQIyIiIiIiIiKSJBSoERERERERERFJEgrUiIiIiIiIiIgkCQVqRERERERERESShAI1IiIiIiIiIiJJQoEaEREREREREZEkoUCNiIiIiIiIiEiSUKBGRERERERERCRJKFAjIiIiIiIiIpIkFKgREREREREREUkSCtSIiIiIiIiIiCQJBWpERERERERERJKEAjUiIiIiIiIiIknCsm07/LulpfaKjRs50MIbUQAUAUsSVF4aMBZYC9QlqMxY9Qf2AcXNLGfChAnU1NQkYIugsqyMbzduxNuEz44GdgI7ErIlofXEHAPLWnAdbaF///7k5uYmpKyMsjJa41wE//mzBKhtRjkFQB/MedhasoB+wFdAhJotLm11LvYC8hKy1tjtAkqDXsvzbUtb2QhUNrOMRJ6LzalPw2mLOjAPGAgsbsV1NleyXBel6ZL9XGyqPkD3rCwq+/dvs23I2LYNq76eqj59Gl4rKSlh9+7dCV+XzsXYZGHuC5JJCbCbg/dcbK6UlBTGjRuHZVmUlJSwa9euNtuWIUOGkJaWxt69e1m7NnRruj2di+OAbUB5C5UfjQUMA1ZBq9xPxap///50797dCvVeasRPbtzIJUccwactsll+s4DbgSMSVF5vYAtwPLA1QWXGagHwIXBjM8upr69PwNYYazdu5IgjmvbtLgceB+5N2NY0dgNwMYnb/8li0aJFHHLIIYkprJXORfCfP9No3vkzC3Nej0rANsVqMvA+cBSQmMtW252LTwBTgBcTtvbILgL+SeO6axbwF+ChVtoORypwNXAZNPu4T+S52Jz6NJy2qANPA54GjiRxQc2WlizXRWm6ZD8Xm+pu4PrJk8lZsKDtNuKOO2D3bnLmzGl46bbbbuPeexPfgtO5GJvJwIKUFLj6akhJaevNgUcf5YOxY3m+qIirr776oDwXmysvL4/du3djWRb33HNPi5w/sdqyZQtFRUV8+OGHnHHGGSGXaS/nooUJEP4MeK1F1hBdOlAGXELz25WJtGjRIrp37x7yvciBGpEY9QVymvjZQszJM7IJn60ENjRxvSLJbjnND/rG6tgI75W14nY40oErW3mdIiKJsH79eiorm9sXMD699+4lZf9+ileuZPjw4VhWyAe00tpSUkwQLSOjrbcE5sxh+rRpdDn66LbeklYzANOzKVa5Xi8sXw4eDz127WrSvUmipK1ZA/v20XnzZoZjeoJIx6JAjSTEQ8B0mv5k1gKWNuEznwAd53LTctzNOY/rteY086ygn4nQXp78i4hIx3XhhRfy6aet+8z2CaAr8MNJkxp6BEhysW2biCknWlhz23Xt0XPAJOJoP+7fjzV2LNg21wLXJmAbrHjW7/7cd74DwAmYHiBdm1jOwagpx7HH9bO550Fr7QcFaiRhngWuaMLnrgIuBCbG+bnfcvANl2oLFiYPiDNS2anAVkCzxqmmAxnAnmZtnd82mtbrSkRERKSt3X///dxyyy1ttv7PyssZ3mZrbzv3A7F+63m5uWzYuBFr4kRuWb2avyRg/Z8Bz0DcZa1csYJePXvCG2/ABRckYEsODsH3LfF8Lh14E2jOgLGFwKnN+Hw8WjRQcwnwvRiW64NJpPhqgtabidkZTwNVzSjnHeDPidigDqIWM0QiXtWYgEC8n01UDhKBzphj/TPMufgUcDlmPGmwSzABk+ujlHkS8ANM7o1IRgJ3AecQPvn3ScB5UcqR5jsHGBP0WqLr50h+ismP1JHci+mNWETrfMeOHq24LonPvcCIVl7nfcC7rbzO5hpCYs+Z64CVYd5zrovhjABYsQKmTwfgjytXJuwhRSi3AYtasPxkdiTwmzbehqbeH1RXV1NW1pSWcmIcDMl+m6Ka2O8xLMuC3FzweKiK43OReKFJZXlzciAvD7KzE7AVBxf3fUus0oB/Y4J24er6aC6hdSfYaNFAzQjgcBpfSLsAZ2GiizWYiFgmprGaSCfGsewSAi96J2Myox+sLsJEFaPJw/RauSTKcn2B/BiWAzODVFslkpLw8jGzz+T7fndyBwXr5Hu9Z5Ty8oCUGJbrirmIzSV88K0QBWpaw2DM/viP67VczJOH7S243gzg+zQ9z1V7dhTmWrmHlv2Og8VS/4txDDC0Fdd3JuZcBNO92mkrtZTzMY3X9qaWxJwzKZiHCrdHWCYD00Z9HqgI8X4/gPR06GmueLvS0uhJy/X6XQsEp4Edirk+X1RTg/X442BZHPHNNwHtsldoPLNfe9MD8/DmWVpn+MFYGu/HPGK76R4M4PXCU09BSgpjP/88pnZyKGo7tz+WZVFUVETq9u0cNnYs03r25KOPPmL//v1tvWkHhUXA63Esn46531hA05MJH89BFKgBWIeZtcNtJKYh8nPMVNazMI3VbWHK6IGZRivW6bxS8H+Ju2l8Uc3D7CznYtUdeIPAZJmvxLiu9sgC/orpvRCtx1F34BTM/omkAJOsa0oM6/+cwClh8zAHYlEMn3XrjNmP7s+VY44pid/3gbMx548F/ILQT1+c8+e2KOVlY27yIy2Xh9mP1fFurLSYbQTW2bOAQTSuxxMpDzP8sSML/t5bmjPrk0R3CTATM319a+gO7Me0ewrxt5VaytQWLLslbSQx50w6EMugAhvT6ybUbIh3A4cMHgyPPQbAnZMnc2xpKeMwN9jheDwesrKyyM/PD7uM1+ulrq6uoTdGb/zXa7cumKHLv6yqgltvBeCUffsa2m9FmLZcew/UgJlI4jJaJ1BzAwTsxwJgPCZJbTTpAHV18NvfAjClvLxRgC0WecA3BAZqLMsiLS2NvLw80tPTKSsro7Kykrq6cH2Tk4uFmWW0paVj2pnB9xgVJG5ofsj1pqeTl5fHxIkTyXrrLb47dSr5U6awfPlyBWokZkmTo2Y7pldGKPFOEd0bKPb9/1oad1e9G5OA1klC24aTKbapWUSPzsf63b8CzPD9P9yF00ncdDihhzc0dciD+3N/oPVnpzlYzMIcD0XAZswTpHANUvf5E6m82wl/XjtlRRtCJSLS1t4GQk+OmngLgA8xbZd4k+xLGwtKFLsGGB1h8R7du3PGGWfw0EMPNS7Dlwx4z549rFy5kvfee49bb7mF3YRuvzUkE87NZfemTVgeD3dcfz333ntvw7S00jTu/ej0SoqlZ8xkYEF6OqxbBxkZPPSHP3DjjfG3Up12l1t+fj5jx47ltttu46ijjuLmm2/mpZdeYtWq9jE3UC6wiZZPbmwBEzDTQrtfe4LY9mFTpKenM378eH7yk5+QmZlJ508+4f0PPuBf69dTURGqT55IaEkTqJGDwzZMICaUebR89/HLMcO63P5FYrK2tycnYy5CsbAwF8ynMD2snB41XxA62ZbToyZaYC2b0F3EJT65wDJavjHT3fdzKIH7VvtRpO0kIrF7ND0xOWruCPHe55ge0BKDzz+HPn0AmFNaSgamp3Gka2VKaSmZzzyD55Xw/bi72jZH1NZyaE0NlxF4vXbrgsmdIh3DmDFjmDlzJkcccQQej4fDDjuMtWvXtptAjeN4zFC+ljIHM8zlTtdrT7bg+nr16sWMGTMoKirijTfe4J133uGTfftYtHMnL335JdXV6kMusVOgRhLKS+heGOBPFLuGwAbhNMyTgpviXNd5mJtKp6xfA6sIHGt/Nf6cKx1JJqbRdgXRuwZbwIOYYRBfY76v+4DfA3tDLB/8vYdyNaarsG7wm8/pHnwT4YeHJsKvMck5S/Cfi1ej/dhRnAd8t603IsivaNljvj2wMTMcVrbgOu4DXgDmB71+Hv4ArsSgf3+4ydSeD99+O8PXrYvetvF6oaLC/PPJyMggJyeHwsJCZsyYwYABA8jKyiIN6OT1Yv385zxdWcnXQUW15tDJ1jIFuDTotb6YBwhP0PJDnyLlLYpZXR1cdhmkpDB96dImJXI/HDMM0v0ArteaNQx+9lkyP/kEy7KYtHkzPdavb+jdHkpr5taIVQnh7xsSoQaTFsG9jpYIlXg8Hrp06cKVV17JhAkT2LhxI48++ih79uzBC2RnZZGfnc3+/fspL481mUfHdCeRj1ULUwdcg5kEI1YpmMDHzYTOQ/sx8Fgc5bUGBWqk1ZUSOBytB2amGfdrfTE3iJFUYYI/zsxETs4d90xFtb6yTotx27yYxFStMe65pdVgvtNYAjXOjB+vYYIC92ECXqEunqMxwZxIM2CcQ/T9J/F5CX+Weg/mptqTwPKdQGo9/nOo1vczk8bnUC0mAXRHk0fs9UksuiSwrOY4HDgd01Bpa2mYvCl/oOUCNfHsx76YHAfx7veFJGZSgudp2Rw1d2CCNMF1+miiD3FtS4k6F9MwdekUGgem4tqH3bvDrFkAvP7II5SvW9eobROLDCDX46Fnaip2166cc8op9OnTh7S0NPB6sa69lncrKxsNfToeM/SpvYm0H0/EDPMKJdqMkpG8ReQcjR7MQ8RHmrGOBl4vPPMMTJlCZkVFTPsoLTWVnJwc+g8YwNatW8nft4/U6uqGzxZ2707f3r3p3aUL7DGZVupKSkgtKwtZvlOntnTP3I4qJSWFnJwcjjrqKKZNm4ZlWXzzzTds2eLvT5eamkp2djbp6ekNOads+2C420i8MzH3LuF6Wlm+fznEV+d5fJ/rTONZZif61qlAjbQ7WZgLaSTOgWRFWNa5qUwJWibT9577tdMwvTzCKcMMv0nFnxAzB5PczT3bVyfMiRduOJZbiq/MXDT1tyS3VOBFzHEaanhaLDJ9/4L1w3QVdisI8do+YCBNH5JRRvsMiA4ksUl4W2umK2eIo6OT77V8zD7MwPR2/GFrbExubkP+jVDybJv1ZWXkdu5MnieR4Ui/ePZjNua7ime/OzefmqWl5STyXEzFTPnsrk/bah9WV1dTWlpKaWkpd955J8OHDyc/P58uXfxh3fS0NFJtu90kjo1kKGYGp1DXMmeWFnd+nVRM/dWU4KVTD/6IyL04WiSnz9138+KHH0bNUZOSkkLXLl045JBDuPTSS/nXv/7F+V98Qb8tWxryZE0/8kh+8IMfcM45/v4EL//lL7z44ovMnx/cN84cy7tpuUBNcLs+Fs7yuTF+9gCNb66TgWVZZGdnM2jQIK666ip69uzJ66+/zocffhiwXG1tLZZlMXz4cDIyMiguLqaiokLBmjCeIHx+VAtzPP+O+Opn57y+gcazPiXrJEIK1EhUtwG3RlnGqfx7EdijJdQyRwYt47we6rVwJmGSgB2Ff6ap+cBHBCYTngPsJLaEYUdhepWItBen0vQpBu/GzGDSVLmYc6up+tA+h7UsxtRhiTIfk3CypfXCJAl3OHXszqDfw9XfCWNZsGED5EVomu/di9WtG598+imMGNEim7GY2Pejk6Q11mTCHlpvhqiObDGJORczMMN83fVpsuzD/fv3U1JSwv79+wMCNeecey7eAwd4+eWX23DrEifctWwW5mbMPSnBaZheSgXEH+wvwiSwTWZjx47ljDPO4JprriE7O5uCggIGPPQQu1y9MyZPnsyIoLpxx44d7NzZnKty042jadcOC/gkxmVPJzkD3126dGHs2LFMnz6dE044gRdeeIHHH3+8UcDsJz/9KT+84gqKiorYuHEjt912Gx988AEbN25soy2X9kCBGoloOr7pBUMYjIlATsTc9J2A6SJ8XJjlX/N9ZjHRp8L8HnBViLI6Y7oi2/gv0O4LtU3jC3eo10JpyWSNIi0h1mM73Ge/wn8u/hPT2FqLfwa3v2HO6x0EnosnYIalOF1F49EDk1i8PXfBbs7zr+sIzLnQ3/dzMCZpbEtJxd+r8UbM02j3PrweOAzX8fD884wbNy6xG7FuHZx2mgnWROop43vPClrO601sLR3rfrSJ71zzBi1bCHwQx3b1B4YB52LOk89o/vVpHUTMXdFeJeJZtPPduvdx8D5sSw888AClpaVcdNFF9C0ykwz379ePorKDZx6ncOdXuHZepM9EkuztvM6dO3PppZdy3nnnkZ2djWVZdO7cmcyMDMDUiUceeSTHHnssgwYNavjchg0b2LBhA7t2tU14cRnx5QoBf3t+BvBthOU8vuXaUhomsXqo+6GUvXvJ+OwzcpYtw3r0Uabu38/EPXsC8ooNBFIefZT8F1/Ek5pK39pafr9zJxUVFQ1DzEMpPO44SE2F8nI6Y2binU7k70sOLgrUSETrI7znNJ/XYLokgumWGC7fvDOcqDLCMo5tYcqKt2uliITnPhedRkWN67VQ5/VM4BRMg+Uy4m8od8bcfN6MSfAHZgiWhemO6jQzU33/fg6c7XvNxgxPiJRbINkVYv7eB3y/X4kZsrkXeLQF1ncJMCroteMw36F7H471bZuTkLRwzhwzi02CvA8s2rWL62yb39x8M1W+G49QMquq+J1tc8/dd3PkxRdz/PHHJ2w72kIqMBzTMyCWW+srMdfeRZjA3tM075g/HhgUbSFJWt9++y3ffvstu3btagjUpKWnm5w1clA57rjjGDVqFAUFBQ1DYjwejwla+/4/Y8YM+vbtS2amGbxs2zYvv/wyq1atYv/+/W2y3VVEb9eDue47Q3Cd6/65RO69ZmGuVZdi6rJ49cc8XLoeuKcJn3e2YTjwdyC4/8vggQMZOXIkw4cP5+O1a3nzzTepqqoiIyODHj16cNppp2E99xz7R46katIkevbsSX11NV/Pm8eKFSsoLi4Ou96bL7yQvNxcWL6clCeeYAThH57LwUmBmg4gh8CEgBZmx48i9Kw+sRrg+zmR5EmIKXIwG4Dpuu0kvxyLGRveFEWYoIlTN3T2/cxyveac1+mu1y7GJDKuwwxBjDdQ4zQyDsMfvHUuRIfin2HKSfo2GpM3JxvT4+cO2negBkxuhD/6/n82Zr/ucr2WSMdjhghsxD9MZDBm/6Xi34e9MMfAJN8yOcuWwaZNVFRU8OVXXzVrG8ZieoU8hQk8PPDAAxHzS+RhZjp64sknsUaNaveBGscjxDa7ydkEfV80L5lwCgrUDMUEIkMJVZ8Gt5O6+V4/gtBDPosA9u+Hj0067rH799OfwPp0ObCnCdteUVHB/v37OXDgQMNr7blHojRmWRa5ublMmzaNfv36Ydt2Q6CmtLSU3AMHsCyLjIwMjjnmGPLz8/H4ehvats1rr73Ghg0bkn7q559gHgCVEPq6H4qFOS+H07RZ6HIw174cmh6ocTxH4BA9y7KYNmIE1SeeSMqECcyZM4dHP/iAutRUunbtyqhRozjk7LMpfOUVvu3Th23HH88xxxyD1+tlbmkpb27Z0jBJRCjX/OQn5BUVwZw58MQTEZZMjOD7xUSK594zC9MuCrctkcpaQ2KS+CeLZgdqwj8TMyeWFWKZDNfPDMxFMlJZzg4Jfr+OpifS7EiGAe8EvZaBmXawOd1AnazbL+Pfh9L2UvHvj0yi72Pnhjgd/zmJ6/dg4c5rR3I3E9q3y4FfYAIcqcBfafo57AyFceoGJ4DS3/Wacxx1C/FaOXAM8QdqegNbgLPw37Q6iQ6/j39mKyfp208xDaORwNI41yXG65jp153ndndivu+n8e/DuzGNomN8yyx4+GEmT57MhuXLOWb06GatfwH+60VHFq5ODRbc5nHXy02hJ3JmqvcfQNhhBqHqU3c7yTl+ZxO6zksFWLYMTjoJgL/W1ODBX8dm0PTExJZlNfSocHSEBKRphL4/iKd9Eyxa+ybUcqn42zwt1b5JT09n0qRJnHnmmXTv7g9H1NfX8/bbb+Nds4bevpv/nj17kp5urti2beP1evn8888paydD4e7EBKFDXfdDcRLH3kDTzp8FtFweuLS0NAYPHkynTp34/PPPefbZZ8nOzqZfv34MGDCAoUOHsmvXLurr65n3wQd8VFVFQUEBvXr14uuvv2bTpuTKmDSClp/58e4Yl7siNZUrUiI8iqyubnQfm4FJFB7vTHvJrFnX795EHifnVG7BT4KcC55zeKb4/oV7YpSG6TJ8a9DrfwRuin1zO6yvMIlyHR5gOyZJ2+vNKHck8CXmafeTmKkMpe39DdPrIZXYn96lYxqg7gbpKsI3SEOd1/g+3zPO7ZX4LMQkXiwFTqbpY7fvwjRejvf9/gGmN8Uq/FOr/xdzXm/F/0T+UcwNj0g8srOzKcjKgl27KCwsxFtZSVVVFbW1kUboHzwi1anB0jA9N35OYFupqVKA1c0s42DwBv5hlG4ZNK5Pg9tJTnt3OKGToN8FXDtxIsybB8BJxx3HlIUL+QGm9+D2Zmz38OHDGT9+PCNHjmxGKe3PbExwK/j+wEN87Ru3aO0bt3RMkM3yrXMPLdO+8Xg8dO3alV/84hfk5eU1BOVqa2t54YUX+M9//kO/4mKG5OUxZswY+vXr1zDs7cCBAyxYsOCgmP2rPaqpqeGrr75i+/btWJbFCSecwKhRozjppJOorKzk/vvv56677mJJXR1Hn302vb73PVJTU/n973/P8uXLqaiI1Jeog/vtb+Haa0O/5/VCz57MLCsLuI+NlK6jvWr2g5Z0TCbuUF1BrwQOwXR1cxsIPINJIFWOSYz0E185oTwPvIqZvs/xd5re5b+jsQmcbtryvVZL86ahrnH9TPYEbR2J06NmP+aGPlpjxAO8iQmEzsd0LX0Z06Btynnd0Z+atzQb/1O95pzD9QTWDc5x0h+T7BfM0xUwx4Tz2mDfzxzM8RKvdMwxMse1bqeX1vOYrsnPNaFcST7p6emce+65dH/zTc7+7nc5bvp0rAsu4J///Cerd+xg0aJFvP3223zzzTdtvaktziZ8nRrs78DXmACBu63UFH/H1Ndi2imR6kt3fRrcTqoJsYxbPZgk2b7cS7WW1VDHVtO0xMSWZTF27FhOP/10pkyZQm5ubvQPHUTSMO2ZUvz3BzdjHh7E2r4JFq19A+Z8+bvv/5djHlxcgQnotUT7pl+/fpxyyikcfvjhAXmHampqePLJJ9mzZw82UFhYyMUXX0xKSkpDMKeiooIPPviA+vr2M77g15j2o/u6XxnxE6a9cS9Nezg/Jo7tmgaklJdjHX00bNzIlZjJTSzM8fh3GtfFnRcvJn3ZMvD1fMv++GPyZs/G6/Xy+61buaGujv6A9/33GbJkCZmZmVy/YweX7d5NLSa48P0m/F0t5X8I3Xt5OvDrbt3MMCy38nI49VR49lkYMKDxB595Bl5/3fw89VS49VY4OsSgpo8/hhtu8P/+8MON1+V24AB3YXpaObrhP75CibQfRwAvhV9bm0lIj9gvCT3u+nuYm7fgKfecyPhRmFwDwzCN92PDlJ/tK8f9fi4mN8oNIZZ3X8qmYWYZcTsS83TkWuDPYdYp0t7VY869aA0ZCzOMcKVv+d6+15t6Xkv7lo3pWXM/plHbBXOj8aHv/c6YXA91wEc0LZnwBMzx404mfASmQdw79MeknUlPT6d3796cd955FHzxBXk9elA4yqQ1HjduHL2qqujZsyd5eXls2rSJsrKyg344R7g6NVg5ZpjaYt/vi2h6/drUAI+0ndTUVDp37syQIUOYNWsW48aNY+DAgaSmpponyWCCQh1EDf72hpN7Itb2TbBo7RsIDMYsBbrGuY549e/fn2nTpjUE4pwgTFpaGlOnTuXII4/kyHffpXN9PRMnTmzITQOmN05OTg4nnHACdXV1lJaWsm7dOsrLk/fMX4X5Xp3r/uc0TiZ8Eb68Ty4jaJ4C/PeMgzG9o9z3kGcAh4OZZemYY2D1ar6trOQzzM3yEZgAenDq3wlDhzJ8+HB69erFpk2beOWttyjfsoV61yyFw4DNts2qmhrs6mo2lZRgY+5jxzXz70q0pTRu34Pv+09Ph8mTefDBBxuOscyqKv7Xtnnsiy/YFWIo18T16xlTUcHjH33EVfX1vLpsGWtDBBaHrFjBdOAvmMTRSzZt4rMQ5XUrKODSSy6BL79kWX09a13vjcF/fIUSaT82Jf9Ra2izocse4DZMl94UzM3BxWGWzcOcPANdr3UHOhE6OZz7jzoac9PRL8RydwHvYpIWdcMM5XHkYJ6cxNvRNFRZm1FjSaQ3jWftKvD9tDDnTLinnb0x53W083FghLJWoZ5fsbIxPayG+P6VYaZ0BhP4HoMJst9I0xrKP8OMU3fnqHGGeRyMPJghE2CO+Sz8x3KW72c68V9vYuFcy4a5XuuNuX62pM6dOzNp0iROOeUUMu66K+A42b9/P1179GDixIn07t2b119/ncWLFyd9IkyRREpJSSEtLY2UlBQ8Hg+FhaZFm5mZSa9evTjxxBP58Y9/jMfjaRTEzMzIoKCggL59+7J9+/YOM4QwGQ0l/M2U+76F9evpVlLSqJ4/PDubY7t1w1qxwv+iZZFp2/ziu981v27fjrV4MQMqK7FX+jO6ZNXWMm7cOAYPHoxt2yxZsoSXX36ZlSsjZX1pW//Gn6Pm55g0FsFbeyymB5MTmBuOubFuyr3UAMw9Zj7++8w+mGCf+76z4SFRZib8/vfwyiu8vns392Kuz1cCD9I4iPGD0aOZOnUqhx56KJ988gm3f/IJOyoqqHMFak4HPu/endn9+lFZWcmCzZuxbZsbCH/vm6xs2+bOO+9k61bTgmvYj3/8Y8hcQzdg7tVv/OUv+TFmkoBQuYZOA07EtCtPw/QovTfEciMLC7n097+Hv/+dJ2pqAsq6CP/xFUqk/di8THwtp81zzF2OCbrcTvgvaTnwOIE7bAHmCe+NIZbvjT9SdhMmCdXL+G8onDGqaZiouoXJOv4jVxnODcMZsf8pDZ9zl2XR9ARyIm5O0t9o3Mt4iD2ZsPtnS7gDc0FyNzedbXXOxUgsYksma4Upqyvq9SNtozP+Y9c55t2/25gcQIlOluxc62wCr2W3+362ZOrJQYMGcf/995teAEHeeecdjjz5ZHr16kXv3r254YYbuPLKKykpKWnVLvyxBgYt179YhKtT3WUkS98hi/B/nxX0UxLHsiy6dOnCoEGD6NatG1lZWdx5552A6SWRlZVFz54mI0qonmZDhg7lh0ceybhx4/j5z3/Oli1boB0NfzmYvI+ZNS+qiy7iEgLvNQB44w2sN96I3EvKtk2dMWaM/3y0bTqPHMl3XUNHhw8fTnV1dVIHamL1GOYez0kmfDnNSya8Dv995iuYIXWXuJZ7ApOTKl4vvPACc+fOJScnh927d4ftHZqZlUVaWhrLly8/6HuPgr9nmAVg27qONFGbB2paSxkm94KNmfbzKswT9z6YE38BcItr+dmY8atXxLmet11leYANzdloEZc3iW2WM+cJfR6Nu5OGk4sZJ1xHywVqAOYC57t+Dz4Xw/Wo+T5m3Gm0HgcTMRdgd1nDCd2NU6S1TcQ88ZkEnOJ7zX3NSCTn+pOHye92O/6nlpdjprR8OMHrdBQVFTFq1Ci6dm08aMC2bf73f/+XS6+9ltNPP51x48Yxbdo0CgoK2L17d6sGat7CdIOOJgvT2IwneWlwnWoBKzCB8zeAC+La0pbRC7NNnYGHaDwUPBPT47kpSVvD6UTHnq0zIyODPn36cNddd9G1a1cGDRpEQUEBlmWRmZkZezm+oYVZWVlMnTqVf/zjH6DEpG3mcsz5Hmwipp4B4K23uH/BAm65JbC2HzVyJGeedRY//p//IadzZ//QJtfNvHXLLViffYY9d27DTb71l79g/etfAWWtW7eO//znPwn5myQ2lb7E+Dt37mwUgElJSWHChAlkLVnSkFuovczO1RwpKSn85je/YebMmfT75z/Ze//9XHHhhWT8v/8HNc3JjtrxRA3U/JHwF+lMTOPjaUw3+GAjME+xXw16Pcf1/1vwTwvbkmxMsCY4sW4ZptFQTeDTxTpMd/F4Tyd3Wc6TTJHmWgNcHeOy1wAnAQcwAY5YctTMpvFQiKac105Z43yvFbiWOxTTc8adKNY95riM8IGaSvzncCROt1h3We1p2GEB/i6bzvf1L8z3+jAmANUVk/Asjcj1czTB+7G5478lunLMcVmP/1gOdf1JBPf1pxaTfNNRBbTkLd2uXbtYvXo1X375JYMGDSKvro6vPv+cZZbFxZiZSt5++226du3KIYccQlpaGqNGjWLXrl1s2xZqTp2W0QkTKHsiynLXYIIZv42xXKcevBf4DP95fTmmV2F2Uza2BVj4h6P+DTMU3O0STHD8+gSu88+EHoreEXTKzuboSZP44Q9/yDHHHENaWhrZ2dkNUy3H480332Tj1q1UVFSwatUqzfrTxqoIXYcHtD86daI6I6PRcks3b2bPiy/y+erVnHrqqQwZMoQuXbpgWRbDhw83N/gZGZCSgu1OKB0U2Fu+fDlLly5l165YH9G1HqcOLMDUp+di2jBO2yZUYtdCzDBrC1NX34IZNh2v1mjbhOshk5qaytFHH03W2rVs2riRJSUlVFZGS518cMjIyCAnJ4fMjAy6dunCT3/6U9Kfeoozpk6lsKCADRs28P777yd8vc7xFYqH8G3nwzHts+B7m2Ch7oFewMx+3BIiBmpKMU9bwnFu7EoI3ejrh7l5C56WsIvr/3sxAZ9ukTZEpIPbR+xTqTsVVJ3vM7EEamoxNyyxTAMb7bwuwT8rUb1ruWGYp8nuz3XUBns4GZjM+s4sCF78Y7R3Yi4OOcAOzH7d5Xo/Xs5+LMVMt61Z9CRRqqqq2LBhA8899xx9+/blgtJSVm/dyjt79zaMx1+9ejXLli1jx44dFBUVMWjQIL7++utWDdSA6Q4frW49F3PuvYMJfkfrwu0MI8rHJKzM971eiD+P3SWhPuhabiwmcXtrWULj7+F4TK+bWK89sbgtgWW1NyeeeCLMmMGJJ55I9+6NU1daloVt29TX13PgwAG2bNmCZVmsXr26ocdN8ebN/LC2lvnz5/OpL0Dz7bffKlDTjpWXl7N69WqKi4spLy+nX79+5ObmkpKSwqGHHkr37t0ZvmULfVyfqa+vx+P1UrF/P2/9978ALF26lE8++YSqqlCP2NqW07YB057d7nsNTNtmb9DytZiHjdvxP3TYS3zT3KfQtm2b9PR0unXrRmlpKfV1deytqKBk//52NUNXc9m+IXvpGRmMGDECy+PhqKOOouiQQ9i0aRO9e/dmz549rFixgozi4oT0tnGOr1BSCN92rsYEXKMdY8H3QKeR+GHrbhEDNRuByyK83xuYCVxH6Ozpd2OS+QaXMRJ/A8SZWeR2RKQt3U9s438jnddnYpKKPYOpzMrwD+m4D3PD4u70+2v8U8cWEb5HTRdMJDx4FoBgoZKLtzc2pk69A3NzeCVwIea7ugTz3f8EM3TiTpo+rMvZj05ZCtRIIm3bto37778fMDM8FgNfbfW3FHbv3s3GjRv59ttv6d27N/3796dz585ts7ExygIexTTwot0WZ2KCOmfjn4b2F5geLM5kCuF0x9Rl45u5vZIcUlNSsOrrueyyy7CnTw+5jOWb2hegurqaLVu28J///AfLspg9ezYej4euXbvy0Ycfcg7w9ZIlvNeKf0OysTA9TOOdIKCn72cvwgdcC4P+HzwJQkuwbZvy8nLmzp3b8JrH46F///6MHz+eS1asoE9+fkMwr6KigrTqakpKSvjVr34FmJ6M5eXlbRIISCNy+6yn6///xSR8zcO0bR7A9Bp3GwcsxLRxPMA5xN5GdaTTtm2bnJwcBg4cyLvvvsuBigrq6urCtnEPRlVVVezfv5+qqqqGlAwAI0eOZNjJJ1NXV8fMmTNZtWoVjz32GBlvvQUhZnmK1+NETiYcru3s5C2KFPeAxvdAC5q2mTHrMDlqRKT19cTMegb+RtFm1/vOaxmYJ9uRWEGfDacjXQhFDha9evUiOztZBgVFdgKNZylxc5JfzsLcWPQGtmDy4TgB2EgTFTiTJTxFyz6pkxYQNAQiLS2NPr17Y23aZHq3hhkiYeMfPnHgwAGWLl3Krbfe2mg5JeQ08jAPk5vCwkx5H4uXmriORPB6vaxfv55u3bqxv7wcKz+/IeHwF198Qfd160itqWHNmuAwR+sbS2ztM4A/+f6B2RehAo4WMAH/UKf2eNxnZWXRt29fvF4vaXv3Qgfq9eb1enn33XcpLi7mtBUrmBH0vsfjISMjg4yMDI488kg6depEXd++EJS/STpAoOY+TMQ9F9NQsvFHx9OB9ZhuyGMwSR4dBb7PnUJ8umMaWNJ8PTH7zM1JyvgBZpx/umuZ7jRvP6bjPybcTSkbGEVgjgeJbjZwret3p0eNe8aDX2MuxNWYpL/hJhg9DzPF3+ExrDc4D1V7dgpm+GnwMb/e93MOTf9b84LKEmkrlmVx0kkn8a+gxJgikYylcRshHKdXZqjlLQLr0zXAd5qyQZ9/Dn3MAJU5paVkANm1tXyyZQs5to31ox9BRkbkMoDuXi9nVFeH3dZcTBAveIBLF8zwvI5gH2YGn3jzQPYEPscEAXaEWeZwzEyxYKZVnoSZQbatTJkyhYGff45dV2eGktg2GzduhNLS2GabagVLgGkR3u8JfOH7/zWYHjW5wDJM4Htt0PJzMD0e7sQc88sTubGtIC0tDa/Xy5YtW/j888+pChrS06dPH0444QSmrF8PH3/cRlvZcmzbZuHChXzxxRf0qKvj0LQ0fjpjBv+uqMBTXQ01NWS46sJhw4bBqFF0jOw98TkoAzV7MU+y7sMk+DkA/BhT0dqYm75pmK7Lv8FM/7YKU3E4rsbcmD/WhPXrCVhilNH44pgFPEhg11R3d8t0TNLHYPE8p+0d9LuNSXYW6ob4UWB+HGV3JBUEDomswAwHcL/mTiC3jfBBh72YgFuoIZYHq0eBefiP+fuAY4ChwB8wyT8fJnpPpHDOCyrr95hg2Xkhls3Hn3B1iu9ntu+1eBvKnTANr/vw5zZLx//EbCamAZ7CQXqBEgC6du3KgAEDGDRoEACZmZmkpGgAXiRTpkzh0ksv9b/w6KMwP/IVaLDvZ28aJ012J5C/DJOTxu1wzLU2WrJlt2tonG+ipRQTe86byzB/7/0h3kvFX58OxzzwaZLaWigpgb/9jYd//3uGr1vH8ampPNirF7/asoXy6dPxjB9Pfn5+w0dWrFjB0qVLWbJkCcXFxVFXYWGuB08DX7tev4/kSVLdGmxMe6CpE3bsIHx7otT1/50kPtF7vPLz88nMyoL9+5N2WudaYm+f7fUte8D3e0mIz9Zg2odbMcd8vEPc2lq3bt0YOnQohxxyCCtXrsSzeze4hqTt2bPnoM8rVVNTY/4BlfX1fP7559TX1zPvvfc4YNsMGzaMsWPHAiawRVpawgI14zAzygZz2pU30zhHzaGYh8bRrnfB18XBmPvOHq5l/kxg/dwcB2U7uALztOEOzE200wX5KUylPhoTqPEC/wAuxQRX3GPazsFU1uHGuUnL82ZmsvvkkwNe61RbC64xvHTpAlN8t44ffwxdu8KoUf73Fy40T7DGjYu+wt274ZNP4LvfBWd6xP37sebN48KpU1n45ZfsKPGf2qdgbqQVqJGWMN/3Lw/TMH8JM2QiH1NvPYhJ8tnUHDWjQ5RVTuNAzSZgsW/d4E8AaPlei7fZ6MxTke/6f5rr/WzXul4jsVMDJ0IeJnlcPJwb4hMwF/UurjK6AEOaUGY0Fv7vtS+BvQrHYfZ1qm+9kXJd5CRg25y/8QTf71OBTikpHL17Nz0/+wxSTVNk/JYt7CXo+1q+HObMMS/MCO5A3bEMGTKEWbNm+V+YNy9qoMaRj2kHhTMFfxA2WKTPudmYa+K7xN7TpTl2EXsb7XhMvRJq+XQC68AmB2ocXbtSlppKBeC1LMpSUrABb3Y25OVhd/FPqVHbuTMVmZnsS0lhdwxFO4lVyyFg+fZ2IxtsCqZXtJtz45OJvw7q6/vp1F2Rrj/7McdjNKmYOsl5WOCeJWgKJv+eY6pvu9zblImpU0Ptv4AZhz7+mCErVoSsT6swEzqEsmvXLioOHAh4rXfv3nRxHUcd1Tj8x0SwNEwvOof7Wha8D3GXU1cHr7wC5eWM8i3jlOU+Tm1gLo3zlWVkZDBu3DgOP/xwevTo4Z9u3eXAgQOsX7+e0t27G4LpB7P6+np27tqFF6hbsICa3bupGz4cNvoHMFqffdZwXudAw3cfrG95udk/tbVMJHAfO+fiECJftyL1/hoSw9+zG387NdW33q6YOmQqppOIAjWSlNzTfQZzTqaUCMs4soGCLl2Y8/TTgW/s2wcDB/rHeY8YAS+/bF4/5RSYPBlucz1jO/98KCiABx+MvvELF8Lpp8Pjj4MzXebKlXDUUfDII/zpxz/mzTffbFh8Baa3Q7i/Rc+G5WDwLoEz1DyBuQAewOTZiDdQ4+Tr+BH+p2h5mAuf5Sv/3qZvbosbCjyLmdEsVs4sQL/H3BSmYp6Ig2mQjMAk4g4eXunUp/to2pNj5wn7sZj8KI4fY27scjBd/EcSXj/fMmW+bbAsC48vT0K919twcxNp+3KAAcDJmL/pESCjrIzUN98k5R0zWKOiooJL6ur4EUHf1z//CS+8AOXlAU8kO7p9+/aRVVOTFMMWazCz1OVhpiidRQd+yFVXB5dcwm/Ky0kF0mtruXXDBjKB+kceoe6RRwLO8wG+f2e5XsvEH8QO5+Ywr6cBebYNe/eCx0NWdTV5+APsOYRvs9i0Xe+R39C4TnUCtl3w15fZrvciHWMZmGG9o2NYdydMHVeOqRfdbbff4L9RcuqubMx35QzFzsXUqbNClB3QDvztb5leV8eJQcukYx5I9CG0BQsWMGn7dib19Yckxo0bR+agQR2ql3EoV2GSth8I8777JrcI/3HUicB9CK4eaQcOwMUXw/79nI//3EzFf5x6MOdSV8z12S0vL48ZM2YwZcoUli5dSklJCfUhekJ9+umnHIsZWtdRlHs8nLRiBWlr15Lmup8CoLa24bzuDAHfvSMDyCouNvunooJrCAyUOeeiF7N/g/dNJM4xEWoWa0dnzDm9En+eOSen3I3489MlkgI1klC9CH+QOg36IyMsE7Dstm2mh0yw4AqvpgZ69oTqali0CP7618bLPv98lDW6lu/Zs/FrAwbwT980c+5tfAjzJC7c33Cw5EoREb9Tia8nUz6mC/0kTNDrKPy9F+YDkzHDb4Of5BdhejSNJv5hfx5Mj4M84DnMjZ2T7PFyzI3By6E/2oiNuZmsTE+nS5cu9OxhnnWvXrOGdF9Qe9++8E2i+cBHmEbyEmBoaiqX33ADZ5xxRkPX51nnn8/8+fPZvmNH4Pd1221cd9RRcMwxMW7twc+2bUaPHs0dxcUN0523peDjq0NLT4ft2zn1hBM45tNPuRiTS2cXJn9erDMrXtfE1U8F1peVYXXrBsBtts2tvvcs4M0wnwMzlKxfE9fbXKHq1DnADMywaCdE8QRwMeYGrIDwAeLrfcvFYxLmBuwowMkacipmyPEf8NeD/8DUT84DjC3Ar/AHAdzcZfHmm/zlo4+48cYbA5aZBfwuwnaVlpZyoLw84LV58+bRZ+lSQrSQO5y5hE7OnoEZZuUEKd3X2DkE7kPwH1vk5cGuXTBmDLesXMm9rrKc43QU5loWSklJCVdccUXDDF1i2MCpw4czfvx4pn33u5x//vmBC7z6KmVnnUUBZpRLqId21wN/GD4cliyBggIuKCsLqFOdc3EPJuATqY4IFuqYCLaMyA+2WoICNZJQJcBxYd57DdPtfzFmerRIvgfcWViI/cEHHHfssZSUmlHDnTFT9nmAe4DHvvqK9LFj+by6mnRMXo97XRXj3zAn7K9iqCzHY07sw227IcAyGHjNsmDhQn52ww3MmzevYfkPMZnrXwpTVoyhIRFpZ2zi6+HiDEtwfy4Vk9RyqO/3wTROmJiKubmaR/TpoEPJ8f08i8BhLXcRX48ggJ/89KcMOPRQhgwZwsCBAwFYtGgRWVlm4s3SUn9mh+3bt7N+/XpWrVoFQOclS7D372/4Ho474QSO/853GDZiBHg82LbN+o0b2bd/vxki4lvOBmzL8g9FlQa268FBGTCR0MfkPzFdwddCo5k3emCSlIN5GvhS0PvXA4fR+Ho9ePBgXnv1VfPLz36GPW9eux96k1CWFbAvnKe7sdYbNvAVodtJHkwb6HoCh/V8iMmb8D7wf507s3DhQizL4t577uGxxx8nHVPf/BDTBgv2PeCKGLatpUT7buygn7F8pqnb4A16Ldx2BG9TqG0JOC98x0W4MsMpLi6mzOvlq6++4sLRpo9QWVkZPysr4+won+0Iwh0HoeqkSMdPQBm+XqPBx0SoYyTsdilI08Dj8dA1P58777qLkSNHUlhY2HDtX7ZsGXPnziXz7bf5IaHPsRAFhjyfQp2T8eyFRNcpidBuAjUzCEzU0x8zfv2eCJ/JwySmrMJ0I70HswOcQEIKpmt1qLJGEphQTGKTTfg56PN9PwsjLOMYA1BRgfXoo1xYUdGQdDaTwJ45VlUVqatXN3Qv3YWJmjsOYJ68uF8Lx8m3sQp/T5iG24OhQ9nSqVNAObXA9jBlH+xPOfrT+NwrwOyb32HOnwFByxyB/zx0BJ+L4W4em50zQCTJWJjEpc7Tvr2YQLNbLqanwnPEPyTBAn6LGZ65GngD020biHumEMuy+N73vkeXAQPo2rVrQzLU9PR0kwQQM3TJsX//fvbs2UNpaSm2bdPr17/miOHDYdQorH/8g0suuYThw4fj8XjYvn07L7/8Mtu2baOmRn0QHSdgrpVOnVoNjFm0CK67Dgu4ed++huFs6ZhraqgGppN4vwuNr7vuxPuhrmW7MUOagl/3pKebYccAnTrRVvoQuQ3oFur640gl+RrDob53MMdDPebJcXB7BMzwnTUeDwwfDh4PJV27sgpzjNiYKa1DlbstYVsuiVZXV0c9UFVV1RD8hsDJGKRlzJg+nR7HH09qfT2pN93Ez3/2M87u35+CkhKse+/ld7/9LdWZ0QYqQuE99zDjiCPocfzxAa8fOW8ePRYt4p7rrw/5uby8aIki2oesrCwOO+wwxo8fT2FhIenp6dTV1bFlyxb+9re/8cUXX3DIhg1xl3spgUnww933xyLUfUuwwgjvtZRkuzaF9CXmxtA9ji8Hk+sg0ti+NMxTyjrMHzoJs8OcRqoH8xQqVFnbMFM0HsxG0bzZGQb4fk7ENALBNPyiddntTPQxmT3ADGn69FMOq61tCJy4D9h+mBt8Z8puaZ5Yj4ci379w+/nnrv+HCrCE+pxzLkZ6SrEohm2TxHHOaws42vW6EzBP8b0e79OHbr6fR2C6mYK/50dHVkHjrv/Od/Ul/u8qVs4NHZiejl82fdMAOPzww7Fzc6mvr6eiooI9e/Zw4MABKioqqK2tbXh6WFBQQF5eHt26deOwww4jOzublPvuY9yYMQw66yz4xz849dRTsXNzKSsrY/PmzTz22GPs2rWL+iTOQdMV/3XrMExgOhwLc61y6lT3Md8Dcw08OuQnjc6YuvNIzPl1BKYd06O0FD41R8lhtbUN5brbN8Gcc6trVhZXjx1r9hXg9Xqxq6pg5Uo+tyxK2uHT307Ent+hG6ZNGG75T2icI0okoZYsoc/GjY3O/aGYQFqkOqGIxvVGf0wgPtLnHLrGBn5Xkdo21NebJO2VlRzXvz/HTZpkZnSzLC4cMwZGjwZfUOHnRxwRW7A6K4vjjjuO464LagHX18O333Jd8OsHibS0NFK8XnJycvjud79L9+7dSU9Pp6amhj179vD+++/z73//m927d9OzCdeg4QQmIQ933x+LLpjzJNI1JQuT+yrcsLeW0OKBGgv/E0OH+/d0/DNTBC/nuDTEa/Mw496Dp292+xYz1ncPZprtkzE3gncC12KeQJzgK8tJBNRRVGOetjaHk/DxZfz70PaVHUo65sbhS/yzf4QzC3iiWzfsjz7irD592LrVZGjIw5909CHM+MV0zNPmcMePxObuNly3cy7qeXrbqsOcv+kEntfu5IfOazn4Z6loyhPpl8K8nkrocznD9dP5v83Bc8z0B94Jes2pY2fTtO64zvc0FTgp6L16zP6ONxltZWUlmzdv5pVXXuHrr79m2bJllJSUNARqTjvtNHr37k2PHj0YP348hx12GB7bJj8/nzxXMkwwPW/Wrl3Ll182N4zUsuowSSdfxuyPx4m+PzKA2zFtDvd+dM6T4H0drAr/zF1TMT1DZ02bxhNPPIFt25zVpw93bN3KLMyT9WPCbNMCTA4kb79+HHjzzYZ9VVlZSfW333LE977H+ZmZbKipaXfJmldh/u5YPIEJtoXKZeE2Pej3dALrnHRC108BSWOrq0m37Yahi05P4HCfDVWWZVlkpAednbaNp6YmZFnOwyqP83pVFXg8pNbXx7T90drhYM6DZDpCMgn/cMf57sPdf7i/h+DX3N+6k/zd/fl4BmLamHM5A7B+/nNmer2Nhis5UwZHqhNSfet9J8pr4Th1EOnpkNIxp7pwX2MjtW3s8nKqjz3W3LM89BD1Dz0EmH1Y+9OfNtTp6UD11Kmxb0BdnTkvXVLr6vDYNjVBrwfz1NYmReL4eFiWRY8ePcivrKRr165cffXVgHlIsHv3bj777DNuu+029uzZ0+RhYjdAoxw1d2LuE58m/HUxlFcwI2ki5ahZjrn+t+aEFy0eqJlI46zL7t4Ps32/p4RYLpI0zFOmKyMsk47JUQLmD93j+n9HZpOYhHEjMUGXfpiZHqZhEl4OCrP815gcMMk8o4tIR/ZnzCxP7vO6FJNF3/Eo8ANMfd0TU5/chQl+J8LvoCH5pZvT0Nzkei1SfdPerMLkEnHrjXngMJz4hyZ4MMNZ8oBnMN/petf7b2AaJNtjLM+yLLAsLMuisrKSJ598kvXr11NXVxfQyHryySexfMtlZGRwxRVXcMXmzfT1fd5dVmpqKhkZGaSmplJX15QsPK3jZ5j8HXmY7+tITNLRcJzvfhZmymf3fvwd5sldLLklnGtsIng8Hjp37kznzmbAk23b4Jve99hjj6Vm+XI2b1Y64GCzCQy2rSJ0w7+hXVlTA927805tLRbmWNiDaY86ZUWTClgTJ7LPlRMPgBUrYPz4kGU5N3HTgE379mH59u3v6uoa6tN0zI1qqG1IIXo7/JfAfTFsf2vIw9+mD8XpbR3q/iPUfrQwvXZtAgMx7+C/Z3GuP2nEPqvZQsxwiVIg4513+PPHH3PTTYGPmH+IqZ8jXcvuwgRcj3e99gvMtXhcDNvh1F3WZ5/BmDFmltMOxn2N/S/R2zaLMfv5T5ggTSnmgf9CAu9/Yr53vflmuPXWgJd+UV/PD7xexkUZ4jTd6+U/sa4nCViWxdChQ/njH//IpA8/BCenGfDtt9+yZMkSPvzwQ11zYtCiMYsHIOyB5cFkn78Vk7vk1zStR0S0uHCa6//B5adhEumNxpxsxzZh/Y4azBPL5G1qBkrEU+ga109viNeD2ZjvpzW+I8uyeP311znyd7/jwxUrGNGjBytXRmpay3WYXmpNNRBzQ3gq5rzeA9zhev9mTNfdG4I+dwjwVyQZeGl8Xrtfg8AnqjWY87oe0xX0JzGsozumZ8LpNB7O8zzwKmb662DO8TUD04NgOmaK74NFqN5Bzu+1Id6LxsJ/IxK8D8O9Fslbb73F2GOOoaCggCFDhnDvvffywgsvsHLlSnbv9s/j16dPHwYPHkxRURFZWVmcc845FL79tkl+a9tYwLp168jt25fs7GzGjRvH+PHjWbx4MbW1teE3oA051yz3uRHpu3O++9qgZWsx58pRBCaCDSfLV9Zbvs91f+01mDwZC5hTWtowE04O4evuhuGnGzZgHW06+u//zW+wpkwhx5ek+dprr6Xv/Pm8+eabLOyAN3CR/BLz3Tr11tmEHoZ4JWaaYNLS4O23Te6mZ56B118n45ln4NRTSbv1Vjg6ykCVCy6AjRvBF+gM4Ophk3bPPYFlnX467NyJZ8oUMu729499+IEHePa55xqCNJdjZlQJNh1Tf58eZrOeJ3p7uzWVY9oa4Z6WX4QJWl0Y9Hqo/ZiDuR+5GNiAaZP83ffe5Zib+yt863KuP7HmsAzoZZ6WxgUXX8zRJwT2Ke/+2mt0f+QR5s2ZE7ac/g88QOdvvmHeI480vNb7mWfo/vrrzHvuuajbkVJejnXqqR2+R8083/9HYOrjea73B/t+5mCScvcDfoo5J5zejQ9i9r9TN79KHL3M6usb9VrsjTkm50XJz9Yl4rvJJT09nd69e3Prrbdy2GGHkff114B5OLBkyRIee+wxPv30U7Zs2dLGW9o+tGigZhOBTz/dLEzjZyUmypsWZrlwLsJ0cZrneu1sTAK214OWHULop1ce/GPROmPGgDZVHSYLv/sUdLpdno1pFL/RjPLbixwa34g7CjBPA2LpNjoBoLwc7r6by/fvb0ik6U4m7JTlPAkC04PrBsCybY755BOyd++mU6dOFBUVHXSBmqsIHdx0GuYZmGMyGuc4HU3zGmJOkq1FmCBNKYE5N0ow54nz2qWYY6K/b72/oGW6VTvbdRWmCzKYXAR/C724NFE5sU1Z3dv380saTzldiUl2Gaoc56nVIt//RzRhGxPlIuIL7Dv11qWYnqC9Mcd7e2ku27bNv/71L/Z7PBx++OEUFRUxadIkUlJSKC4upry8PCBHTa9evSgoKCAtLY3+/fuTmpHBvrIy9m/dSpFt8+ijj9JlwAAGDhxI9+7dycnJMb1sWlFPTH3jXDNi4d6PkW7UQg25cOuKmRnwL0Tult0Nc5N4pPNCaan5h+8a6ZOKedoeiVVZ2ZDfJvWpp8yT9WpzCzn07beZ9u239NuzJ2DWxiNDlNPRrMTUR069NYnQib37OP/xeuHjj83N8Pr1UFEBH31kbtCWLYs+vMxJyr11K/zhD4HvlZQ0/PfFZctY6yrr8upqOgNrd+3ixQ8/bHj99S1b+BTTm8aLCdKEql9HYAKK4erwyshb3erqMdsa7vw5FrPNwX9PqOuP05dhMWZ/u2uipQRODnEU/naEozMmKNSD0BraqM88Q1G/fo3vNVavhpoaJrv2WyNbtkBODpMnu870Dz+ErKzA18LZuzf6Ms3Qk8j1aK7r/853Fak+7Y2rPY//Xmp00HJjMJOYhFq3+94A33Lub+obTNoLR2dMe7EO+CjovVTMtftrzDT23YBDMUNLw6V7APM3/i8m/cauEO9PxOTairDnG7zpKyvZZWZmMmzYMI466ii6du1KSkoKtbW1bFq/nmeffZZ58+bx7bffUlkZuVY5nsb3i8eBmTr97ruhurrRMeGcixXEfg/kGExyTiKUFKOAvvD9i8exNM4rMxrzJQfnmjkNf6BmLeZilIW5QQzXjTUeTlk/CFFWOv6nvgd7oGYv5sJ3cZj3d2Keig+Moaw8gLIyrBtvDJuHaDqNx5Mfj6tr6G9NFp6u/fpxSEpKo5vCYLFsV7KwML1Wymic+d9piGQBQc28iBLRO8GL6d6fQ/QK7xeYBlAFpjIOdf4kQjrm+7rAt31Oo6y9BWoGYwJbWZhutxbmmI1nyKhbuLKcp0qx7seOaBrx3bg4jY0zMT1IszHHuwczHCkbc5yODPqc0/Afiv+4jWedTiM1HxgW9H5n4gt2zZkzh9rsbGpqajjmmGPo0aMHp5xyStgAixO4KSsro3NdHZs3b2bdF19QBDz88MNk9+rFIYccwvjx4wMCPa2lAFPfdCL2mRzc+zFab6T1RJ6VpQrTVon0V4/EDE9bha/uysujqLep4VevWUNhXR35mJvWcDMbDsAcXzWWxZb0dPpXV5O1YAEsWQJ1dVhAyQMPkF9by+T6+kYBn68i/pVtK5PG50w4+ZhjPtryhZhzcRSBN3l1wArMVPfhPgfgra9n1U03YWOOsXxg3S9/yXCg+Mkno87UMxhzg1GxcSMbbgxszab73l8F/PXJJxumVgcTPO4MLF+xghtvDG4FJ79Q1zInAW4q/v2W7/vpweyjcEPJnP0Yrk6Nx07MvcNg/O0INydJbbjZKZ0hcOsfeijkdSPPt71rou23ceNg+fKGXwtKSsivqWGd67VwPGVlDMf0ZqyxbdavX8/w4cOjfi5WBYRv+0Pgzab7u1qBuZ4G6465rzre9VqkNmo8bV3HWgKHsA33bVeoujkd02vuQUzwb6Rve24lchssDzPBxh8JPVz2Bsy+j+WMdcpKZhkZGfTs2ZNJkybRq1cvUlPNnj9QXs4777zDU089Rec9exgQImDdF/95nU7oezzABKx/+Usg9DFxnuv/4Y6L9TRuw/UJtWASSIpATWs6DXOyTMZ0bRtH84cBhSvLwiQ0ym9m+e3Ff4mchCkeszDJ/7AsvEENeGeMsTfoNfdS7ijshE2bmLApXN+uQNUEziLVUI5tN5pdynL9c2vN243LCUykBeZ7uzjO7Qj+/uLh/g48mPG7YLoQR/u+/oh5IvEuZiaVSE8mIol2s7MU8yR0H+bYur2J62kJ7u/D43ot+Bh81bXcYt/P6B2eowtXVqT96PDgz98Q/F448SRiTFYXElvvIYeTBP00TB15NCZ5dpmvrGMx09OHGpJgAe81cTud/XEmjZOoHo8592Ltx1JbW8vs2bOZP38+Rx99NH/84x/Jzc3F4wm9R71eL9XV1bzyyit8d88ePl61in9/9VXD0Ipt27axbds23nrrrfj+qARZhun6Hs9EAu792Fr9M21cdddZZzUkEz7BlUx4P+YGI1Iy4S0ZGZw7YgTzliwh58EHsadPh+Ji6NeP/x07lgUbNlDi6rHRHowm9DkTinOcx7K8hRnK6T43Smj8NN/tbszT21r817IbMPlHxmKepl9JYD0eyjLMNWsxMCXovVG+7ZqMOR5C1cuh2iSQ3PWujRnWGsz5O3pgkni65RLbvgwVwrAJfY112njB39XjmFnB3O0Ity2YiU3C5a1xJrwId92YhWmTRDq+AFi82Mw45HMDpq03enTUT/rrrtNOa6i7Fi1K3FyayzA9TsLpjemJApG/K8cCAmfsCddGdfZhuDZg8Plgu5Y9g8DrYuv26Uxe4WbSbXjNtrF8w5id5R3Dhg1j+vTp/N///R8pKSkNw523bd/Oz372MyzMPcqMCOuPtU6PR/D9Yqhz8ZUWWG8idLhAjbQjvXphr1jByBEj2L7dpLzMw3/jeAum63g65kI5HTMsYjjwWRNXmQ7swF+RpwDYNvTvz/OVlQH5dTpjZp76c1AZv6Htp5KeC5wf47IezHf6P5iulfF6m9Bd5C8k8OmjM6bXSQCYg5l5zJl5xv29x+tIYHUTP9uWLMxQH6dbsHOBXIH/whKu4d1aQu1HMOei05U3E3OuREru6EjmG4a2tArTDdqtF+ZYGEnsSX8dznmdh8n5czuBwYW5mJwUG+Ios6amhvXr11NcXMyHH35ISkpKQ4+aUD1ivF4vO3bs4P2qKvbbNiXl0foTSEvp06cP/37hBTqNHw/AypUr+ey//+Vi4IsvvqDUG0ua2+SymMAn7pH8DZPnIdp18SpMnTeZ+M4Nh/taloEZ1r8LU8c/T/QcfU4vkiNpXJ+m4L9mBJ9tnX0/p4b4HPhnqUlGiwm9H2dj/p5EC3eN/dT3WnsZknqwux/Tzo/URo10Xjv3Bu5hqDamp06o4Yt/o3FOo47oTUKnIUgH2LYNq0sXVuzfH7KNOmzYMCZNmtSQX6u4uJhsXw47p+7q2YLbHk7w/WJ7okBNBzCE6E9xmiIH/5Sj4zC9iRK1nj4Au3djXXghD+3Z0zAmOA1/hXAhJuLu8b3+W0wDJYfI/oTpwRGLfsDfLAsefZR7772Xz1xPIJ7GPBVwl/UwydEYqiX0hSgU50lFRRyfcasHXsB8Dw9iLpgHQix3DaYx6UwL/zAmSeYKzIwGPyD+RNOdgH/Svm/+O2OCfZ9hug8/hekptTvEspdgbtp/Bfwb0xMg1JP9kzDfZ6SuyI60oLL6EX0/nuR77/uYY8fZruDxwJHKCjVeuyPz0vj8c+qy8hDvReN+AlmL6XXhFq6OSMWczyN+8QszK1B5eUM9XwfmaVp1NWlbt5pZoMKs3/YtW1NbyzDMmP6JvqdrDWUFuY/Y62aJT2pqKr179WoIrJWXl7PJ19O0vr6+VXuCJko9sZ8XNcR2XazGnIv7iO/BweOYAE2oa5lzzN9L9IdID2OGAKwmfH16GY3r01DtkVCSMVNfuP1Yh3kQ9GfXa871pxxzXQm3j87BTL38sxjW77Qjrgc2Y65ld/nKSMbvq6OoxhwXkdqokc7rcG3x/WGWT8QkKweDWwjfrryma1fsp57i8h/+sGESAef8ATO7oPsBTm1tLXV1dXg8Hrrk5pK7bx9pts3qfv34cILJtNazRw8OHTeOPlu2wH33YT//PNbll7P9kENYN24cXy9ezNy5c/HaNucAP+rbFx56CGbO5LaKioA61akHyzEzkZ6PCcBVYfLQ/hHT5v0jjQPahxP9PrYvpr17vOu1EZjha2Mwx2on33fornt+SOg2fSwUqOkAaon/aWys1vp+jsScCNHWcyTmgI4W0MkFk4yvZ09KUlKocL33OCbn0H7f+lIwlfguTPfkLr7f7QsvxMrIYMeOHSxfvpz1GzZwPqbrcHDC6XAaxjefcgqLnn464HNVIcpqq+fF2fifKAzFVBSxDkNznrSdTNPGbxdi8hMV+H5/i9Bjds/F5KRxvq9yzMVgAaZhPJf4L5Tx5u1IVovwT+MLZqhLqJxKx2N6WbyB+c66EvrpRB7mvIjlyUUq5hhwynL2YyGh87A4PWrqfNtsu7Yr+Lxyzp9wx4QkHw/mKXZGQQEUFsIe05wpIej8jGOGplrMjaUTnGtUFqb++neTtlhiUVdXx5YtWxjo9eIBunTpwogRJktRWloanvp6vO2wV02yWIlpe4S6llmYc8Cp5yNx2hB7g5Y9BJOAGkLXzR5MvR+tzv9emNePxrQj3O2GTZiZotrSFgK/h3N9P93Xn2AzMEHuVGK7Bjq9fbth9pOTPLgH/u+rl+/nRTT+7rOBYzDX3GpCz1robNeoEK+H+u5jcTgmtUIsn3P+RpFoFhB6iF4hQEYGTJvGe5mZjZJxg+lFW19f39DDNjMzk/T0dDp37syMGTNImz0bamtJGziQzjNnApDVrRspI0diL1qElZpK3cknk9apE6tTUpiXkcGynBxe96XBGAOQkwPTpkFaWqM61bkv242/jnDq1Hzgu77/O/eLbtVEv4+tw3/v6XDaN9vxBxX3+n7vBMzE9DpvqqiBmu7du5OeHjou2bO+HrZvp1evXlhhxqmH4wE8W7fSrWtXijLj+xOKi4ujLyQNNmKewLSkVzBJR6Ot527MRSnacrOAY/LzsR99lOvmzmVrRUXA+0dhGvX3YoIMFwB34k/ydZZlUX///Xi6dGH1/Pn87a9/5YUNG1qkG20yyMdkld+KqTQ9wG1xfD4T0zsi1OxokRRhKqbBmArQiwk2hOrV1AnTWHBmPEjFn0TPKaujBmqaohjTYyYnJ4f8vKBvoqIC9uzhsTjKC07a/UCU5S1MzzcvpldQOo1nznMayeGOiWDOMRFqBr626C4bTiHxzRLo7J2e+L+rPpjvsND3fmqIMp2/uRfxN7I9+HuadcJ/o+Fwn4uhtovLLoMJE2D1auz//pe7aFqPOzA9LhdieuqcCSHLmo650S3i4D2ve2H2BQSeP+EEnz9dDhwws78AverrYyrLab2Vl5czd+5c/qeujpT6egYMGMCAs86Ca6+lqKiIPbt3U1bW1D3cMVn4g+sQ/lrmnIvdiF5vOI3y4Pr0fExPSjB1804CZx/KxQxRPTXGbQ+W7SvDaTd0wfTOaetATVP8CjN0rZbY2kFO3Xol/qHYqZhh7A7ngcYvaRwccr77M3zLPRdiGed6HUrwdx+rHEx9Euvniom/13Ks0oh8bLuv3049H0k6/tl4I50/we1KtwwUnGpt5eXllJaWUl5eTmZmJvn5+aR26kR2QQHXXXcdGS+9BLW1DBgwgP7nmrBrfX09tbW1VFdVkeH1sn3HDnrV1/P111/z6vbt1NXVJXyyAed+0S2W+1j3vadjAf5cdxYmuHs/Jn9ob0ygpjmiBmpefvnl8FO/FRdDnz58vmgRFMU5ubXXC1278uSTT8KMSGmFAlVXV5ObmwtR5pwX6WhsTDK3OzBPhIKTh4ZjYaLPs2icmDgSJ0GeM/7XxlxMl0VYD5iuxc7vN7n+vy6OdQeX2dHU4J+d6YbLL+cPwdO4PvEEXJKo1N6h5WGeuLo5Y3/dCQAh/DERzDkmws30lizDM15qwmeCkwKv9b32kuv9zTRm0fScV873fx7mSbR7v5yMSdoXbrs488yG1yKd17FuxwT8XYFDlWVhhqX+qRnrSWbOfnT2SR7mIUosn2v4vv79b/j3v+Mqy1lm+/btXH311fzAtqncuRPPnj10983I8ef77uPOp55i7ty5VFUFTz4s4eRi6kD3dSjctcxJohmNU9bhhK4PHLMIvF5HS2gbzSxMHqu+vt+fIHBa6vZoDTEk6MWfaPcEAicbGYw/4OZMSjCa8MmEd2OGXgRzX69DmUXgdx+rhmTCcX6uJRxK5OPVLZZ6PviaEXz+OD0XgtuVocpxJEv74WD25ptvsnr1aqqqqjjmmGMYPHgwaWlpWBkZjB49OqBThxN8KSkpYcmSJaR8/DFHVVZyxeWX89C2bWw+cIDPN8d6VB28NPRJRJrtQUyU+T1M197gfBhg8k/k459Obx5mXOvnwL8wSaBjH0xhbjSfbMrGdhQ9e8Lnn0dfrqYGhg+HF1+Eww6DNWvgO98x04B27tx4+Wuugb17TTDIcfvtZl0vvQTA6aefzqQvv+QmzFP+cMdEsHmYY+LhMO/bNL1XRyKdDnwZx/K5mJvtE/APF03HJBA+G5Nr61zf+249MefHBEz+i3hYmBlPcjH5Ma51veeci1dF2K70OXMajgfvCSfEvA9DmYN5enVnhGW+wMx85R7+1L7mIIrMxuzHGzGBs32YG6xINw9DCaxTzzvvPO77059MWRMmcOOOHVHLcr772zANYxu4++672fXRR5w+YQLfA8Yeeij/93//x7Bhw7j77rsT8vd2JMdjzp/DCX0tc87FnxO9h8o8zH5fhOl95ric8AFskWQQ6brYE1PHg8kzFG2Yq/uaEXz+uK+LNxLYrnRruJYR2EZtjjmYwJuTL9OdlDoUZ4KIDwjdvu2M6REUS4LbcLMxJZtNmzZxyy23cPPNN9O9e3eyQyxTunMn6z77DMuy+Otf/8rnn3/OYcXFHFlXx7vvvssBdcZoEHOgpri4mJtuCrxM5B84YJJ8XXMNZIfaFRHYtumef9998MILMX8stb6eR+vqGIzpaiQSiWVZDUmtDnYWcN9993HYCy+QMX9+TE/vnM9lYy6e58SxvhT8Fchx+IZBWRa/efBBarKy+Pjjj3nsMf/gmwrMECtnXGsd5qa71Pf7NuIb+tTUxFzt3RACnyyNee012BF0G792LZSVwU0xNO3r682/v/3N5CTZu9fUz7/9LYQa9vrll1BdHVj2559DSUnDaz/bvLlhGICF6UIey77tjjkmQuXnSSY7iW8bncSfJa7PpWNurHdi/ua6CGXuiLK+VEzCPPcF3cI/LnoCpqed4zBMT7ibfctdgxlTDea8ToGA4yGefRhKX0wDO9LfUO/bhmTf99H0JvC7BtM938LcVBzmey3Lt1ykQE0+gefPkC++gJtuMmWVlcVUlvPdl+J6+rx5M59VV1O2YgXfA9LT0xk6dChHH300xxxzDB999FHsf3A7Mp7ovVrGYPbhE0S/LqYTeP4UYs7B3xE4a4pzLu4i+vHtDE2pDVo2GQLUbeEYAveZM2V5tu/1UMe803vF2Y/ROPvxLvz70W0aZsYhCzPTZ3A92AUzXKI6ynaFMwRzrofb1l9h2kfJLtbr4jmY4bCR9MVco5y6MxOTkPUEGtenGTSuc8F1LcPkKWlO4L8Os4+d8npjekH9ltC5/BxZmCDRfYTeh9Mw6SDiCcIm+7FQX1/Pnj17+Mc//sH8+fM559tvmbJ1KzddcgkPVVaSDSxevJg/3WYG7S1dupRdu3Yx3Nebs6q6OuT582t8IwW2boUf/QgqKhrVz6HOxV6Y4UfuB2E30/h4ONT3ueDzcDFmKFNbiRioyQPqP/4YSkup37yZ3U8FdqpsuP39dzNSAL4XX3wzBZNRH0zldprrvR6Yk/m0oM+4pw4+AfO0YgQmOjmd+Mds1mKSxYkkm/N8Yz7p2pVZsX7ItmHuXE487DDoEUc6Ya8XXn8dbJsxgwczpm9f+OADLrzgAsjPx7btgECNmDrVXT9ZmCczEzH1kZPI92RCB6IGYyptd5f07Koq2L0b27aZO3cutXV19MXUcW8H1dmheDCNhflvvBGYBf/55yN/0FX2KGBIly7gmwWgc11dwFOULsTWW+pDmjYErqPzYK6Ln+GfycDy/VuICfS4jxln2uAuvt+dvFZOWRbA/v2Qmgr792MBF06dCmlpgSt+6y0YORL6Ru+0H3y9Dpb51luMGzmS3UFlDR4cadBA8snDDEeYS+AxPwfzxNkJnqUT26xs4Joydt068w/TK8cRrawhmF4fH/h+r6yqYvPmzWb4OlBZWUlOt26MGDGCU089lTVr1lBaWkp9fahJWpNHcH0aSV/fv1kxLu98nydihjctDrGMczY450+u7/cuNH7KPpf4e8V1dAvxJ813OEOtYzl/8ol9f0NgD6Za/PcH3/O9ZxP6WubBBA+cgE9X4gvUdPKVETzULA2T2P0PJP/NeazewuRWijasLhVTV3bFfy3L8f3u1KH5+K9locpzrmUfYXq+xCoVU68E70P3kDcnP9geCJjsJJiz3D5Ct+kqMEHdXZg6oqVyCMVjCuahWbBxAFVV8MornFxV1fD3OH+jc38NmPuKL7+k+ssvqcT8/U//4x/8GRNAKS4u5s0Q+Wad7z4H06501+/fx7Rr2bcPnjaDDE8Ms/0Op444PGiZaSE+B6audx9L432/J22gZiCYp6opKRR5vSHHXu4NKqwTsc/ukYd50ug+MHMxJ3FwxDozM5PMjAxsYN++feRgKlH3TuqEOWG/Q+DTB/cf+XtMxe/0Bng8xm11l1VH/BWxtA3bthOehCrZVfz4x9TMmhX7B7xe8gYO5MC111J3ahypCKuryevXD6umhqrp06m54AJyjzoq7u3tSAbSeAy789S2Dn+D5H5C1y+ZmK7F7vxDN5x9Nn/4wx+wvV4u6NqVffv2NYx5jyVPkZNr6AZCZ/qPxd3A9SNGwJw5ZpsmT+bYTz/lD76/4wI06xOY65uTKNe54cjB7FcPjZPo5rk+F2qqdIdT1m/x57PxAOsxT/eDc085+SecKSMvxz8dZ0PuqbvvhsmTzRC4MWNM4C4/31+IbUOfPnDVVRBDfXM2EZKV+8q6+qqruDpUWQsWRC0/mYQ75rfgT3pph3g/mAfTNb7Mt3x6WhrZvt7L+8rKyLLtgCloywjdDX8aJsfGGPc22jZ1vkDMli1b6F5QQJ8+fTjnnHP46quveOutt9i/v6mD3VpHqPo0kr0xLJOBOQf2Y84/C5NUN1TGLye3iXP+OLlNzkZT/SbC7SFeewgTlI50r+HM+FRPbMM1LUwdu5/A88e5P3Dn4Qt3Xt+PP0fNGcR3fzCL0Ndr5/g6WNiYIUqx9LpZgHnwcAv+a9mfgDcxvVmWYYbu3owJnv0wRBkZmBtv57o4wvd6bohlwZ94vRPRcz05QaBHiLyvnTadc+8ZrqyXMdfkZGgn3RPpzd27sc44I2TagYcifGwl/h6iYL7r4PZOJ8y5+7Lv9x8RekibWzmhYwhe/HVEZ0x9XOf7P5hzPdRjiFcwScUdyZCrK2KgZjFQ++abTJ40iVXLlzPmkEMiFnYa5uAuIHol5cFEEC+icUK0X9H44nv3bbdx3XXXUVNdTc/8fN6vqeEjTNc3xxzM9HfbCEzKdRr+HT8Jc8Achbn49iS+C6rzN4okq8svv5ynn469+eoBdtk2F51/Pq/FMUwsA9hr22QAf/nLX3j6r39ladxb27EsJrCHX3A9WIRpWIwmdGPmbkzdJe3TJ0G/W5iGp3PWhWqUW8SWyNcpK/g1SU77iN5WGoWZanSAb/lZF17I448/jg2M7tOHO7ZtC+hV4LRvgl1P5N4Hf//73znriiuYPHky/fv356STTmL+/PlJH6hZTGB9mgjOdzUWUzcfrDOQtVeXY3oeRLrXmI8Jmq0iMDgZTj5m2E6k8+cPIV6XlvW/mLw0YK5l/yQwIf4yIl87neXeDPp9Q4RloWl1czj5RD623GUdzIZjvgfnO74QE/R0a0p75QJCxxD24K8jlmICLguAj33LnUroB5PJ+Fg/eo4aywKPBzyeqH+A7fppA2lpaeTm5jJkyBAyMzM588wzOeSQQ8x03rZNzqRJPHT33ew/7jjq6+vZuHEjXS++mFE9ezIuI4OlS5dSW2s6GdohtsMm/Jdqh/n/q5jATBam983XEcoIJQcTkVvues0pa0mIskKk4pRW1JFy1LjF04vIi/9civdzDesDvLZt6osIvoO/C+pATAPoSsKfP5G4p3t+jfAB1xBZVtqU+28M+O7xf6fh6rZkvIhIdPsxjTG3dExCxB9iuuWeiXnQ4NYDk1z0eCKPr3eXtTjovViSFIZy/vnnszgri8E1Nbxi20w88kj2u2ZsAPiwpIQ//fKXvPT73zdxLbGVNb6ykigD8ZrsMmKfIc95ihqtvrEwT4ODn6AG576I1IaBxvVBQzvItrEtq9Fno5UXzplnnsnAgQOxLIu6ujpef/11ysvLm1BS62upOtGpmyX5xHIPELxsJNGuu9I2HsNMg+zBDIO7HnM9dF8Xr8EEUy8P8fng6+JgTI+JiYTuaXWXr6yfEjlBMMR+zMSyXLR1taalhM/J9b2zzuLOO+/EBo479lhKSk2Gyc6Y/eMJ8znw9ywK93tT/Q3Tq8bhDNmyQ/wMvmdpL+d6xEBNH8D717/Ciy/SY9euyN2hMCdBJqbblA2k1NeTWVlJwdatpKWlMerdd+m5bJmZXtu2oaaGPm+9BWvX4vV66b13L+mVlZy0cyd9U1PZXl/fcLE87tVXYccOUurq+H1dHf0x4+Hc2zTS9zM36HX3CPdBrv/XYbo2xjMKexQmgveo67X+mCnkwpXVspPktp4UTMKulBDvjcRElqMdI8dhui1GW24MQFkZ1vXXc/O+fTyGqXCl/ZoNAT1ursd0R11P5PMnnFH4z60XiHwjaxM54Zv4WZZFp06duPDCC1m4cCHffvtt0j9ZT3ZezNNdNyeZ8EbMNaQmxDLOt76OyN3F3WUFl9FUGzdtYhX+xteaNWsaPTmsxUz7vGr79mavL1JZLdX1+K+Y61GsMjFDyQZFWxAYFkNZTlspnIII7zVHVmYmQ/r04egBA7DeeYfh//oXOR98AJmZpNfW8v2vvuLEqqqweaUKduyA664zv6xYwRGYhMeOmYTvydAnYX+FSPKYiRlyEct5HWwMje9b8JVlYYYl7wrxuUT3JmtpFmaoUiwh4P6Ye43LfJ9Lx/SCGIIJDFiYXhnDMN/TZSHKSPWV4VwXG65lhO4Fs8+3/JqY/pqDUxXh2xDb8vJgxAiwbdalpTW0SZxeh3cTuR1uYYahZWHa/rPj3LYrMfeZFBbC9dcD8Objj7NihT8D0c00PhcLMQ/BDnOV9XMiDMV2OcJVDq6y3Nk8+8f5d8QrYqCmE2AvWwabNpFVUcGkKIV19RX4nexshg4dSlZWFh73E7jduxuSTQJmppFVq6jfto2a2lrS6+qgtpYhlkW31FR2ZWZyoMKkaeq1aRPU1+Pxeplo2+RgGljubXISI6YHvQ6mi9NE4Cv8iZ9qMWMe40nedBomsdif8FfEk4EfA38m9FO24+MoP5mlAFdjuu8F37Z1wfRwiHaM9MJUstGW6wFm2uBPP+WntbV8StMCNQdbjpoiTEVlYc61UZhx992cBRYtYuiOHRwdR5nBZcUqjcgR9GBzCUzCfSkmueV8Ip8/4ZyGP1DzBOG7lbZHR2C6iQYrwpw/7v3bf8sW+PhjsG0m1tVxAJPQLT1ouXCC92Nqaiq9evXikksuoaqqip07dypQIweleHvp5GBuFiI9CczGJCBcSOO2xRGYm4Y6zA3GJKLf0M0PUU5TeDwecnNysMrKGDVqFD0mT+akQw+Fykq6bd/OpoULKS0tpaq6miL8uXRCya6shE99Hcf37KE75m92DCZ8b+JOYV6XjiuH2K5VjpGY9ujRhD5/nGMvK0S5NfjzeCXKIkwbrD+mLRXLee3Wg9D3Lc4N2qGET1gbbjrsZFODGXYSy1A0IOAez8Ls7+GYHhNOL+nDMN97GuHvKT4htjxFEp8jjjiCnTtNKzWnrg4WLuTr8ePZGGEGaMu2qV+4EOrrKSks5NOhQ8MuG8qsJUvMJAcFBQ0PCv47bx6vuQI1U2h8LmYB/fDHCMCkF+gXwzqDjy+nLHeHhW8x1/WWEjFQswqoffhhJk+ezIblyzlm9OiIhZ0GPJuSwmXDh/P3//f/GDFiBJmZmaEX9nrxdOtG/Z13snfKFLZv386ePXs48qyz+HrGDN7u3ZuXXnqJb775BoC7L7+c66+/ntrqak7IzWVeTQ0fEpij5hVMpGsnZko/Nycp1/c5uG7o2sJPaTy27xXM9J/Reg/djblwBu+fYLOAJ7p1g48+Mgkrt7b3SVsT4/uY7pk1mBwxt2Oe1jfcOMycyQ11dfwiznIzMPumqVLxJ9yjuhqqqkiprfW/FoITIEq2oUltrRozFjuUVExQ5R33a7Nnw4svYgEvV1cD/mTp7zQqAVI8HlJTU/F6vdTX1+O17YALQVZWFocffjhjxoyhR48eDclLRTq6cuDYKMuMxPQcnEropKN34k86egwt0/3asixSUlLweDzU19djeb1kZWZy+OGHk/LBB1x//fUwYwZZWVnU/ehH1NfX87dbbuH5559n06ZNUcsfOWAAyz/2jfafMYPXX3uNXwPOHB53Ej6X3yIgcrZD6WiGEfpaFY4Hc/P0dpj3nTZF/6ByUzBP/BPdq+ss38/TaNp5PQvTlmvv9y3pELbNtx84KY6y5mGC1Ddh9vc2TM+i1zABnC2Y7/0OTCeBWIevSvNZlsVLL73kf2HvXujalWeffdbMAhmO1wtdu8K+fUyfNo3pTzwR34onT/Y/IAgj1Lm4DNNjfz4mTw2Evo8NxUkm7Bxfy31l3RvPdjdT9Bw1ccpIT+eqq65i4MCBZGREuk0zvvjiCz7ZsIEPPviAjz/+mG/27OGJJ57gacvC602mkXvSXtXX11Ptu4E9GKzCPEnYjrnAv465cK0HWLWKn918M/945pmYy/P4ympO0sSr8SV9s23oZ+LU36+vZ2aEz6RhhhEEjx3tyLYSeT/chenBd7zrtV9cfTV33nEHttdLv5492VdWxg+BW2k8RCMtLY3TTz+du+++mw8++ICnn36aT+bNo9S1jBOoSUkJNchRRJJZVlYWgwcP5qKLLuL444/nlVdeYfzbb9O3rIxX5swhs3dv6NSJnZWVfPXVV7z//vu88cYbrFmzhj179rT15ksH9BXxJcmfjrlZ6knogMgHmCfgq/BNKezjXBelZbxD4tpyaZgeiM4MPHqgJx1VwgM1qWlpnHrKKXTq1Kkhiatt25SWlvLMM8+w1dczwrJt7qiq4oknnmBuaiplZWWUlZXh9XrxEl+uCpFwvF4vW7Zs4WPn6d9BwMb0urAxw/dqcA0ZSkujLiUl7qlBp6enc+msWUyZMoUB/ftTU1NDRkYGlscTvpt/bS3WySdj1dXxPCZg9JxlwauvQk4Ob7z2GnfccUfYdT6PSe79rOtvkcjDv+ox35V7mfqUFMjIAK+XGstqmIYwVFkpqakce9JJZOfnc6C2lj0HDhAcwszMzGTcuHEK1Ii0Q5MmTeLMM8/k7LPPJi8vj169etGpthbrlVcaHp6tXbuWN9av58knn6S0tJQ9e/ZQVVWlh2PSJoKvadHUYYbGvB/mfafvf39MzwxHd9+/BUHLp2B6+D5P6Fx28eSx6sguh4TN/Pl3zGQvD7peaw+9ig5mr732GpMnT270ek59PW/aNt+/4AI2ZGWF/bwHeLO8nBzg1dde444QZUXy96VLOQRYv2EDF/o+u3LlwX9UJDxQY1kW3bp1MzMTANXV1WzdupX//ve/zJkzh5ISk2rIA9xWV8eW4uKAsV0HTzYRaWuWZeG1baqqqvSkMAKPx0P9YYfR6cQTYfx4VtfU8OKLLzJp0iTGjBlDYWEhtm03DJVpyPlTU0O27zwvxjXbzBFHQH4+pStXRuxaWIkZ1xlL90NJDI/Hw9ChQ7Ftm927dzeMMQ6WmmouDRkZGaSn61mWSHtgWRZ9+vThkEMOoXdvc3vZqVMn6NqVmpoaNq1bxyCvl/fee49Xq6v55ptvqKtLRAacttETMxwikY5r5udTgF/Q9IeNTgLp3gT+bcHbdTb+gAR0rBlG1wJ/jPB+P8z3UQF86Hr9EEw+mA+Dls/E9N74nNBJeydicis9Co0ebIjfUhLXnivHtCvVPkwepaWllJaWNnrd6QW++OuvIwbTLPwPEUtLS/k0RFmROEmoKysr+TTKEKiDScIDNY6amhpqamooKSnhvffe4+GHH2bLli3U1Ji4uUXL9ZpJxT8DlCPX93MwzZsSrC8myDQKfxe/gb4yRwKrSb7ZZTJp/H00hTPt6EAaj73PAeI75VqeZVlUVVUdVMOeEs3j8dCpUydOP/10DjvsMLKysvjqq6+45ZZb+MEPfsC5557L6NGjqauro7a2tqHXG4BVW8skrzchU+xJ67Asi7y8PGpqatizZw+73cndferr6ykpKcG2bfLz880sfdLi0mlcTzszCwwl8pC4SHVzKPmYG5kR8W2iJDnLssjOziYnJ4fq6mp27dpFVVUVebt24TlwgLfeeovL6up49913+ci223WQBkxQ4+IWKHddEz9XiWkDXtSMdTvneSGBf1vwDGAnYgIIji3ENxlAMmlKGzVcDiQwCb+LMPlo3HksZ2ES894YtHweZuj2Hwnda+MGzP74ZZzbKCJtx7lXT8e0pQa63ou3reTUT05Z4eorD6FnRm6OFgnU2LbNli1bWLt2LfPnz+eee+5pCNC0hh6YhD/BnXgtYE4CyreAJUGveTA9CiaTfBHgMSSuO6IFhMp+YuHLkZJEbNvmm2++6RBd45oqMzOTQw45hCuuuIJOnTqxZcsWKnwzrf3jH/9gxYoVFBUVUV5ezt69e1m2bBlVVVWAqbDKCJ88TpKP1+tl7dq19O7Zk7SUFFI9noYZnzyY87i6spIvFi1i+ne/y7AhQxg0cCCffvJJxHIbgnW+IJ6HwIC4FfR7e+85Gfz3NbUMd1lDMNetUN6LobxwdXO4ZcEMAVCg9eDh9XopKSlh7dq15OTkcM899/D1119z/qZNTN+5k5//7//yfUxPg6q23tgEWIbpCdESmnJeLCawl0tTLMfcBCzGzGDiuAH4vev3KzFDh4M19Xxuy3pgNIlro4L5W5T7ruMIbl+E41xzwy1vRXgvXFmSnGxMB4IlmP15bdD78baVlrp+vzZEeS0pYYGaTp06kWNZ1NbU8M/nnmPajBkcffTRDBo0iH79+vHII4+wcuXKhpvAlmZjLnbbfb/nARswTyBWN6Pc7wKPYMa+OjcbE4G3mlFmaziF5k9JmI55ajM9RFmzm1l2S7Btm5deeom33kr2vdM2Ro8ezSmnnMIll1xCVlYWc+fO5fXXX+e1115rWObLL79k8eLFDdOcK4dB+5Wdnc3QoUOZOnUqnb/zHX69bBk3+ALo6cCbmF6O1u7dpP/5z2T87W9Mra/nxNragHHioWQCfPYZdDETIL554EDDxcXCDHFz6sttJKaHX1tyvqvmsPB/75EuxMHXslAi1c2h/A0zVeVPMNdFOXi88sorzJ07l5SUFKqrq6mvr+dku72HRjueIwH3gO3gByLPk5hp2x3pmBmG2sJiAhPkN9fbmCD0rxNYpiSvqQSeK+E4uYjc7RG3LN/7sZYlyWsu/um4P8MEZhbiv1eP9Z7YaSudH1TWX6J8LpGRjoQFalJTU0nzeEixLCYcfjjZ2dmkpqZSVFTE1KlTGThwIIsWLWLhwoV88cUXbC0ujl5oM5VjnviDPyrmfq0pKjAneBn+E708/OJJ4wDN+7vBn3U9VFkt0oF61y6YMQN27eIa4FzXW30xU4Efj3+qxj9iKtgcANsm5YIL+NHatUzbsaPhpCmARmVF4pTFzJncsngxP3O9F9wVub3p0aMHY8eOZcCAAXg8HlatWsU333zD9u3+W0Kv19vk4MxJhH7i53Dvw6bo4fr/wzQ+D69DyefcevTowdSpU+nUqROeykqeralhTmoq3fLyeHjXLh7t25dv09LIyMzkiCOO4MwzzuCrxYv58MMPmTdvXsSyLwHOHjYM7rkHgFuuu44xK1fyI0w9eRmm3jgJOK9F/8qWtwYzy1lzpQH/Bm7B9Ho8EQLqF7eNRM6NEKluDqUGk7y7udcEST51dXWNhjQpTNP+rAaud/1+DvAj1+/3Ym4aEqmtelnVk9i6qB5TXx6I83Oh2hFgUiYUEbk9A6ZN0sm3XDznXKKnC+9I7sNcR2PRD5Oc2GmPBLsGM8zltzGWZ4cpR1pWX6Kfi25FwIWYjhaO3xJbQO5QTFvtOVdZVbRu2ylhgZra2lpqPR48KSn069uXtLQ0PB4PWVlZ9O7dm27dutGzZ0/69+/PoEGD+OzTT0ldtAjqNb+ThDB0KBxzjPl/Sgr7CHyi7O7Bk4KpMHdhxiQ7UdRn33mn0UwC9dCorEi65OfD974HwN4VKwI+9yzmpq09siyLnJwcCgsLyc7OBkyCroqKioTl9OkLdAX+Geb9OmA/se+LYO4Utzvxj89PAX4A3N7Ecg9W2dnZDBgwoGE2p9QxY8gaPZq8ggKshx8md+pUuvbuTVpaGlljx8Kpp7I7M5OlGzbwepSyjwfIz4fp0wFYcPvtAReXtzDnXSHtP1CzD6J+H7FIx3TN74oJCKdikqOG8v0oZaVijvsZmDHZ0QwldNLMSC6icf61bOAYEvN0MVJZgxNQvkh7spfAemZM0PuLSEw9JCZw/Tjhh7wUYtor0doq6ZibumlN2IY9mAcebk4Pj7MxPVFjsRo4eOY4jW4JjVNRhOP05HXaI8HOxVyPdV4lryXEPzrEuddw9xh07hejGYZppznn/mzgmzjX31wJC9RUVFRQDti5uVRVV5Oank5qaioejwfLssjIyGDUqFGMGDGCk046iTffeIP0r78mtaaGFEwCS5EGRx8Njz1merPMncvjFRVhk8elAxcAd2LyA40EzsQkhwuujKdiLsiREtG5jezVi+899hgA98+YwWubN8f3dySp1NRUsrKyzIwgPt26daNHjx7k5uZSUVGRkESTezFPL0I5CvMk5N4mln0acLLv/7/G33vGOR4kUFpaGp07++cG+e53v8uUn/yErp06kfLoo1z0/e/jnTgR27bxeDykpqaSm5tLfn4+WVlZVFYmW5r09q8YkzA0B/Mk9rZmlJUC/A+xT3Mb6xDgOmArjRNwgknSfxYmeWdzRStrC5omN5qemAZpsDxMY68PpgdqN8yTwSavp64Otmwxv1RV0Qno1YzyRNpSJabuDOcGTD0dri3jcLdJthN/T/Pg+t8JHF0ZY1kFmAdjHSlQIx3LG4Rui0Ti3GvMxzzMAv/9YjRPYIJ30c79lpTwZMKVlZVc94tf8NPLL2fYsGHk5+c3vOfxePB4PHTr1o2LLroI66qr6NuzJ0X19WzZskX5LySQxtW3mIyMDLp06UKPHj2wbRvLsrjgggsYN24cb7/9Nk8//TTffvutAqgHkW7dunHsscc2/N69Wzfy+vRh744dYNusXLWK+txcunTpQv/+/QGYOHFiwyxQ//xnuL5R0hQ1+HuKODcCTU1E6iT2PoP4ktlHmknKsQbTOy6ULcBNxB74jiSWslQbRfZ+lPc3+X4+2dwVrVkDfc1RYWOeRJ+LhliJOE6g+UOv8zC9AGIt65Vmrk9Ekk/CAzW1tbW8+OKLvD53LuPHj+eEE07gvPPOIzs7m5UrV1JQUMDgwYPJTE/HAn57222c0asXL7zwAo888kiiN0da0UxMgqZI8vAnvowke/ZscJIAb9/OfcAdYZZ1knJK7Gpra5k/fz6WZfGDH/yACRMmkJ2dzaGHHsqwYcM466yzWLJkCbNnz2bBggWUlibb5OsSr5SUlIAeVPPmzePVHTt47403WFhXx//dcANrunVjwoQJDUEZj8dDQUEBI0e29/S/IgcPy7L44osvyHUC6WVlMHo0vP8+DBnS+AMPPQT//je89x6MGgV//SucdFJCtmXWrFm88847Aa/FMva/PQnXtnFmbPsAk/MpEQoTVI6IiLR/LTI9d0VlJQcqK/niiy/Yvn07n332Gampqezdu5dDDjmEU045hZNPPBGA3Lw8hg0bxjHHHMPzzz+PZ/9+9aRop74AHouyzHmY/Ajhgi6OKRMmcNmll5pfrrmGLuecQ5ejjw69cF0d/Oxn3HzjjZQMHkz+1q1Yv/41Dz34IDVZWQGLdrnmGi475xyOD1dWEHePsINJbW0tGzdupKamhtraWubNm8ehhx7K8OHDGTBgABkZGeTl5ZGamkqPHj144YUX2L27reaEkETYunUrs2fPpqioiBPKy/nqq6+Ys2YNJZs3Y9s2e/ftw+rencLCQmzbprq6mg0bNrBu3brY8hatXQuzZgFw87p1DTccFvAQpgfJECAf0520uULcjop0GD17ujIaOQHYwkIoCjGoKTcXUlOhd2/weKCgIPRyTbArI4OtCSkpeWX7/oXTFsGVazAJhlvKFMxMU05draFtcjBwt0eCBR/ziZBObNN9i4TT5EBNQUEB/fr1Y9euXZSUlFBTUwNBQ5d2797N7t27+eYbf+qdrVu3Ul9fT68ePRjjWz4zM5OCggI8nsTOTH8y/uRBznPkEzCBgqY6EvOlnYa/m+8I1/tTgO5Bn+kBdIT+CGuJ3gV+NOZGLdpy1pAhXDZrlgna3XSTyVnjuwlspLoarriCabm50LUrHDB52C/s0gWyg5pXHg9TOnViSteuUbbAZc4cACbu2EG0IzQHk8wz2Stmr9dLWVkZlZWVVFVVkZeXx7p16zjssMM49NBDyc3NZdiwYRx99NHYts2yZcuYP39+wrdjFOZcaoojXf93n9dpmCedoc5FgHFNXF97t23bNv7zn//Qs2dPDi8rY1tZGWsJ7I2Wl5dH794mE8iuXbuYN28ey5YtY+vWyLdiy4GP6+qY4gvmda6rC7ix6YJ54twJs2/iOPvC2k37TeYt0hSbN29mju965JZ24ABTgXnvv8/+NY3PiiErVtC3vJx5r7zC1Npavly4kB0JGmq+Y8eOhJSTjOZiri3VmCmkg6Vh8t7NI3RuoOYINdRlv29dUzFtjUTUo+FkYP4+Zx0fAutacH0iLa0cMzwsJ8z7wcd8orxC+5gdWJJTkwM1hx56KD/+8Y+ZP38+c+fOZfv27XgOHACvlxSPh7owjYBVq1axa9cuqisr+VNNDRlAVVUVu3btYv/+/djN6E1zADN9djbmRvl+AsdMl2ESCDVHKuZmIzjQsBczrOc3hB5Hn+hpFA92NTU17N27F4Bcr5eqykpqfL834jztv+EGwNwIdgbKLrig0Zj5XKDq/vupuf/+uLfp5zEs0xk4H9Owa5EpyxPM6VkDsGbNGt566y0GDhzI2LFjufXWW+nRowfjx49n6tSpLFiwoFnnZ7AyTPLQs5r4eXfl9XtMZnb3e+HOReCgfwIcys6dOxuGKPwqzDJpaWkNs4CtWbOGp556ioULF0Yt+0lg1YgRLPDdRN4weTLHfvopf8DUwRdgEnvPwszGdUZz/hCRg4CFeWgRT7jk83ff5Yfvvtvo9TxgPXDjFVewKsTnrsKcgxefeSbrgd/dfjuvxb3FHYtTb72CedAWPCMP+HOIXE7z85HEYjOm7twN/A5adB86STRVV0tbySR0HjXntVzimx67DPhhhPcfwgRxOvIxn0Jsuevc4l1e4tPkQE3fvn0555xzOPvssznvvPOYPXs2nd57D8/KlYwcOZJVa9aYXjZB6uvr2blzJ//5z3+4u7aWDEzvnPHjx3P00UeT/umnEOJzsTgf+BFmVh8b03sj0Tdkp2GCNAUEBoEyMMGaUwmdzFGDueLz3HPP8fzzzwOwxbb51eWX8/QVV4T/gCuAMAozhdsAGs/6tAVzk/p0QrfWbxmmgXMvJjh3SAutpyVUVlayefNmtm3bRnFxMZdccgnp6el0796dE044ga5du7Jnz56EJf2e3MzPnwa87Pv/JPwN5WjnooSXmppKerrpY7N8+XL270/0c2IRAXOTsTOB5VnAJxHew7e+ZO/tKSIC8L+Ef0BqYdrbiWShhMzjCJzGOla6rrScZueosSyLcePGMWjQIFLGjiX72mt58qmnWLtuHZ999hkbNmygV69eLFmyhDPOOIPCwkIsyyIrI4Osiy8GID09nf79+3PfffeRO3UqNCNxqR30/0QHSGzXT3fZ3jCvS9M5vTfsoN+jibQv7KCfLaG9HwPp6ekce+yxFBUVkZWVRXl5OZaV+Gq4ud9RuHNd52LT9evXj4kTJ7b1ZnRIg4EVTfysk1D9n5ipZmPlJEMNJZayCoG7iH+6zHBltabRNP37jiSWxPbvAYlMz90ZWIiZevTbEO9fBpyJf2rSaMn8RUTa0vlAVpj3emCGAB4PlCRwnXdhepR0ZMtoeu6r9XEs+z3if1jbHxhG/D2eBgLXY6a5b4+aHKhxhisVFBSQlZVFVlYWdO+OJyWF4cOG0aNnT/r378+ePXvo3Lkzxx13HKNHj26YdSTV4yElJQUbM7NIRkYGQ4cOJTWlo58mIm0nOzubQYMGce6555KTk0NVVRXFxcV8/vnnVFRUJHTokySf9PR0srOzsW2b9evXc+BAPB2LpTn2Ao828bMpmATtc4CNTfi8u7Fbjwm8xHIlvhkz483nTVhnKIsSVE4sdtL07zuc4zHBkGh9Dg9AyCFKTeV0PV8fptxSTPLMRK5TWl8P4B7MDeylmOOtpRyBctJI29kU4T2nn+86EjtqYh8tm/epPaiida4TeTRtyFQWZkRLvD7DXB+dHlo/B86O4XNHYIbg3dOEdYJ5iNJcTQ7U7Nixg08//ZQTTjiBrKysgCfumZmZ9MzODpyVIJjXi2VZAU+8E51MWDq2iTQev5qBSTob25xP8Qv3BKA9yMzMZMCAARx//PEcd9xxZGRksH79ehYvXsx7771HZWX45+t9gMOcXxYuNLOQrFlDOi33XWvC6MRK8XhITU0lNdVcFjZs2EBFRUUbb1XHsQv4YxM/mw78FniO5g/3qwf+HOOy12ASrkZLDp+MdtD07zucFPy9VkQSaQumB9gkzHE2nNDJ8hNlDy3T40xEOq6F+Cf3iddYTJ6hDU347HPAN8B43+/9fP+i6YZJMD2pCet0zMfkLW2qJgdqFi5cyK9+9SueeeYZhg8fTlpaWpPHqNm2TV1dHZs2bWJIfbj0n3KwsDABk3iWT43jMxmYLv1vhnn/BuAXcaw/Xsl2BFuWFVNPmL59+zJjxgyuuuoqMjMzqa+v55NPPuFf//oXr776atjP1WAi0+eAyRV0hq9jYn093YB3EvA3hOLBDG2qpv0PcUontuM7hcbnT2pdHVRVgddLhm03zFxAhDLd55SzTKdOncjNzSUnx8yJsG/fPmpra+P8Sw5+8dZfsWj2GGQRaTOx1t/N8aLvnwfYhmnHKCF0eKHajNGui5HKiqXeT4vy/sGuNc4DWmkdkng2Zsa6plqAmX2uOUOtj4lz+WRIqt7k9mFlZSXffvstd911F9dccw3Dhg1rcubnAwcOsHr1ambNmsX7e/Y0dZOknZhI4yS/kaQDfwMejHH5aAHDnwH/iGP98Uq22Z6GDx/Orl27KI2Q+6lr1678+Mc/Ztq0afTo0QPbtnnvvfd49913Wbp0acTyT8JEyUcCiy0LNm2CvDx4+mm2/vjHDEroX+M3HZM4vCcmWNSezSa22V+cBqP7/En585/hgQewgE2+ROwpvn/hzrM0zKwht/p+TwdGjhzJyJEj6dGjB16vl759+7Js2bKG2dfEGE989VcsUoDVCS5TRFrHIlr3YUEseZA6uqtpnJMi2nUxnBRMgCza5zr6mIB3iG8Wu6ZS4lrpSJr1IK+qqop33nmHbdu2ceyxx3L8/v2cGMPn6uvrOVBeTj7w3//8h/fmzmXhwoV8++231NUl222uJNIDwH/i/Mwc4GHg9QRtwzra/429417gJlwNt9NP59ebN/MT1zJZGzdSV1tLpL4RqWVlFD3wAF2efRZPZiYAR+7dy+CdO/nfsrKQ3fYszA3/g0A5rmFf6emQkQG+ITQt9V07NUUN7bdHjQ2cQuwJ7K7EzCTm3r8XXXABV1xxBbbXy4xTT6X8wAGmA78m8pMnD4EXgKuXLyf7rrvgscfwADetW8cVe/fGvP86f/MNTDbp4f6+dCm5MX6uvVlF6Kl6myueJMAi0vbKabmhvdG0xnTg7dWPMdMsB5uOuXaeHmd5FwHTgAtjWDaH8L25D2YraJnrYiRNn3ZGpP1oVqDGtm12797NV199RXl5OSnAlOpq7r/3XuwIs8R4vV5qqqq4sbqaTz75hLfq69mwYQP19fXt9oZLYrOJyEnCQqnBBFc01XKg+zFJrsAkrJoA8OWXDAGGuBeMkFumQV0dbNxo/vl08f2LptEU5PffD5mZ8MUX5GC6aLeEgyVHTTwJVL+HyWDvPheOLSoyARKvl0WpqewDRmAS7j0UppzLMFPYf+Z6Lbu+nnE9etBpwgTK9u3jrWXLOFBfH/NQvqLcXMYceywAX2/aRJ/ycjphErY2Z3xusjlA8tZFFwHHtuL6EpEo72B1FSYxo1tnzA1fjwSvK5OO+ZS5Jy13fXEMJvwNYT3JWxd0ZOH6AI/AtCfj3WfHYgLpsXyuqSML2rtkvi7KwWMiLV/nu40hsQmrmyIhQ+PLy8v56quv6INpmPzyV7+KGnCxMF0TV61erczyInGy8Q9bAeidmsrlQ4cCULx1K/v2BXbStSyL9PR0unTpQpcuocMvFRUVlJeXU1ZWFlNuEguT0HAjrt4Ats3w558nxeOBffvIBi6O6y+Lj54qhldB+CSv5wBfBL3fJSMDDj2UUeedx65t23jh7bfZWVlJrTe2zszjCwu56GKzt19/800mbN1KIfDLpv8JEiMb80TzhFZe7xbMbFXiV415sDAzxHtZmB4YY1pgvSs4eHqKxqqAlr2+gDm32rqhLm0vndgeDjk9SQfT/ODpwdorNZl1pvUfAg5u5fW1V+uBcbR8nR9scyuvL1i7zmFo0bgitKK8n4h1iiSdwkJYvhyAm2bN4qmnAm/Rs7OyGDJkCJdeeik/+clPSEkxg23ciYY/nz+f1157jX/9619sdPWsCScdk4H9QvxPUizLYvenn5Kfnw9PPEHJJZcwOjF/4UGjOXVIrJ+1MTOChHuyaAHX+v412LsXHn4YHn6YLsDb8W7c4sUwxtyCPuM7ppbHW4Y0SS3+2Qykba2DsHXeFsyQxJacJStUHXGwtluWYaZPFWlJNjCI8NfTYBZmyL60LzZwHLHv50Ty0n6H8LeW77f1BrSRdh2ouYrAXA3gz9VhYZ4wJTqxVRod76mVtH8VFRUsXbqUG2+8kdtuu42cnBwmTJjAhAkT2Lt3L8888wwHDhygpqaGmhod4S1lHGba06bKxPSEieY54L8R3v8MeAb4SzO2JdjEI4/krbfeAuCUk09m0aJFSTcDmkhbe4jYpz9PlAzg21Zep8jB4n7gkRiXzcNMHzyRxCWIL09QORLZ5ZiRHm2lrA3XLckr4YGaTsCrRI8MWr5lb8HMwuMoAK4Bzg1afsTjj8O8eaR6vbxUW8sIAqeWDWZjTrrdcW5/LGpR5FPaH6/XS2VlJVVVVezfv5+PPvqIb775htraWkpLS6mv1211S1tP8xsCsQR6an3/wvFihqkmsmFQnpJiZvsCDqSkqNEhEuSH+POKtTbd7Ik0TQ2xP6B1eq+Voxvv9qaKxnnFRNpazIGa/Px8Lrkkck7vQzE9TqbFUqBtwzPPcMRxx0G/fv7X//lPDp0wgUN9+TYaqa9nh2XxHzt8qKQTZnz4e2hscUuYAYwKeq0vkE/LZH3PboEyOzLbtqmtrWXnzp3s3LmzrTenQ9lH4mYvC+Wiiy6iMobk0XkvvsgRw4ZxySGNUkE32eDBGmktEsl7bb0BIiIi0m7EHKjp3bs3jz32WOLW7PXCf/4DV10FM2aY12wb5s6FSy6BWbNCfqyuupqfPP98xOEZvQmdyE+ar5jQiZwKME/pB7TAOg/4/olIeB6PhwceeCC2hRcs4Lxzz+W8669v2Y0SERGJIA0oaoX1xDKLpYhIMoktUBOh90pCBJVvt8Y6JW41hM9O/gpmCsuW6FEjIpHZcdaXFqpnRUSk7Y2l9WZWKW6l9YiIJELEQM1YgNNPh/T0SIs1jW1DWRlcfDFkukZtb9/ONddcw79vuinMx2wlOxWJ4L777uOOO+5o8fVYNTWkDx/OnBdfpOaww8xrlkVuriaVbE0PPfQQzzzzTFyfmVdSwj9vv52H//znFtmm0tLSFilXREQOLkuIMWVCAigTn4i0JxEDNcXAbS2Uw8ICHgQe2b2br4Pe+3jvXrbu3dsi6+2obgZKWrD8Q4F3WrB8iV2XLl3o0qUVOvlWV4Nl0b1bNyhqjY7LEkp5eTnl5fGlCq0DysrK2FqmdIciItJ2alE+SRGRUCIGanYBT7XQii3gPuBd4LUWWocYezE3Zl1bcB1fAYtbsHyJrKqqijlz5rTqOj21tUzzepn/8cfsCdGDou/ixRS26haJiIiIiIi0fxEDNSlAXgut2IN/GjtpWWuBM9p6I6RF7d69mzPOaN29nI6ZfvKGG27g0xDvzwJub9UtEhERERERaf8iBmrGAbtbcOUK1IiIiIiIiIiI+EUM1CwDzmnhDdjSQuVawAeYIT+twcL0MBBpaYOBFW29EeiYFxERERERaQkRAzVVwKpW2pBEKgOua6N1b2yj9UrH8D4Qej60tqNjvn2aAfRo5XWOaeX1iYiIiIi0RxEDNe1VOfDHtt4IkRawyPevvUgHjm7ldaZhcmBJeF8C/YFJbbDu9nT8ioiIiIi0hYMyUCMiyaE78HFbb4Q0clFbb4CIiIiIiIRl2bYd9k3baMXNEYdlWdTU1CSkrPT0dLQfW5/24cFB+7H90z48OGg/tn/ahwcH7cf2T/vw4KD92P55PJ46zICARiIGarxer11bW9tS2yUiIiIiIiIi0uFkZGR8ARwe6j2lchARERERERERSRIK1IiIiIiIiIiIJAkFakREREREREREkoQCNSIiIiIiIiIiSUKBGhERERERERGRJKFAjYiIiIiIiIhIklCgRkREREREREQkSShQIyIiIiIiIiKSJBSoERERERERERFJEgrUiIiIiIiIiIgkCQVqRERERERERESShAI1IiIiIiIiIiJJQoEaEREREREREZEkoUCNiIiIiIiIiEiSUKBGRERERERERCRJKFAjIiIiIiIiIpIkFKgREREREREREUkSCtSIiIiIiIiIiCQJBWpERERERERERJKEAjUiIiIiIiIiIklCgRoRERERERERkSShQI2IiIiIiIiISJJQoEZEREREREREJEkoUCMiIiIiIiIikiQUqBERERERERERSRIK1IiIiIiIiIiIJAkFakREREREREREkoQCNSIiIiIiIiIiSUKBGhERERERERGRJKFAjYiIiIiIiIhIklCgRkREREREREQkSShQIyIiIiIiIiKSJBSoERERERERERFJEgrUiIiIiIiIiIgkCQVqRERERERERESShAI1IiIiIiIiIiJJQoEaEREREREREZEkoUCNiIiIiIiIiEiSUKBGRERERERERCRJKFAjIiIiIiIiIpIkFKgREREREREREUkSCtSIiIiIiIiIiCQJBWpERERERERERJKEAjUiIiIiIiIiIklCgRoRERERERERkSShQI2IiIiIiIiISJJQoEZEREREREREJEkoUCMiIiIiIiIikiQUqBERERERERERSRIK1IiIiIiIiIiIJAkFakREREREREREkoQCNSIiIiIiIiIiSUKBGhERERERERGRJKFAjYiIiIiIiIhIklCgRkREREREREQkSShQIyIiIiIiIiKSJBSoERERERERERFJEgrUiIiIiIiIiIgkCQVqRERERERERESShAI1IiIiIiIiIiJJwrJtO9L7pcDGVtoWEREREREREZGOoD/QPdQb0QI1IiIiIiIiIiLSSjT0SUREREREREQkSShQIyIiIiIiIiKSJBSoERERERERERFJEgrUiIiIiIiIiIgkCQVqRERERERERESSxP8HOQhbf0CVA8oAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1440x288 with 11 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display_digits_with_boxes(img, label_preds, labels, bbox_preds, bbox_true, \"Un titre\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMH-Cp1MPt3F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPybzykcPt3F"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "YOLO_MNIST_Localization.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "bfc2e043f4a41bbce066f7921047bdcfe8769546c7198e4df6482fd90de274f1"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1082b52862584f47ac371b7c37e45f32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_536017ec05ad45b28cacb6abf83795cc",
              "IPY_MODEL_2debc45cf0ee4ecdbb84ade1ab39615b",
              "IPY_MODEL_1c871c8171144624a9c857c8e4354a7a"
            ],
            "layout": "IPY_MODEL_db27cee5aa524f5c89ee9ef3c50a0e60"
          }
        },
        "536017ec05ad45b28cacb6abf83795cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f398dda6e744f01904c796703da58cd",
            "placeholder": "​",
            "style": "IPY_MODEL_dee265ee475f44329263cd7b22ee1637",
            "value": "100%"
          }
        },
        "2debc45cf0ee4ecdbb84ade1ab39615b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_628dc217877c4f539974877fa3d87696",
            "max": 9912422,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8635c9299fef4220aab7ee1765e2709c",
            "value": 9912422
          }
        },
        "1c871c8171144624a9c857c8e4354a7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dfc26754d20455ba0da75e33218a36e",
            "placeholder": "​",
            "style": "IPY_MODEL_393d0d522b4743e9a04a4e3d708f529f",
            "value": " 9912422/9912422 [00:00&lt;00:00, 9161291.15it/s]"
          }
        },
        "db27cee5aa524f5c89ee9ef3c50a0e60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f398dda6e744f01904c796703da58cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dee265ee475f44329263cd7b22ee1637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "628dc217877c4f539974877fa3d87696": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8635c9299fef4220aab7ee1765e2709c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2dfc26754d20455ba0da75e33218a36e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "393d0d522b4743e9a04a4e3d708f529f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4104399bac454257be472b40203ef926": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3664bfa18d95441bbf8a0875cc2e8f56",
              "IPY_MODEL_b7243342e8e9473e8695e687276e9eac",
              "IPY_MODEL_183da87c5a514626bff112af3cbbcffa"
            ],
            "layout": "IPY_MODEL_d5481277c0294251a3b141983ec2950c"
          }
        },
        "3664bfa18d95441bbf8a0875cc2e8f56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15636d229dbf4a54b9f11faf54658dba",
            "placeholder": "​",
            "style": "IPY_MODEL_498e99190ffc4810b5150c07da2e1b3d",
            "value": "100%"
          }
        },
        "b7243342e8e9473e8695e687276e9eac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08133968c3a24449b92b80f7eb61e4d9",
            "max": 28881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88f47dba11ff48009ad3440061de6ee2",
            "value": 28881
          }
        },
        "183da87c5a514626bff112af3cbbcffa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05564490bce74bedaf40cbc368909fd4",
            "placeholder": "​",
            "style": "IPY_MODEL_889032b33cd548cabbce84d692a18a9a",
            "value": " 28881/28881 [00:00&lt;00:00, 731005.75it/s]"
          }
        },
        "d5481277c0294251a3b141983ec2950c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15636d229dbf4a54b9f11faf54658dba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "498e99190ffc4810b5150c07da2e1b3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08133968c3a24449b92b80f7eb61e4d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88f47dba11ff48009ad3440061de6ee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "05564490bce74bedaf40cbc368909fd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "889032b33cd548cabbce84d692a18a9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec5a25f7484a4bb0b6c404a183f78343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_deb656adb8114376b067f10b37f6232b",
              "IPY_MODEL_ed55481101c446ac900f073ed6272749",
              "IPY_MODEL_0fc8eb581ed9457aacb56dc01a9dc9e6"
            ],
            "layout": "IPY_MODEL_aa921fc6bdef41049de138c4678ef731"
          }
        },
        "deb656adb8114376b067f10b37f6232b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65d9876405d94ac09981657509957cc0",
            "placeholder": "​",
            "style": "IPY_MODEL_6b5cf599b6f14f59abbd59cdfeb2757c",
            "value": "100%"
          }
        },
        "ed55481101c446ac900f073ed6272749": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b0a6f743129427d8be2e358ac90c848",
            "max": 1648877,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60149f89d4684950b8de058c6479e2ec",
            "value": 1648877
          }
        },
        "0fc8eb581ed9457aacb56dc01a9dc9e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57f9abfdefd74ab6b4806bc500e7f8c1",
            "placeholder": "​",
            "style": "IPY_MODEL_54ecc4bec948431d8202cafaedffe8f6",
            "value": " 1648877/1648877 [00:00&lt;00:00, 15007992.45it/s]"
          }
        },
        "aa921fc6bdef41049de138c4678ef731": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65d9876405d94ac09981657509957cc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b5cf599b6f14f59abbd59cdfeb2757c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b0a6f743129427d8be2e358ac90c848": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60149f89d4684950b8de058c6479e2ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "57f9abfdefd74ab6b4806bc500e7f8c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54ecc4bec948431d8202cafaedffe8f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "858bd330763a4c71b0c9287f2e5ad46e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f78eed65f4724831a6dfcf5272b25993",
              "IPY_MODEL_6ce32bcdc7714b84ac70f7a48835cc49",
              "IPY_MODEL_26b940b2d66a4cb59306b2a3abb8b6b5"
            ],
            "layout": "IPY_MODEL_cd8c25d30c8e46239415fb04da6f9bb5"
          }
        },
        "f78eed65f4724831a6dfcf5272b25993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1b608ab95554c9aaf779b61ca7c5cbf",
            "placeholder": "​",
            "style": "IPY_MODEL_699a5bbfd9ed494ca22f5067871ed5b1",
            "value": "100%"
          }
        },
        "6ce32bcdc7714b84ac70f7a48835cc49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b183fa1ae8ec4e9ebb00943f16fd4752",
            "max": 4542,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_adaba7d4206e4a68a02229712d9de299",
            "value": 4542
          }
        },
        "26b940b2d66a4cb59306b2a3abb8b6b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85c6e81be5de4a2a92f92fa28606274e",
            "placeholder": "​",
            "style": "IPY_MODEL_8b77517829854981acb8f1bf7eb9307c",
            "value": " 4542/4542 [00:00&lt;00:00, 141960.48it/s]"
          }
        },
        "cd8c25d30c8e46239415fb04da6f9bb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1b608ab95554c9aaf779b61ca7c5cbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "699a5bbfd9ed494ca22f5067871ed5b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b183fa1ae8ec4e9ebb00943f16fd4752": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adaba7d4206e4a68a02229712d9de299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "85c6e81be5de4a2a92f92fa28606274e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b77517829854981acb8f1bf7eb9307c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}