{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThOpaque/Food_Recognition/blob/main/WarmingUp_with_MNIST/YOLO_MNIST_Localization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAHeh1BRufX4",
        "outputId": "4cef43db-1d01-46ed-9300-5b6200fec7c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchinfo in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (1.7.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: torchmetrics in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (0.9.3)\n",
            "Requirement already satisfied: torch>=1.3.1 in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from torchmetrics) (1.13.0.dev20220730)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from torchmetrics) (1.23.1)\n",
            "Requirement already satisfied: packaging in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from torch>=1.3.1->torchmetrics) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/thierryksstentini/opt/miniforge3/lib/python3.9/site-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "import os, time, datetime\n",
        "from timeit import default_timer as timer\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "%pip install torchinfo;\n",
        "%pip install torchmetrics;\n",
        "from torchmetrics import MeanSquaredError;\n",
        "from torchinfo import summary;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOraK1TX7XZB",
        "outputId": "259d5789-4ab7-48b9-f310-4505121f5f11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------------\n",
            "Execute notebook on - cpu -\n",
            "------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Choosing device between CPU or GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "# elif torch.has_mps:\n",
        "#     device=torch.device('mps')\n",
        "else:\n",
        "    device=torch.device('cpu')\n",
        "    \n",
        "print(\"\\n------------------------------------\")\n",
        "print(f\"Execute notebook on - {device} -\")\n",
        "print(\"------------------------------------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "M7VztqFE71JZ"
      },
      "outputs": [],
      "source": [
        "class my_mnist_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root:str, split:str=\"train\", download:bool=False, S=6, sizeHW=75):\n",
        "        if split == \"test\":\n",
        "            train = False\n",
        "        else:\n",
        "            train = True\n",
        "        \n",
        "        self.dataset = torchvision.datasets.MNIST(root=root, train=train, download=download)\n",
        "        \n",
        "        self.cell_size = sizeHW / S\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def _numpy_pad_to_bounding_box(self, image, offset_height=0, offset_width=0, target_height=0, target_width=0):\n",
        "        assert image.shape[:-1][0] <= target_height-offset_height, \"height must be <= target - offset\"\n",
        "        assert image.shape[:-1][1] <= target_width-offset_width, \"width must be <= target - offset\"\n",
        "        \n",
        "        target_array = np.zeros((target_height, target_width, image.shape[-1]))\n",
        "\n",
        "        for k in range(image.shape[0]):\n",
        "            target_array[offset_height+k][offset_width:image.shape[1]+offset_width] = image[k]\n",
        "        \n",
        "        return target_array\n",
        "\n",
        "    def _transform_pasting75(self, image, label):\n",
        "        ### xmin, ymin of digit\n",
        "        xmin = torch.randint(0, 48, (1,))\n",
        "        ymin = torch.randint(0, 48, (1,))\n",
        "        \n",
        "        image = torchvision.transforms.ToTensor()(image)\n",
        "        image = torch.reshape(image, (28,28,1,))\n",
        "        image = torch.from_numpy(self._numpy_pad_to_bounding_box(image, ymin, xmin, 75, 75))\n",
        "        image = image.permute(2, 0, 1) #(C,H,W)\n",
        "        image = image.to(torch.float)\n",
        "        \n",
        "        xmin, ymin = xmin.to(torch.float), ymin.to(torch.float)\n",
        "\n",
        "        xmax_bbox, ymax_bbox = (xmin + 28), (ymin + 28)\n",
        "        xmin_bbox, ymin_bbox = xmin, ymin\n",
        "        w_bbox = xmax_bbox-xmin_bbox\n",
        "        h_bbox = ymax_bbox-ymin_bbox\n",
        "\n",
        "        rw = w_bbox / 75\n",
        "        rh = h_bbox / 75\n",
        "        cx = (xmin + (w_bbox/2))/75\n",
        "        cy = (ymin + (h_bbox/2))/75\n",
        "\n",
        "        cx_rcell = cx % self.cell_size / self.cell_size\n",
        "        cy_rcell = cy % self.cell_size / self.cell_size\n",
        "\n",
        "\n",
        "        label_one_hot = F.one_hot(torch.as_tensor(label, dtype=torch.int64), 10)\n",
        "        bbox_coord = torch.Tensor([cx_rcell, cy_rcell, rw, rh])\n",
        "\n",
        "        return image, label_one_hot, bbox_coord\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        image, one_hot_label, bbox_coord = self._transform_pasting75(self.dataset[idx][0], self.dataset[idx][1])\n",
        "        \n",
        "        return image, one_hot_label.to(torch.float), bbox_coord\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423,
          "referenced_widgets": [
            "1082b52862584f47ac371b7c37e45f32",
            "536017ec05ad45b28cacb6abf83795cc",
            "2debc45cf0ee4ecdbb84ade1ab39615b",
            "1c871c8171144624a9c857c8e4354a7a",
            "db27cee5aa524f5c89ee9ef3c50a0e60",
            "2f398dda6e744f01904c796703da58cd",
            "dee265ee475f44329263cd7b22ee1637",
            "628dc217877c4f539974877fa3d87696",
            "8635c9299fef4220aab7ee1765e2709c",
            "2dfc26754d20455ba0da75e33218a36e",
            "393d0d522b4743e9a04a4e3d708f529f",
            "4104399bac454257be472b40203ef926",
            "3664bfa18d95441bbf8a0875cc2e8f56",
            "b7243342e8e9473e8695e687276e9eac",
            "183da87c5a514626bff112af3cbbcffa",
            "d5481277c0294251a3b141983ec2950c",
            "15636d229dbf4a54b9f11faf54658dba",
            "498e99190ffc4810b5150c07da2e1b3d",
            "08133968c3a24449b92b80f7eb61e4d9",
            "88f47dba11ff48009ad3440061de6ee2",
            "05564490bce74bedaf40cbc368909fd4",
            "889032b33cd548cabbce84d692a18a9a",
            "ec5a25f7484a4bb0b6c404a183f78343",
            "deb656adb8114376b067f10b37f6232b",
            "ed55481101c446ac900f073ed6272749",
            "0fc8eb581ed9457aacb56dc01a9dc9e6",
            "aa921fc6bdef41049de138c4678ef731",
            "65d9876405d94ac09981657509957cc0",
            "6b5cf599b6f14f59abbd59cdfeb2757c",
            "5b0a6f743129427d8be2e358ac90c848",
            "60149f89d4684950b8de058c6479e2ec",
            "57f9abfdefd74ab6b4806bc500e7f8c1",
            "54ecc4bec948431d8202cafaedffe8f6",
            "858bd330763a4c71b0c9287f2e5ad46e",
            "f78eed65f4724831a6dfcf5272b25993",
            "6ce32bcdc7714b84ac70f7a48835cc49",
            "26b940b2d66a4cb59306b2a3abb8b6b5",
            "cd8c25d30c8e46239415fb04da6f9bb5",
            "d1b608ab95554c9aaf779b61ca7c5cbf",
            "699a5bbfd9ed494ca22f5067871ed5b1",
            "b183fa1ae8ec4e9ebb00943f16fd4752",
            "adaba7d4206e4a68a02229712d9de299",
            "85c6e81be5de4a2a92f92fa28606274e",
            "8b77517829854981acb8f1bf7eb9307c"
          ]
        },
        "id": "yQNznLfO8jOx",
        "outputId": "9c0bccfa-f9b5-4373-a538-51e594faed4b"
      },
      "outputs": [],
      "source": [
        "def get_training_dataset(BATCH_SIZE=64):\n",
        "    \"\"\"\n",
        "    Loads and maps the training split of the dataset using the custom dataset class. \n",
        "    \"\"\"\n",
        "    dataset = my_mnist_dataset(root=\"data\", split=\"train\", download=True)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    return dataloader, len(dataset)\n",
        "\n",
        "def get_validation_dataset(BATCH_SIZE = None):\n",
        "    \"\"\"\n",
        "    Loads and maps the validation split of the datasetusing the custom dataset class. \n",
        "    \"\"\"\n",
        "    dataset = my_mnist_dataset(root=\"data\", split=\"test\", download=True)\n",
        "    if BATCH_SIZE is None:\n",
        "        BATCH_SIZE = len(dataset)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    return dataloader, len(dataset)\n",
        "\n",
        "# instantiate the datasets\n",
        "training_dataset, len_training_ds = get_training_dataset()\n",
        "validation_dataset, len_validation_ds = get_validation_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wfgUx1srt90c"
      },
      "outputs": [],
      "source": [
        "class CNNBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(CNNBlock, self).__init__()\n",
        "        self.conv = torch.nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "        self.bn = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.l_relu = torch.nn.LeakyReLU(0.1)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        x = self.conv(input)\n",
        "        x = self.bn(x)\n",
        "        return self.l_relu(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3VjCkTEce-CT"
      },
      "outputs": [],
      "source": [
        "class YoloMNIST(torch.nn.Module):\n",
        "    def __init__(self, sizeHW, S, C, B):\n",
        "        super(YoloMNIST, self).__init__()\n",
        "        self.S, self.C, self.B = S, C, B\n",
        "        self.sizeHW = sizeHW\n",
        "        self.cell_size = self.sizeHW / self.S\n",
        "\n",
        "        self.seq = torch.nn.Sequential()        \n",
        "        self.seq.add_module(f\"conv_1\", CNNBlock(1, 32, stride=2, kernel_size=7, padding=2))\n",
        "        self.seq.add_module(f\"maxpool_1\", torch.nn.MaxPool2d(2))\n",
        "        self.seq.add_module(f\"conv_3\", CNNBlock(32, 128, stride=1, kernel_size=3, padding=0))\n",
        "        self.seq.add_module(f\"maxpool_2\", torch.nn.MaxPool2d(2))\n",
        "        self.seq.add_module(f\"conv_5\", CNNBlock(128, 64, stride=1, kernel_size=1, padding=0))\n",
        "        self.seq.add_module(f\"conv_4\", CNNBlock(64, 128, stride=1, kernel_size=3, padding=0))\n",
        "        self.seq.add_module(f\"conv_6\", CNNBlock(128, 128, stride=1, kernel_size=3, padding=1))\n",
        "        \n",
        "        self.fcs = self._create_fcs()\n",
        "\n",
        "    def _size_output(self, sizeHW:int, kernel:int, stride:int, padding:int=0, isMaxPool:bool=False)->int:\n",
        "        \"\"\"\n",
        "        Output size (width/height) of convolutional or maxpool layers.\n",
        "\n",
        "        Args:\n",
        "            sizeHW : int\n",
        "                Image size (we suppose this is a square image)\n",
        "            kernel : int\n",
        "                Size of a square kernel\n",
        "            stride : int\n",
        "                Stride of convolution layer\n",
        "            padding : int\n",
        "                Padding of convolution layer\n",
        "            isMaxPool : Bool, default is False.\n",
        "                Specify if it is a Maxpool layer (True) or not (False). \n",
        "\n",
        "        Return:\n",
        "            output_size : int\n",
        "                Image output size after a convolutional or MaxPool layer.\n",
        "        \"\"\" \n",
        "        if isMaxPool == True:\n",
        "            output_size = int(sizeHW/2)\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "        if padding == 'same':\n",
        "            output_size = sizeHW\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "        else:\n",
        "            output_size = (sizeHW + 2 * padding - (kernel-1)-1)/stride\n",
        "            output_size = int(output_size + 1)\n",
        "            print(output_size)\n",
        "            return output_size\n",
        "\n",
        "    def _create_fcs(self):\n",
        "        output = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(128 * self.S * self.S, 4096),\n",
        "            torch.nn.LeakyReLU(0.1),\n",
        "            torch.nn.Linear(4096, self.S * self.S * (self.C + self.B*5))\n",
        "        )\n",
        "        return output\n",
        "    \n",
        "\n",
        "    def forward(self, input:torch.Tensor)->tuple:\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            input : torch.Tensor of shape (N, C, H, W)\n",
        "                Batch of images.\n",
        "\n",
        "        Return:\n",
        "            box_coord : torch.Tensor of shape (N, 6, 6, 5)\n",
        "                Contains xc_rcell, yc_rcell, rw, rh and the confidence number c\n",
        "                over 6x6 grid cells.\n",
        "            classifier : torch.Tensor of shape (N, 6, 6, 10)\n",
        "                Contains the one-hot encoding of each digit number over\n",
        "                6x6 grid cells.\n",
        "        \"\"\"     \n",
        "        x = self.seq(input)\n",
        "        x = self.fcs(x)\n",
        "        x = x.view(x.size(0), self.S, self.S, self.B * 5 + self.C)\n",
        "        box_coord = x[:,:,:,0:5]\n",
        "        classifier = x[:,:,:,5:]\n",
        "        return box_coord, classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "T_jOg_i_p2_c"
      },
      "outputs": [],
      "source": [
        "class YoloLoss(torch.nn.Module):\n",
        "    def __init__(self, lambd_coord:int, lambd_noobj:float, device:torch.device, S:int=6):\n",
        "        super(YoloLoss, self).__init__()\n",
        "        self.LAMBD_COORD = lambd_coord\n",
        "        self.LAMBD_NOOBJ = lambd_noobj\n",
        "        self.S = S\n",
        "        self.device = device\n",
        "\n",
        "    def _coordloss(self, pred_coord_rcell, true_coord_rcell):\n",
        "        \"\"\"\n",
        "        Args : \n",
        "            pred_coord_rcell : torch.Tensor of shape (N, 2)\n",
        "            true_coord_rcell : torch.Tensor of shape (N, 2)\n",
        "        Returns :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        xc_hat, yc_hat = pred_coord_rcell.permute(1,0)\n",
        "        xc, yc = true_coord_rcell.permute(1,0)\n",
        "\n",
        "        squared_error = torch.pow(xc - xc_hat,2) + torch.pow(yc - yc_hat,2)\n",
        "        return squared_error\n",
        "\n",
        "    def _sizeloss(self, pred_size, true_size):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_size : torch.Tensor of shape (N, 2)\n",
        "            true_size : torch.Tensor of shape (N, 2)\n",
        "        Returns : \n",
        "            root_squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        rw_hat, rh_hat = pred_size.permute(1,0)\n",
        "        rw, rh = true_size.permute(1,0)\n",
        "\n",
        "        #sizes can't be negative\n",
        "        rw_hat = rw_hat.clip(min=0)\n",
        "        rh_hat = rh_hat.clip(min=0)\n",
        "\n",
        "        root_squared_error_w = torch.pow(torch.sqrt(rw) - torch.sqrt(rw_hat),2)\n",
        "        root_squared_error_h = torch.pow(torch.sqrt(rh) - torch.sqrt(rh_hat),2)\n",
        "        root_squared_error = root_squared_error_w + root_squared_error_h\n",
        "        return root_squared_error\n",
        "\n",
        "    def _confidenceloss(self, pred_c, true_c):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_c : torch.Tensor of shape (N)\n",
        "            true_c : torch.Tensor of shape (N)\n",
        "        Return :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        squared_error = torch.pow(true_c - pred_c, 2)\n",
        "        return squared_error\n",
        "\n",
        "    def _classloss(self, pred_class, true_class):\n",
        "        \"\"\"\n",
        "        Args :\n",
        "            pred_class : torch.Tensor of shape (N, 10)\n",
        "            true_class : torch.Tensor of shape (N, 10)\n",
        "        Returns :\n",
        "            squared_error : torch.Tensor of shape (N)\n",
        "        \"\"\"\n",
        "        squared_error = torch.pow(true_class - pred_class, 2)\n",
        "        return torch.sum(squared_error, dim=1)\n",
        "\n",
        "    def forward(self, pred_box:torch.Tensor, true_box:torch.Tensor, pred_class:torch.Tensor, true_class:torch.Tensor):\n",
        "        \"\"\"\n",
        "        Grid forward pass.\n",
        "\n",
        "        Args:\n",
        "            pred_box : torch.Tensor of shape (N, S, S, 5)\n",
        "                Batch predicted outputs containing xc_rcell, yc_rcell, rw, rh,\n",
        "                and confident number c for each grid cell.\n",
        "            true_box : torch.Tensor of shape (N, S, S, 5)\n",
        "                Groundtrue batch containing bbox values for each cell and\n",
        "                c indicate if there is an object to detect or not (1/0).\n",
        "            pred_class : torch.Tensor of shape (N, S, S, 10)\n",
        "                Probability of each digit class in each grid cell\n",
        "            true_class : torch.Tensor of shape (N, 10)\n",
        "                one-hot vect of each digit\n",
        "\n",
        "        Return:\n",
        "            loss : float\n",
        "                The batch loss value of the grid\n",
        "        \"\"\"\n",
        "        BATCH_SIZE = len(pred_box)\n",
        "\n",
        "        ### Initialization of the losses\n",
        "        losses_list = ['loss_xy', 'loss_wh', 'loss_conf_obj', 'loss_conf_noobj', 'loss_class','isObject']\n",
        "        losses = {key : torch.zeros(BATCH_SIZE).to(self.device) for key in losses_list}\n",
        "        check_loss = []\n",
        "        ### Compute the losses for all images in the batch\n",
        "        for i in range(self.S):\n",
        "            for j in range(self.S):\n",
        "                ### Intersection over Union\n",
        "                #IoU = self._intersection_over_union(pred_box[:,i,j], true_box[:,i,j])\n",
        "\n",
        "                ### bbox coordinates\n",
        "                xy_hat = pred_box[:,i,j,:2]\n",
        "                xy = true_box[:,i,j,:2]\n",
        "                wh_hat = pred_box[:,i,j,2:4]\n",
        "                wh = true_box[:,i,j,2:4]\n",
        "                \n",
        "                ### confidence numbers\n",
        "                pred_c = pred_box[:,i,j,4]# * IoU\n",
        "                true_c = true_box[:,i,j,4]\n",
        "\n",
        "                ### objects to detect\n",
        "                isObject = true_c.to(torch.bool)\n",
        "                isNoObject = torch.logical_not(true_c) #(~bool) doesn't work on MPS device\n",
        "\n",
        "                ### sum the losses over the grid\n",
        "                losses['isObject'] += isObject\n",
        "                losses['loss_xy'] += isObject * self._coordloss(xy_hat, xy)\n",
        "                check_loss.append(losses['loss_xy'])\n",
        "                losses['loss_wh'] += isObject * self._sizeloss(wh_hat, wh)\n",
        "                losses['loss_conf_obj'] += isObject * self._confidenceloss(pred_c, true_c)\n",
        "                losses['loss_conf_noobj'] += isNoObject * self._confidenceloss(pred_c, true_c)\n",
        "                losses['loss_class'] += isObject * self._classloss(pred_class[:,i,j], true_class)\n",
        "        \n",
        "\n",
        "\n",
        "        ### Yolo_v1 loss over the batch, shape : (BATCH_SIZE)\n",
        "        loss = self.LAMBD_COORD * losses['loss_xy'] \\\n",
        "                + self.LAMBD_COORD * losses['loss_wh'] \\\n",
        "                + losses['loss_conf_obj'] \\\n",
        "                + self.LAMBD_NOOBJ * losses['loss_conf_noobj'] \\\n",
        "                + losses['loss_class']\n",
        "\n",
        "\n",
        "        assert torch.isnan(torch.sum(losses['loss_conf_obj']))==False, \"La loss {} est devenu nan\".format('loss_conf_obj')\n",
        "        assert torch.isnan(torch.sum(losses['loss_conf_noobj']))==False, \"La loss {} est devenu nan\".format('loss_conf_noobj')\n",
        "        assert torch.isnan(torch.sum(losses['loss_class']))==False, \"La loss {} est devenu nan\".format('loss_class')\n",
        "        assert torch.isnan(torch.sum(losses['isObject']))==False, \"La loss {} est devenu nan\".format('isObject')\n",
        "        assert torch.isnan(torch.sum(losses['loss_wh']))==False, \"La loss {} est devenu nan\".format('loss_wh')\n",
        "        assert torch.isnan(torch.sum(losses['loss_xy']))==False, \"La loss {} est devenu nan\".format('loss_xy')\n",
        "\n",
        "\n",
        "        loss = torch.sum(loss) / BATCH_SIZE\n",
        "\n",
        "        return check_loss, losses, loss\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OdIlkN61mXJj"
      },
      "outputs": [],
      "source": [
        "def bbox2Tensor(bbox:torch.Tensor, S:int=6, sizeHW:int=75, device=torch.device('cpu'))->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Constructs en Tensor and puts bbox values in the corresponding i,j grid cell.\n",
        "\n",
        "    Args :\n",
        "        bbox : torch.Tensor of shape (N,4)\n",
        "            Contains bbox values xc_rcell, yc_rcell, rw and rh.\n",
        "        S : int, default is 6\n",
        "            Size of the grid.\n",
        "        sizeHW : int, default is 75\n",
        "            Size of the image.\n",
        "\n",
        "    Return :\n",
        "        bbox_t : torch.Tensor of shape (N, S, S, 5)\n",
        "            Tensor containing all 4 bbox values in the corresponding i,j grid\n",
        "            cell position i.e. in the i,j position where an object should be\n",
        "            detected.\n",
        "    \"\"\"\n",
        "    assert bbox.shape[-1] == 4, \"True bbox should be of size (N,S,S,4).\"\n",
        "\n",
        "    N = len(bbox)\n",
        "    bbox_t = torch.zeros(N,S,S,5).to(device)\n",
        "    cell_size = sizeHW/S\n",
        "\n",
        "    xc_rcell, yc_rcell, rw, rh = bbox.permute(1,0).to(device)\n",
        "    xc = xc_rcell * cell_size - (1/cell_size) * (xc_rcell/cell_size).to(torch.int32)\n",
        "    yc = yc_rcell * cell_size - (1/cell_size) * (yc_rcell/cell_size).to(torch.int32)\n",
        "\n",
        "    N_range = torch.arange(N)\n",
        "    lines = (yc * S).to(torch.long)\n",
        "    columns = (xc * S).to(torch.long)\n",
        "    bbox_t[N_range, lines, columns] = torch.stack((xc_rcell, yc_rcell, rw, rh, torch.ones(N))).permute(1,0)\n",
        "    \n",
        "    return bbox_t.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mHUOkfWPe-CW"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "model_MNIST = YoloMNIST(sizeHW=75, S=6, C=10, B=1)\n",
        "model_MNIST = model_MNIST.to(device)\n",
        "optimizer = torch.optim.Adam(params=model_MNIST.parameters(), lr=learning_rate, weight_decay=0.0005)\n",
        "loss_yolo = YoloLoss(lambd_coord=5, lambd_noobj=0.5, S=6, device=device)\n",
        "\n",
        "# print(optimizer)\n",
        "#summary(model_MNIST, input_size = (BATCH_SIZE,1,75,75))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9tSO0Qo7e-CX",
        "outputId": "30e9cb02-0115-4295-8901-373725937086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[START] : 2022-08-26 17:58:48 :\n",
            "[Training on] : CPU\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 1/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.09439\n",
            "xy_coord training loss for this batch : 0.00019\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.03031\n",
            "confidence without object training loss for this batch : 0.04939\n",
            "class proba training loss for this batch : 0.03779\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.08519\n",
            "xy_coord training loss for this batch : 0.00099\n",
            "wh_sizes training loss for this batch : 0.00031\n",
            "confidence with object training loss for this batch : 0.02470\n",
            "confidence without object training loss for this batch : 0.05698\n",
            "class proba training loss for this batch : 0.02549\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.10395\n",
            "xy_coord training loss for this batch : 0.00023\n",
            "wh_sizes training loss for this batch : 0.00018\n",
            "confidence with object training loss for this batch : 0.03329\n",
            "confidence without object training loss for this batch : 0.05603\n",
            "class proba training loss for this batch : 0.04058\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.10638\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.02419\n",
            "confidence without object training loss for this batch : 0.05187\n",
            "class proba training loss for this batch : 0.05551\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.10645\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.02743\n",
            "confidence without object training loss for this batch : 0.05348\n",
            "class proba training loss for this batch : 0.05138\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.13770\n",
            "xy_coord training loss for this batch : 0.00003\n",
            "wh_sizes training loss for this batch : 0.00020\n",
            "confidence with object training loss for this batch : 0.02019\n",
            "confidence without object training loss for this batch : 0.04441\n",
            "class proba training loss for this batch : 0.09417\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.07867\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.01098\n",
            "confidence without object training loss for this batch : 0.01979\n",
            "class proba training loss for this batch : 0.05674\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.12112\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.02806\n",
            "confidence without object training loss for this batch : 0.04989\n",
            "class proba training loss for this batch : 0.06707\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.08081\n",
            "xy_coord training loss for this batch : 0.00034\n",
            "wh_sizes training loss for this batch : 0.00050\n",
            "confidence with object training loss for this batch : 0.01680\n",
            "confidence without object training loss for this batch : 0.02968\n",
            "class proba training loss for this batch : 0.04498\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.05703\n",
            "xy_coord training loss for this batch : 0.00013\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.01417\n",
            "confidence without object training loss for this batch : 0.03282\n",
            "class proba training loss for this batch : 0.02498\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.08396\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.03066\n",
            "confidence without object training loss for this batch : 0.06265\n",
            "class proba training loss for this batch : 0.02106\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:57.650195\n",
            "Mean training loss for this epoch : 0.09038\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 2/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.09173\n",
            "xy_coord training loss for this batch : 0.00012\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.01858\n",
            "confidence without object training loss for this batch : 0.03295\n",
            "class proba training loss for this batch : 0.05538\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.06509\n",
            "xy_coord training loss for this batch : 0.00012\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.01541\n",
            "confidence without object training loss for this batch : 0.03136\n",
            "class proba training loss for this batch : 0.03270\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.07973\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.01547\n",
            "confidence without object training loss for this batch : 0.03445\n",
            "class proba training loss for this batch : 0.04607\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.07496\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.02685\n",
            "confidence without object training loss for this batch : 0.05310\n",
            "class proba training loss for this batch : 0.02053\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.08368\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00019\n",
            "confidence with object training loss for this batch : 0.01975\n",
            "confidence without object training loss for this batch : 0.03551\n",
            "class proba training loss for this batch : 0.04471\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.06606\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.01489\n",
            "confidence without object training loss for this batch : 0.03546\n",
            "class proba training loss for this batch : 0.03223\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.11111\n",
            "xy_coord training loss for this batch : 0.00018\n",
            "wh_sizes training loss for this batch : 0.00018\n",
            "confidence with object training loss for this batch : 0.02116\n",
            "confidence without object training loss for this batch : 0.03725\n",
            "class proba training loss for this batch : 0.06955\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.14527\n",
            "xy_coord training loss for this batch : 0.00013\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.05380\n",
            "confidence without object training loss for this batch : 0.08763\n",
            "class proba training loss for this batch : 0.04650\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.07322\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.02709\n",
            "confidence without object training loss for this batch : 0.04310\n",
            "class proba training loss for this batch : 0.02347\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.11145\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.04289\n",
            "confidence without object training loss for this batch : 0.06041\n",
            "class proba training loss for this batch : 0.03730\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.11547\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00016\n",
            "confidence with object training loss for this batch : 0.05234\n",
            "confidence without object training loss for this batch : 0.07041\n",
            "class proba training loss for this batch : 0.02666\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:56.459185\n",
            "Mean training loss for this epoch : 0.08923\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 3/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.08336\n",
            "xy_coord training loss for this batch : 0.00036\n",
            "wh_sizes training loss for this batch : 0.00025\n",
            "confidence with object training loss for this batch : 0.02421\n",
            "confidence without object training loss for this batch : 0.04690\n",
            "class proba training loss for this batch : 0.03268\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.06516\n",
            "xy_coord training loss for this batch : 0.00012\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.01404\n",
            "confidence without object training loss for this batch : 0.03006\n",
            "class proba training loss for this batch : 0.03482\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.08599\n",
            "xy_coord training loss for this batch : 0.00003\n",
            "wh_sizes training loss for this batch : 0.00020\n",
            "confidence with object training loss for this batch : 0.02775\n",
            "confidence without object training loss for this batch : 0.04570\n",
            "class proba training loss for this batch : 0.03421\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.06789\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.01728\n",
            "confidence without object training loss for this batch : 0.03030\n",
            "class proba training loss for this batch : 0.03451\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.09584\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00018\n",
            "confidence with object training loss for this batch : 0.02434\n",
            "confidence without object training loss for this batch : 0.03720\n",
            "class proba training loss for this batch : 0.05159\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.06190\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.01276\n",
            "confidence without object training loss for this batch : 0.02326\n",
            "class proba training loss for this batch : 0.03652\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.07418\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02107\n",
            "confidence without object training loss for this batch : 0.03494\n",
            "class proba training loss for this batch : 0.03481\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.06980\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.02522\n",
            "confidence without object training loss for this batch : 0.04467\n",
            "class proba training loss for this batch : 0.02124\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.09082\n",
            "xy_coord training loss for this batch : 0.00019\n",
            "wh_sizes training loss for this batch : 0.00020\n",
            "confidence with object training loss for this batch : 0.02565\n",
            "confidence without object training loss for this batch : 0.06633\n",
            "class proba training loss for this batch : 0.03006\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.06620\n",
            "xy_coord training loss for this batch : 0.00014\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.01700\n",
            "confidence without object training loss for this batch : 0.02717\n",
            "class proba training loss for this batch : 0.03439\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.09271\n",
            "xy_coord training loss for this batch : 0.00017\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.02175\n",
            "confidence without object training loss for this batch : 0.04452\n",
            "class proba training loss for this batch : 0.04726\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:56.631517\n",
            "Mean training loss for this epoch : 0.08772\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 4/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.08024\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00019\n",
            "confidence with object training loss for this batch : 0.02737\n",
            "confidence without object training loss for this batch : 0.04552\n",
            "class proba training loss for this batch : 0.02866\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.09471\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00018\n",
            "confidence with object training loss for this batch : 0.03293\n",
            "confidence without object training loss for this batch : 0.06195\n",
            "class proba training loss for this batch : 0.02950\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.10564\n",
            "xy_coord training loss for this batch : 0.00014\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.03292\n",
            "confidence without object training loss for this batch : 0.05261\n",
            "class proba training loss for this batch : 0.04511\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.07517\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.02474\n",
            "confidence without object training loss for this batch : 0.04318\n",
            "class proba training loss for this batch : 0.02789\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.10294\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.02586\n",
            "confidence without object training loss for this batch : 0.04466\n",
            "class proba training loss for this batch : 0.05376\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.08961\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02195\n",
            "confidence without object training loss for this batch : 0.03785\n",
            "class proba training loss for this batch : 0.04774\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.06810\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.01643\n",
            "confidence without object training loss for this batch : 0.02890\n",
            "class proba training loss for this batch : 0.03642\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.10339\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.03173\n",
            "confidence without object training loss for this batch : 0.04481\n",
            "class proba training loss for this batch : 0.04821\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.08301\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.02701\n",
            "confidence without object training loss for this batch : 0.04607\n",
            "class proba training loss for this batch : 0.03189\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.06550\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.01781\n",
            "confidence without object training loss for this batch : 0.04599\n",
            "class proba training loss for this batch : 0.02370\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.10133\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.02585\n",
            "confidence without object training loss for this batch : 0.04989\n",
            "class proba training loss for this batch : 0.04933\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:55.753316\n",
            "Mean training loss for this epoch : 0.08465\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 5/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.11983\n",
            "xy_coord training loss for this batch : 0.00015\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.03035\n",
            "confidence without object training loss for this batch : 0.04148\n",
            "class proba training loss for this batch : 0.06724\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.11224\n",
            "xy_coord training loss for this batch : 0.00027\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.03120\n",
            "confidence without object training loss for this batch : 0.05016\n",
            "class proba training loss for this batch : 0.05388\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.07145\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.02110\n",
            "confidence without object training loss for this batch : 0.04308\n",
            "class proba training loss for this batch : 0.02811\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.08510\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.01724\n",
            "confidence without object training loss for this batch : 0.03003\n",
            "class proba training loss for this batch : 0.05171\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.08496\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.02557\n",
            "confidence without object training loss for this batch : 0.04429\n",
            "class proba training loss for this batch : 0.03622\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.12778\n",
            "xy_coord training loss for this batch : 0.00004\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.02765\n",
            "confidence without object training loss for this batch : 0.05068\n",
            "class proba training loss for this batch : 0.07411\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.09183\n",
            "xy_coord training loss for this batch : 0.00016\n",
            "wh_sizes training loss for this batch : 0.00046\n",
            "confidence with object training loss for this batch : 0.02159\n",
            "confidence without object training loss for this batch : 0.03889\n",
            "class proba training loss for this batch : 0.04772\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.09356\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.02913\n",
            "confidence without object training loss for this batch : 0.05331\n",
            "class proba training loss for this batch : 0.03642\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.07143\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.02738\n",
            "confidence without object training loss for this batch : 0.04739\n",
            "class proba training loss for this batch : 0.01931\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.09531\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02921\n",
            "confidence without object training loss for this batch : 0.04613\n",
            "class proba training loss for this batch : 0.04205\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.09217\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.03564\n",
            "confidence without object training loss for this batch : 0.05594\n",
            "class proba training loss for this batch : 0.02745\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:55.334478\n",
            "Mean training loss for this epoch : 0.08431\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 6/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.07006\n",
            "xy_coord training loss for this batch : 0.00015\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.01923\n",
            "confidence without object training loss for this batch : 0.03845\n",
            "class proba training loss for this batch : 0.03015\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.09544\n",
            "xy_coord training loss for this batch : 0.00017\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.02448\n",
            "confidence without object training loss for this batch : 0.04319\n",
            "class proba training loss for this batch : 0.04785\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.08287\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.02044\n",
            "confidence without object training loss for this batch : 0.03612\n",
            "class proba training loss for this batch : 0.04363\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.07609\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00016\n",
            "confidence with object training loss for this batch : 0.02348\n",
            "confidence without object training loss for this batch : 0.03572\n",
            "class proba training loss for this batch : 0.03365\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.07497\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.03330\n",
            "confidence without object training loss for this batch : 0.04344\n",
            "class proba training loss for this batch : 0.01904\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.12991\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.04061\n",
            "confidence without object training loss for this batch : 0.06286\n",
            "class proba training loss for this batch : 0.05702\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.06424\n",
            "xy_coord training loss for this batch : 0.00015\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.01527\n",
            "confidence without object training loss for this batch : 0.03205\n",
            "class proba training loss for this batch : 0.03135\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.10622\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00023\n",
            "confidence with object training loss for this batch : 0.03240\n",
            "confidence without object training loss for this batch : 0.05534\n",
            "class proba training loss for this batch : 0.04450\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.05259\n",
            "xy_coord training loss for this batch : 0.00018\n",
            "wh_sizes training loss for this batch : 0.00023\n",
            "confidence with object training loss for this batch : 0.01570\n",
            "confidence without object training loss for this batch : 0.03024\n",
            "class proba training loss for this batch : 0.01975\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.08334\n",
            "xy_coord training loss for this batch : 0.00054\n",
            "wh_sizes training loss for this batch : 0.00021\n",
            "confidence with object training loss for this batch : 0.02755\n",
            "confidence without object training loss for this batch : 0.03871\n",
            "class proba training loss for this batch : 0.03268\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.12842\n",
            "xy_coord training loss for this batch : 0.00024\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.04882\n",
            "confidence without object training loss for this batch : 0.07335\n",
            "class proba training loss for this batch : 0.04102\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:56.721960\n",
            "Mean training loss for this epoch : 0.08369\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 7/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.05451\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00019\n",
            "confidence with object training loss for this batch : 0.01478\n",
            "confidence without object training loss for this batch : 0.02802\n",
            "class proba training loss for this batch : 0.02422\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.06715\n",
            "xy_coord training loss for this batch : 0.00014\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.02101\n",
            "confidence without object training loss for this batch : 0.03877\n",
            "class proba training loss for this batch : 0.02538\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.06385\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.01487\n",
            "confidence without object training loss for this batch : 0.03632\n",
            "class proba training loss for this batch : 0.02952\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.10680\n",
            "xy_coord training loss for this batch : 0.00013\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.03402\n",
            "confidence without object training loss for this batch : 0.05434\n",
            "class proba training loss for this batch : 0.04432\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.10260\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.04385\n",
            "confidence without object training loss for this batch : 0.05872\n",
            "class proba training loss for this batch : 0.02817\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.09076\n",
            "xy_coord training loss for this batch : 0.00018\n",
            "wh_sizes training loss for this batch : 0.00018\n",
            "confidence with object training loss for this batch : 0.02743\n",
            "confidence without object training loss for this batch : 0.05180\n",
            "class proba training loss for this batch : 0.03565\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.05871\n",
            "xy_coord training loss for this batch : 0.00004\n",
            "wh_sizes training loss for this batch : 0.00007\n",
            "confidence with object training loss for this batch : 0.01105\n",
            "confidence without object training loss for this batch : 0.02359\n",
            "class proba training loss for this batch : 0.03532\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.04066\n",
            "xy_coord training loss for this batch : 0.00003\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.01180\n",
            "confidence without object training loss for this batch : 0.02655\n",
            "class proba training loss for this batch : 0.01506\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.07510\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00012\n",
            "confidence with object training loss for this batch : 0.02218\n",
            "confidence without object training loss for this batch : 0.03829\n",
            "class proba training loss for this batch : 0.03287\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.05745\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.01998\n",
            "confidence without object training loss for this batch : 0.03895\n",
            "class proba training loss for this batch : 0.01697\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.19073\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.02661\n",
            "confidence without object training loss for this batch : 0.04557\n",
            "class proba training loss for this batch : 0.14070\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:12.574158\n",
            "Mean training loss for this epoch : 0.08056\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 8/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.07927\n",
            "xy_coord training loss for this batch : 0.00010\n",
            "wh_sizes training loss for this batch : 0.00007\n",
            "confidence with object training loss for this batch : 0.02852\n",
            "confidence without object training loss for this batch : 0.04449\n",
            "class proba training loss for this batch : 0.02766\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.10151\n",
            "xy_coord training loss for this batch : 0.00024\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.03401\n",
            "confidence without object training loss for this batch : 0.05782\n",
            "class proba training loss for this batch : 0.03673\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.07040\n",
            "xy_coord training loss for this batch : 0.00013\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.02990\n",
            "confidence without object training loss for this batch : 0.05196\n",
            "class proba training loss for this batch : 0.01314\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.07875\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.02615\n",
            "confidence without object training loss for this batch : 0.04449\n",
            "class proba training loss for this batch : 0.02938\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.09732\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02366\n",
            "confidence without object training loss for this batch : 0.04658\n",
            "class proba training loss for this batch : 0.04968\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.07584\n",
            "xy_coord training loss for this batch : 0.00020\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.02877\n",
            "confidence without object training loss for this batch : 0.04067\n",
            "class proba training loss for this batch : 0.02504\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.10806\n",
            "xy_coord training loss for this batch : 0.00024\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.04151\n",
            "confidence without object training loss for this batch : 0.06483\n",
            "class proba training loss for this batch : 0.03224\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.09691\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.03430\n",
            "confidence without object training loss for this batch : 0.05421\n",
            "class proba training loss for this batch : 0.03440\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.07912\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.03432\n",
            "confidence without object training loss for this batch : 0.04667\n",
            "class proba training loss for this batch : 0.02029\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.07137\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00005\n",
            "confidence with object training loss for this batch : 0.02058\n",
            "confidence without object training loss for this batch : 0.03914\n",
            "class proba training loss for this batch : 0.03063\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.05552\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.01215\n",
            "confidence without object training loss for this batch : 0.01576\n",
            "class proba training loss for this batch : 0.03446\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:06.819568\n",
            "Mean training loss for this epoch : 0.08130\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 9/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.06489\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.00946\n",
            "confidence without object training loss for this batch : 0.02048\n",
            "class proba training loss for this batch : 0.04447\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.05294\n",
            "xy_coord training loss for this batch : 0.00004\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.01566\n",
            "confidence without object training loss for this batch : 0.02475\n",
            "class proba training loss for this batch : 0.02422\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.07293\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02097\n",
            "confidence without object training loss for this batch : 0.03436\n",
            "class proba training loss for this batch : 0.03407\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.10113\n",
            "xy_coord training loss for this batch : 0.00008\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.02283\n",
            "confidence without object training loss for this batch : 0.03617\n",
            "class proba training loss for this batch : 0.05917\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.06490\n",
            "xy_coord training loss for this batch : 0.00026\n",
            "wh_sizes training loss for this batch : 0.00020\n",
            "confidence with object training loss for this batch : 0.02060\n",
            "confidence without object training loss for this batch : 0.04534\n",
            "class proba training loss for this batch : 0.01933\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.11825\n",
            "xy_coord training loss for this batch : 0.00014\n",
            "wh_sizes training loss for this batch : 0.00014\n",
            "confidence with object training loss for this batch : 0.02599\n",
            "confidence without object training loss for this batch : 0.05779\n",
            "class proba training loss for this batch : 0.06195\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.06326\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.01985\n",
            "confidence without object training loss for this batch : 0.02962\n",
            "class proba training loss for this batch : 0.02780\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.06217\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00016\n",
            "confidence with object training loss for this batch : 0.02094\n",
            "confidence without object training loss for this batch : 0.03773\n",
            "class proba training loss for this batch : 0.02132\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.13603\n",
            "xy_coord training loss for this batch : 0.00009\n",
            "wh_sizes training loss for this batch : 0.00017\n",
            "confidence with object training loss for this batch : 0.05199\n",
            "confidence without object training loss for this batch : 0.07714\n",
            "class proba training loss for this batch : 0.04419\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.11675\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.03062\n",
            "confidence without object training loss for this batch : 0.04582\n",
            "class proba training loss for this batch : 0.06231\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.08727\n",
            "xy_coord training loss for this batch : 0.00004\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.03553\n",
            "confidence without object training loss for this batch : 0.06989\n",
            "class proba training loss for this batch : 0.01582\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:02:59.223043\n",
            "Mean training loss for this epoch : 0.08019\n",
            "--------------------\n",
            "     2022-08-26 17:58:48 : EPOCH 10/10\n",
            "--------------------\n",
            " --- Image : 64/60000  : loss = 0.10570\n",
            "xy_coord training loss for this batch : 0.00021\n",
            "wh_sizes training loss for this batch : 0.00013\n",
            "confidence with object training loss for this batch : 0.03178\n",
            "confidence without object training loss for this batch : 0.06125\n",
            "class proba training loss for this batch : 0.04160\n",
            "\n",
            "\n",
            " --- Image : 6400/60000  : loss = 0.06053\n",
            "xy_coord training loss for this batch : 0.00011\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.01647\n",
            "confidence without object training loss for this batch : 0.03675\n",
            "class proba training loss for this batch : 0.02472\n",
            "\n",
            "\n",
            " --- Image : 12800/60000  : loss = 0.05142\n",
            "xy_coord training loss for this batch : 0.00015\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.01232\n",
            "confidence without object training loss for this batch : 0.02203\n",
            "class proba training loss for this batch : 0.02656\n",
            "\n",
            "\n",
            " --- Image : 19200/60000  : loss = 0.14592\n",
            "xy_coord training loss for this batch : 0.00020\n",
            "wh_sizes training loss for this batch : 0.00015\n",
            "confidence with object training loss for this batch : 0.04142\n",
            "confidence without object training loss for this batch : 0.07727\n",
            "class proba training loss for this batch : 0.06407\n",
            "\n",
            "\n",
            " --- Image : 25600/60000  : loss = 0.07642\n",
            "xy_coord training loss for this batch : 0.00006\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02089\n",
            "confidence without object training loss for this batch : 0.03983\n",
            "class proba training loss for this batch : 0.03486\n",
            "\n",
            "\n",
            " --- Image : 32000/60000  : loss = 0.07278\n",
            "xy_coord training loss for this batch : 0.00012\n",
            "wh_sizes training loss for this batch : 0.00020\n",
            "confidence with object training loss for this batch : 0.02589\n",
            "confidence without object training loss for this batch : 0.04409\n",
            "class proba training loss for this batch : 0.02326\n",
            "\n",
            "\n",
            " --- Image : 38400/60000  : loss = 0.06789\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.02204\n",
            "confidence without object training loss for this batch : 0.03870\n",
            "class proba training loss for this batch : 0.02569\n",
            "\n",
            "\n",
            " --- Image : 44800/60000  : loss = 0.07532\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00011\n",
            "confidence with object training loss for this batch : 0.02212\n",
            "confidence without object training loss for this batch : 0.03580\n",
            "class proba training loss for this batch : 0.03447\n",
            "\n",
            "\n",
            " --- Image : 51200/60000  : loss = 0.05659\n",
            "xy_coord training loss for this batch : 0.00014\n",
            "wh_sizes training loss for this batch : 0.00010\n",
            "confidence with object training loss for this batch : 0.01872\n",
            "confidence without object training loss for this batch : 0.02611\n",
            "class proba training loss for this batch : 0.02362\n",
            "\n",
            "\n",
            " --- Image : 57600/60000  : loss = 0.08370\n",
            "xy_coord training loss for this batch : 0.00007\n",
            "wh_sizes training loss for this batch : 0.00009\n",
            "confidence with object training loss for this batch : 0.02106\n",
            "confidence without object training loss for this batch : 0.03717\n",
            "class proba training loss for this batch : 0.04328\n",
            "\n",
            "\n",
            " --- Image : 60000/60000  : loss = 0.08118\n",
            "xy_coord training loss for this batch : 0.00005\n",
            "wh_sizes training loss for this batch : 0.00008\n",
            "confidence with object training loss for this batch : 0.00537\n",
            "confidence without object training loss for this batch : 0.00688\n",
            "class proba training loss for this batch : 0.07170\n",
            "\n",
            "\n",
            "Total elapsed time for training : 0:03:01.382402\n",
            "Mean training loss for this epoch : 0.07840\n"
          ]
        }
      ],
      "source": [
        "delta_time = datetime.timedelta(hours=1)\n",
        "timezone = datetime.timezone(offset=delta_time)\n",
        "\n",
        "t = datetime.datetime.now(tz=timezone)\n",
        "str_t = '{:%Y-%m-%d %H:%M:%S}'.format(t)\n",
        "print(f\"[START] : {str_t} :\")\n",
        "print(f\"[Training on] : {str(device).upper()}\")\n",
        "\n",
        "EPOCHS = 10\n",
        "size_grid = 6\n",
        "batch_loss_list = []\n",
        "model_MNIST = model_MNIST.to(device)\n",
        "check = []\n",
        "\n",
        "for epoch in range(EPOCHS) : \n",
        "    begin_time = timer()\n",
        "    epochs_loss = 0.\n",
        "    \n",
        "    print(\"-\"*20)\n",
        "    str_t = '{:%Y-%m-%d %H:%M:%S}'.format(t)\n",
        "    print(\" \"*5 + f\"{str_t} : EPOCH {epoch+1}/{EPOCHS}\")\n",
        "    print(\"-\"*20)\n",
        "\n",
        "    model_MNIST.train()\n",
        "    for batch, (img, labels, bbox_true) in enumerate(training_dataset):\n",
        "        loss = 0\n",
        "        begin_batch_time = timer()\n",
        "        img, labels, bbox_true = img.to(device), labels.to(device), bbox_true.to(device)\n",
        "        \n",
        "        ### turn bbox into NxSxSx5 tensor\n",
        "        bbox_true_6x6 = bbox2Tensor(bbox_true).to(device)\n",
        "        \n",
        "        ### clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        ### compute predictions\n",
        "        bbox_preds, label_preds = model_MNIST(img)\n",
        "        \n",
        "        ### compute losses over each grid cell for each image in the batch\n",
        "        check_xy, losses, loss = loss_yolo(bbox_preds, bbox_true_6x6, label_preds, labels)\n",
        "        check.append(check_xy)\n",
        "    \n",
        "        ### compute gradients\n",
        "        loss.backward()\n",
        "        \n",
        "        ### Weight updates\n",
        "        optimizer.step()\n",
        "        \n",
        "        ######### print part #######################\n",
        "        current_loss = loss.item()\n",
        "        batch_loss_list.append(current_loss)\n",
        "        epochs_loss = epochs_loss + current_loss\n",
        "\n",
        "        if batch+1 <= len_training_ds//BATCH_SIZE:\n",
        "            current_training_sample = (batch+1)*BATCH_SIZE\n",
        "        else:\n",
        "            current_training_sample = (batch)*BATCH_SIZE + len_training_ds%BATCH_SIZE\n",
        "        \n",
        "        if (batch) == 0 or (batch+1)%100 == 0 or batch == len_training_ds//BATCH_SIZE:\n",
        "            print(f\" --- Image : {current_training_sample}/{len_training_ds}\",\\\n",
        "                    f\" : loss = {current_loss:.5f}\")\n",
        "            print(f\"xy_coord training loss for this batch : {torch.sum(losses['loss_xy']) / len(img):.5f}\")\n",
        "            print(f\"wh_sizes training loss for this batch : {torch.sum(losses['loss_wh']) / len(img):.5f}\")\n",
        "            print(f\"confidence with object training loss for this batch : {torch.sum(losses['loss_conf_obj']) / len(img):.5f}\")\n",
        "            print(f\"confidence without object training loss for this batch : {torch.sum(losses['loss_conf_noobj']) / len(img):.5f}\")\n",
        "            print(f\"class proba training loss for this batch : {torch.sum(losses['loss_class']) / len(img):.5f}\")\n",
        "            print('\\n')\n",
        "            if batch == (len_training_ds//BATCH_SIZE):\n",
        "                print(f\"Total elapsed time for training : {datetime.timedelta(seconds=timer()-begin_time)}\")\n",
        "                print(f\"Mean training loss for this epoch : {epochs_loss / len(training_dataset):.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pS0A-3zLweFR",
        "outputId": "4b665641-a6c6-4245-99f4-83e59f174f69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.save(model_MNIST.state_dict(), \"yolo_mnist_model_Xepochs.pt\")\n",
        "model_MNIST.load_state_dict(torch.load(\"../yolo_mnist_model_Xepochs.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "7O8u0S3PweFR"
      },
      "outputs": [],
      "source": [
        "def relative2absolute(bbox_relative:torch.Tensor, SIZEHW=75, S=6)->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Turns bounding box relative to cell coordinates into absolute coordinates \n",
        "    (pixels). Used to calculate IoU. \n",
        "\n",
        "    Args:\n",
        "        bbox_relative : torch.Tensor of shape (N, 4)\n",
        "            Bounding box coordinates to convert.\n",
        "    Return:\n",
        "        bbox_absolute : torch.Tensor of shape (N, 4)\n",
        "    \"\"\"\n",
        "    assert len(bbox_relative.shape)==2, \"Bbox should be of size (N,4) or (N,5).\"\n",
        "    # assert bbox_relative.max() <= 1., \"Bbox input should be relative to a grid cell, not absolute to the image.\"\n",
        "    \n",
        "    CELL_SIZE = SIZEHW/S\n",
        "\n",
        "    cx_rcell, cy_rcell, rw, rh = bbox_relative[:,:4].permute(1,0)\n",
        "    \n",
        "    ### xc,yc centers relative to the frame coordinates\n",
        "    cx = cx_rcell * CELL_SIZE - (1/CELL_SIZE) * (cx_rcell/CELL_SIZE).to(torch.int32)\n",
        "    cy = cy_rcell * CELL_SIZE - (1/CELL_SIZE) * (cy_rcell/CELL_SIZE).to(torch.int32)\n",
        "\n",
        "    ### xc,yc centers absolute coordinates\n",
        "    cx_abs = SIZEHW * cx\n",
        "    cy_abs = SIZEHW * cy\n",
        "\n",
        "    ### x,y absolute positions \n",
        "    x_min = cx_abs - (SIZEHW * (rw/2))\n",
        "    y_min = cy_abs - (SIZEHW * (rh/2))\n",
        "    x_max = cx_abs + (SIZEHW * (rw/2))\n",
        "    y_max = cy_abs + (SIZEHW * (rh/2))\n",
        "\n",
        "    bbox_absolute = torch.stack((x_min, y_min, x_max, y_max), dim=-1)\n",
        "    return bbox_absolute\n",
        "\n",
        "def intersection_over_union(bboxes1:torch.Tensor, bboxes2:torch.Tensor)->torch.Tensor:\n",
        "    \"\"\"\n",
        "    Intersection over Union method.\n",
        "\n",
        "    Args:\n",
        "        bboxes1 : torch.Tensor of shape (N, 5)\n",
        "            Bounding boxes of a batch, in a given cell.\n",
        "        bboxes2 : torch.Tensor of shape (N, 5)\n",
        "            Bounding boxes of a batch, in a given cell.\n",
        "\n",
        "    Return:\n",
        "        iou : torch.Tensor of shape (N,)\n",
        "            Batch of floats between 0 and 1 where 1 is a perfect overlap.\n",
        "    \"\"\"\n",
        "    assert bboxes1.shape[-1] >= 4 and bboxes2.shape[-1] >= 4, \"All bbox should be of shape (N,4) or (N,5).\"\n",
        "\n",
        "    ### Convert cell reltative coordinates to absolute coordinates\n",
        "    bboxes1 = relative2absolute(bboxes1)\n",
        "    bboxes2 = relative2absolute(bboxes2)   \n",
        "    xmin1, ymin1, xmax1, ymax1 = bboxes1.permute(1,0)\n",
        "    xmin2, ymin2, xmax2, ymax2 = bboxes2.permute(1,0)\n",
        "\n",
        "    ### There is no object if all coordinates are zero\n",
        "    isObject = xmin2 + ymin2 + xmax2 + ymax2\n",
        "    isObject = isObject.to(torch.bool)\n",
        "\n",
        "    smoothing_factor = 1e-10\n",
        "\n",
        "    ### x, y overlaps btw pred and groundtrue\n",
        "    xmin_overlap = torch.maximum(xmin1, xmin2)\n",
        "    xmax_overlap = torch.minimum(xmax1, xmax2)\n",
        "    ymin_overlap = torch.maximum(ymin1, ymin2)\n",
        "    ymax_overlap = torch.minimum(ymax1, ymax2)\n",
        "    \n",
        "    ### Pred and groundtrue areas\n",
        "    box1_area = (xmax1 - xmin1) * (ymax1 - ymin1)\n",
        "    box2_area = (xmax2 - xmin2) * (ymax2 - ymin2)\n",
        "\n",
        "    ### Compute intersection area, union area and IoU\n",
        "    overlap_area = torch.maximum((xmax_overlap - xmin_overlap), torch.Tensor([0]).to(device)) * torch.maximum((ymax_overlap - ymin_overlap), torch.Tensor([0]).to(device))\n",
        "    union_area = (box1_area + box2_area) - overlap_area\n",
        "    iou = (overlap_area + smoothing_factor) / (union_area + smoothing_factor)\n",
        "    \n",
        "    ### Set IoU to zero when there is no coordinates (i.e. no object)\n",
        "    iou = iou * isObject\n",
        "\n",
        "    return iou   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "OWc1Cr8yrSH8",
        "outputId": "bd79409e-b55c-42a0-fa11-f609b48225cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE BOX : 0.00018\n",
            "MSE confidence score : 0.21326\n",
            "class acc : 98.99%\n"
          ]
        }
      ],
      "source": [
        "S=6\n",
        "for (img, labels, bbox_true) in validation_dataset:\n",
        "    img, labels, bbox_true = img.to(device), labels.to(device), bbox_true.to(device)\n",
        "    model_MNIST.eval()\n",
        "    with torch.no_grad():\n",
        "        ### prediction\n",
        "        bbox_preds, label_preds = model_MNIST(img)\n",
        "\n",
        "        ### (N,4) -> (N, S, S, 5)\n",
        "        bbox_true_6x6 = bbox2Tensor(bbox_true).to(device)\n",
        "\n",
        "        ### keeping only cells (i,j) with an object \n",
        "        cells_with_obj = bbox_true_6x6.nonzero()[::5]\n",
        "        N, cells_i, cells_j, _ = cells_with_obj.permute(1,0)\n",
        "\n",
        "        ### MSE along bbox coordinates and sizes in the cells containing an object\n",
        "        mse_box = (1/len(img)) * torch.sum(torch.pow(bbox_true - bbox_preds[N, cells_i, cells_j,:4],2))\n",
        "        \n",
        "        ### confidence score accuracy : sum of the all grid confidence scores\n",
        "        ### pred confidence score is confidence score times IoU.\n",
        "        mse_confidence_score = torch.zeros(len(img))\n",
        "        for i in range(S):\n",
        "            for j in range(S):\n",
        "                iou = intersection_over_union(bbox_true_6x6[:,i,j], bbox_preds[:,i,j])\n",
        "                mse_confidence_score += torch.pow(bbox_true_6x6[:,i,j,-1] - bbox_preds[:,i,j,-1] * iou,2)\n",
        "        \n",
        "        mse_confidence_score = (1/(len(img))) * torch.sum(mse_confidence_score)\n",
        "\n",
        "        ### applied softmax to class predictions and compute accuracy\n",
        "        softmax_pred_classes = torch.softmax(label_preds[N, cells_i, cells_j], dim=1)\n",
        "        classes_acc = (1/len(img)) * torch.sum(torch.argmax(labels, dim=1) == torch.argmax(softmax_pred_classes, dim=1))\n",
        "\n",
        "print(f\"MSE BOX : {mse_box.item():.5f}\")\n",
        "print(f\"MSE confidence score : {mse_confidence_score.item():.5f}\")\n",
        "print(f\"class acc : {classes_acc.item()*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "DLyoeZO4QYHB"
      },
      "outputs": [],
      "source": [
        "def non_max_suppression(bbox, labels, iou_threshold):\n",
        "    \"\"\"\n",
        "    - Get the highest pc number\n",
        "    - Discard all other bbox with a HIGH IOU (bc if the iou is low, it means  this pc number doesn't stand for the current object)\n",
        "\n",
        "    Args :\n",
        "        bbox : torch.Tensor of shape (N, S, S, 5)\n",
        "            Predicted bounding boxes with x, y, w, h and confident pc number\n",
        "        labels: torch.Tensor of shape (N, S, S, 10)\n",
        "            Predicted label\n",
        "        iou_threshold : float\n",
        "            iou from which the bboxes with low pc will be discard\n",
        "        \n",
        "    \"\"\"\n",
        "    def find_indices_max(tens):\n",
        "        \"\"\"\n",
        "        input tens : torch.Tensor of shape (N, S, S)\n",
        "        output batch_indices : torch.Tensor of shape (N,2)\n",
        "        \"\"\"\n",
        "        S = tens.shape[1]\n",
        "        N = tens.shape[0]\n",
        "\n",
        "        # Reshape to (N, S*S)\n",
        "        tens_reshape = tens.view(N, S*S)\n",
        "        indices = torch.argmax(tens_reshape, dim=1)\n",
        "\n",
        "        col_indices = (indices / S).to(torch.int32)\n",
        "        row_indices = indices % S\n",
        "\n",
        "        batch_indices = torch.stack((col_indices, row_indices)).T\n",
        "        return batch_indices\n",
        "    \n",
        "    assert 0. < iou_threshold < 1, \"iou_threshold should be a float between greeter than 0 and lower than 1.\"\n",
        "\n",
        "    N = len(bbox)\n",
        "    S = bbox.shape[1]\n",
        "    labels_prob = torch.softmax(labels, dim=-1)\n",
        "    bbox[:,:,:,4] = bbox[:,:,:,4] * torch.max(labels_prob, dim=-1)[0]\n",
        "\n",
        "    ### 1) finding indices i,j of the max confidence number of each image \n",
        "    ### in the batch\n",
        "    m = find_indices_max(bbox[:,:,:,4])\n",
        "\n",
        "    ### Getting bboxes with the highest pc number for each image\n",
        "    ### Shape : (N, 4)\n",
        "    bbox_max_confidence = bbox[range(N), m[:,0], m[:,1], :4]\n",
        "\n",
        "    ### Removing bboxes with the highest pc numbers\n",
        "    bbox[range(N), m[:,0], m[:,1]] = torch.Tensor([0])\n",
        "    for cell_i in range(S):\n",
        "        for cell_j in range(S):\n",
        "            iou = intersection_over_union(bbox[:,cell_i, cell_j], bbox_max_confidence)\n",
        "            iou_bool = iou >= iou_threshold\n",
        "            # print(iou_bool)\n",
        "            \n",
        "            ### iou to shape (N,4)\n",
        "            iou_bool = iou_bool.unsqueeze(1).repeat((1, bbox.shape[-1]))\n",
        "\n",
        "            bbox[:,cell_i, cell_j] = bbox[:,cell_i, cell_j].masked_fill(iou_bool, 0)\n",
        "    return bbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "test = non_max_suppression(bbox_preds, label_preds, 0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "iAUtKFv24Rhq"
      },
      "outputs": [],
      "source": [
        "############### Matplotlib config\n",
        "plt.rc('image', cmap='gray')\n",
        "plt.rc('grid', linewidth=0)\n",
        "plt.rc('xtick', top=False, bottom=False, labelsize='large')\n",
        "plt.rc('ytick', left=False, right=False, labelsize='large')\n",
        "plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')\n",
        "plt.rc('text', color='a8151a')\n",
        "plt.rc('figure', facecolor='F0F0F0')# Matplotlib fonts\n",
        "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), \"mpl-data/fonts/ttf\")\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "0ZjSvwrpPt3C"
      },
      "outputs": [],
      "source": [
        "def draw_ONE_bounding_box_on_image(image, ymin:int, xmin:int, ymax:int, xmax:int, \n",
        "                               color:str='red', thickness:int=1, display_str:bool=None):\n",
        "  \"\"\"Adds a bounding box to an image.\n",
        "  Bounding box coordinates can be specified in either absolute (pixel) or\n",
        "  normalized coordinates by setting the use_normalized_coordinates argument.\n",
        "  \n",
        "  Args:\n",
        "    image: a PIL.Image object.\n",
        "    ymin: ymin of bounding box.\n",
        "    xmin: xmin of bounding box.\n",
        "    ymax: ymax of bounding box.\n",
        "    xmax: xmax of bounding box.\n",
        "    color: color to draw bounding box. Default is red.\n",
        "    thickness: line thickness. Default value is 4.\n",
        "    display_str_list: string to display in box\n",
        "    use_normalized_coordinates: If True (default), treat coordinates\n",
        "      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n",
        "      coordinates as absolute.\n",
        "  \"\"\"\n",
        "  draw = PIL.ImageDraw.Draw(image)\n",
        "  im_width, im_height = image.size\n",
        "  \n",
        "  left, right, top, bottom = xmin, xmax, ymin, ymax\n",
        "  \n",
        "  draw.line([(left, top), (left, bottom), (right, bottom), (right, top), (left, top)], width=thickness, fill=color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "XTE3VutBPt3D"
      },
      "outputs": [],
      "source": [
        "def draw_bounding_boxes_on_image(image, boxes_dict:dict, color_list:list=[], \n",
        "                                 thickness:int=1, display_str_list:tuple=()):\n",
        "  \"\"\"Draws bounding boxes on image.\n",
        "\n",
        "  Args:\n",
        "    image: PIL.Image.\n",
        "    boxes: numpy array of shape (N,4)\n",
        "      Contains (ymin, xmin, ymax, xmax). The coordinates are absolute.\n",
        "    color: list, default is empty\n",
        "      Color to draw bounding box.\n",
        "    thickness: int, default value is 4\n",
        "      Line thickness.\n",
        "    display_str_list: tuple\n",
        "      A list of strings for each bounding box.\n",
        "                           \n",
        "  Raises:\n",
        "    ValueError: if boxes is not a [N, 4] array\n",
        "  \"\"\"\n",
        "  for key in boxes_dict.keys():\n",
        "    if key == \"true_bbox\":\n",
        "      color = color_list[0]\n",
        "      thickness = 2\n",
        "    else : \n",
        "      color = color_list[1]\n",
        "      thickness = 1\n",
        "\n",
        "    boxes = np.asarray(boxes_dict[key])\n",
        "    boxes_shape = boxes.shape\n",
        "    if not boxes_shape:\n",
        "      return\n",
        "    if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n",
        "      raise ValueError('Input must be of size [N, 4]')\n",
        "    \n",
        "\n",
        "    for i in range(boxes_shape[0]):\n",
        "      draw_ONE_bounding_box_on_image(image, \n",
        "                                    # boxes[i, 1], boxes[i, 0], \n",
        "                                    # boxes[i, 3], boxes[i, 2], \n",
        "                                    boxes[i, 1], boxes[i, 0], \n",
        "                                    boxes[i, 3], boxes[i, 2], \n",
        "                                    color=color, thickness=thickness)\n",
        "                                    #, thickness, display_str_list[i])\n",
        "    \n",
        "                              "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "tMV5UzxQPt3D"
      },
      "outputs": [],
      "source": [
        "def draw_bounding_boxes_on_image_array(image:np.ndarray, boxes:dict, color:list=[], \n",
        "                                       thickness:int=1, display_str_list:tuple=()):\n",
        "  \"\"\"Draws bounding boxes on image (numpy array).\n",
        "\n",
        "  Args:\n",
        "    image: a numpy array object.\n",
        "    ####boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
        "           The coordinates are in normalized format between [0, 1].######\n",
        "    color: color to draw bounding box. Default is red.\n",
        "    thickness: line thickness. Default value is 4.\n",
        "    display_str_list_list: a list of strings for each bounding box.\n",
        "  Raises:\n",
        "    ValueError: if boxes is not a [N, 4] array\n",
        "  \"\"\"\n",
        "  image_pil = PIL.Image.fromarray(image)\n",
        "  rgbimg = PIL.Image.new(\"RGBA\", image_pil.size)\n",
        "  rgbimg.paste(image_pil)\n",
        "  draw_bounding_boxes_on_image(rgbimg, boxes, color, thickness, display_str_list)\n",
        "  return np.array(rgbimg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "6GOOWyP5Pt3E"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "def display_digits_with_boxes(digits, predictions, labels, pred_bboxes, bboxes, title, nb_sample=10):\n",
        "  \"\"\"Utility to display a row of digits with their predictions.\n",
        "\n",
        "  Args:\n",
        "    digits : np.ndarray of shape (N,75,75,1)\n",
        "        Raw image with normalized pixel values (from 0 to 1)\n",
        "    predictions : np.ndarray of shape (N,)\n",
        "        Predicted label with the same shape as labels\n",
        "    labels : np.ndarray of shape (N,)\n",
        "        Labels of the digits (from 0 to 9)\n",
        "    pred_bboxes : np.ndarray of shape (N, 4) ??\n",
        "        Predicted bboxes locations\n",
        "    bboxes : np.ndarray of shape (N, 4)\n",
        "        Ground true bboxe locations\n",
        "    iou : list of shape (N,)\n",
        "        IoU of each bboxes\n",
        "    title : str\n",
        "        Figure's title\n",
        "  \"\"\"\n",
        "  iou_threshold = 0.6\n",
        "  nb_sample = 10\n",
        "  indexes = np.random.choice(len(predictions), size=nb_sample)\n",
        "  \n",
        "  n_digits = digits[indexes].numpy()\n",
        "  # Rescale pixel values to un-normed values (from 0 -black- to 255 -white-)\n",
        "  n_digits = n_digits * 255.0\n",
        "  n_digits = n_digits.reshape(nb_sample, 75, 75)\n",
        "  \n",
        "  n_predictions = predictions[indexes]\n",
        "  # Argmax of one-hot vectors. Shape : (N,S,S,10) -> (N,S,S)\n",
        "  n_predictions = torch.argmax(torch.softmax(n_predictions, dim=-1), dim=-1).numpy()\n",
        "  \n",
        "  ### shape : (N, S, S, 5)\n",
        "  n_pred_bboxes = pred_bboxes[indexes]\n",
        "\n",
        "  ### shape : (N, 4)\n",
        "  n_bboxes_rel = bboxes[indexes]\n",
        "  n_bboxes = relative2absolute(torch.as_tensor(n_bboxes_rel)).numpy()\n",
        "  # n_bboxes = n_bboxes_rel/75\n",
        "\n",
        "  # Set plot config\n",
        "  fig = plt.figure(figsize=(20, 4))\n",
        "  plt.title(title)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  \n",
        "  bboxes_to_plot = {\"true_bbox\":[], \"pred_bbox\":[]}\n",
        "  for i in range(nb_sample):\n",
        "    bboxes_to_plot[\"pred_bbox\"] = []\n",
        "    bboxes_to_plot[\"true_bbox\"] = []\n",
        "    \n",
        "    for cell_i in range(6):\n",
        "      for cell_j in range(6):\n",
        "        n_pred_bboxes_ij = n_pred_bboxes[:, cell_i, cell_j, :4]\n",
        "        \n",
        "        # n_iou = intersection_over_union(n_pred_bboxes_ij, n_bboxes_rel)\n",
        "        \n",
        "        # n_predictions_ij = n_predictions[:, cell_i, cell_j]\n",
        "\n",
        "        n_pred_bboxes_ij = relative2absolute(n_pred_bboxes_ij).numpy()\n",
        "        \n",
        "        bboxes_to_plot[\"pred_bbox\"].append(n_pred_bboxes_ij[i])\n",
        "    \n",
        "    bboxes_to_plot[\"true_bbox\"].append(n_bboxes[i])\n",
        "    \n",
        "    ax = fig.add_subplot(1, nb_sample, i+1)\n",
        "    img_to_draw = draw_bounding_boxes_on_image_array(image=n_digits[i], boxes = bboxes_to_plot, color=[\"white\", \"red\"])#, display_str_list=[\"true\", \"pred\"])\n",
        "    # img_to_draw = draw_bounding_boxes_on_image_array(image=n_digits[i], boxes = np.asarray(bboxes_to_plot[\"pred_bbox\"]), color=\"red\")#, display_str_list=[\"true\", \"pred\"])\n",
        "\n",
        "# plt.xlabel(n_predictions[i])\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "# if n_predictions[i] != n_labels[i]:\n",
        "#   ax.xaxis.label.set_color('red')\n",
        "\n",
        "    plt.imshow(img_to_draw)\n",
        " \n",
        "# if len(iou) > i :\n",
        "# color = \"black\"\n",
        "# if (n_iou[i] < iou_threshold):\n",
        "#   color = \"red\"\n",
        "# ax.text(0.2, -0.3, \"iou: %s\" %(n_iou[i]), color=color, transform=ax.transAxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "0ouE3mxQPt3E",
        "outputId": "82f85c80-d9c8-4ffd-b651-e8c568936d59"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABGoAAAD3CAYAAABFALKIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABemElEQVR4nO3deXxU1f3/8dedmUz2hQQICbvssorIorLY4oIo7tbWftWq37ZqW9u6fqutrdVWW62t/dUuWrdirbV1Q6wFFWQTBBGRsMmeQMgCZCX73N8fZxKSMElmJpPMJHk/H488YObee85JZu72ued8jlVUVGQjIiIiIiIiIiJh5wh3A0RERERERERExFCgRkREREREREQkQihQIyIiIiIiIiISIRSoERERERERERGJEArUiIiIiIiIiIhECAVqREREREREREQihAI1IiIi0i0tnXg6xw9kh7sZIiIiIgFRoEZEREQ63bsjTqV8//4m733x5P/jszvuDqq8dddeT/Y//9XkvXM/+4S4QQMB2Hz3j9j5m98F11gRERGRTqRAjYiIiPR4ntracDdBREREBFCgRkRERCLQkXUfs+zsc9j71+f4YNrZLDtzFjn/es3nujt/81uObfiEbT97iKUTT2frzx4CTvTayf7HP8ld9DZ7n/krSyeeziffvBWA5XPmsufPz7DqoktZOuF0PLW1FH36GWuv/hrvTZ7G6osv48i6jzvtdxYREREBcIW7ASIiIiK+VBcWUltaxpxVyziyeg2ffvcHpJ/7ZaKSk5usN/KH3+fYJ5+SecnFDLz6ypPKGXjN1RzbuImYfumM/OHtTZblvr2Y05/+I+5evaguPMIn3/w2E379CL1nzeTImrVs+s7tzHx3Me601A79XUVERETqqUeNiIiIRCTL5WLYd27BERVFnzmzccXFUb5nX0jrGHzd14nNyMAZE8OhNxfRZ/Ys+syZjeVw0PvsM0kaN46CD1eEtE4RERGR1qhHjYiIiHQ6y+nErmmaF8aurcXhOnFpEpWS0uS1IzaG2uPlIW1HTEZGw/8rDh3i8H/+S/4Hy5u0KW361JDWKSIiItIaBWpERESk08VkZFBx8CAJw4c1vFeRfZC4oYODKs+yrDaWt7SgcZv6kXnpAsY9/GBQbRAREREJBQ19EhERkU7Xb/4F7H7qz1TmHsb2eChcvYb8Zcvod8H5QZXn7p1GRXZ2K8t7U5Gd02oZmZdcTP4HyyhYuQq7ro66qiqOrPuYytzDQbVJREREJBgK1IiIiEinG/6dW0k5bRJrv/p13j99Ojt/9TgTH/8ViSNHBFXe4Ov/h8PvLuG906ez9cGHT1o+4KrLKdu1i/cmT2PjLd/xWUZsRgaT//j/2PPHv/DBtLNYPvNL7Hv6WWzbE1SbRERERIJhFRUV2eFuhIiIiIiIiIiIqEeNiIiIiIiIiEjEUKBGRERERERERCRCKFAjIiIiIiIiIhIhFKgREREREREREYkQrtYWJiUlYdvKNSwiIiIiIiIiEkqlpaU+3281UGPbNjU1NR3SIBERERERERGRnsjpdLa4TEOfREREREREREQihAI1IiIiIiIiIiIRQoEaEREREREREZEIoUCNiIiIiIiIiEiEUKBGRERERERERCRCKFAjIiIiIiIiIhIhFKgREREREREREYkQCtSIiIiIiIiIiEQIBWpERERERERERCKEAjUiIiIiIiIiIhFCgRoRERERERERkQihQI2IiIiIiIiISIRQoEZEREREREREJEIoUCMiIiIiIiIiEiEUqBERERERERERiRAK1IiIiIiIiIiIRAgFakREREREREREIoQCNSIiIiIiIiIiEUKBGhERERERERGRCKFAjYiIiIiIiIhIhFCgRkREREREREQkQihQIyIiIiIiIiISIRSoERERERERERGJEArUiIiIiIiIiIhECAVqREREREREREQihAI1IiIiIiIiIiIRQoEaEREREREREZEIoUCNiIiIiIiIiEiEUKBGRERERERERCRCKFAjIiIiIiIiIhIhFKgREREREREREYkQCtSIiIiIiIiIiEQIBWpERERERERERCKEAjUiIiIiIiIiIhFCgRoRERERERERkQihQI2IiIiIiIiISIRQoEZEREREREREJEIoUCMiIiIiIiIiEiEUqBERERERERERiRAK1IiIiIiIiIiIRAgFakREREREREREIoQCNSIiIiIiIiIiEUKBGhERERERERGRCKFAjYiIiIiIiIhIhFCgRkREREREREQkQihQIyIiIiIiIiISIRSoERERERERERGJEArUiIiIiIiIiIhECAVqREREREREREQihAI1IiIiIiIiIiIRQoEaEREREREREZEIoUCNiIiIiIiIiEiEUKBGRERERERERCRCKFAjIiIiIiIiIhIhFKgREREREREREYkQCtSIiIiIiIiIiEQIBWpERERERERERCKEAjUiIiIiIiIiIhFCgRoRERERERERkQihQI2IiIiIiIiISIRwtbaw5MgR9uzfjyfElcYAY4FNQF2Iy+4ow2njjxVi8aefTnV1dUjKqigp8etznAAcBI6EpFbfBgAJwPYOrCNSDB48mKSkpJCU5e9nGEqd8X1objRQ6q03Upwehn1RQqu77Iu1wFDMubOj1e+LR4ic87X2xa6vq++LYnTnfTEeGAXsBOxOqtOFuc/YDNR0Up3aFwMXrmvUZFo+93fnfbGnGDx4MG632+eyVmMPu/bv54wzzgh5g8YAW4BzgOKQl94xcjA7yJZOqu+uutBdEvv7OeYAPwNeCFnNJ/sVcJb3p7tbv34948ePD0lZHbUvtqYzvg/NrQFWAPd2Yp1tqQvDviih1V32xaPAi8BUOv4Gon5ffIHIOV9rX+z6uvq+KEZ33hdnAMuAiUBobn/blok5zl8IHOqkOrUvBi5c16gX0/K5vzvviz3F+vXrGTFihM9lndlJpMt7lc67ab0rxOUNAWLbWCcKc7IYE8J6y4DsEJYnIiIiIiIioRNDaO8BwfTKzQ9xmT2JAjU9xN+B6bT+FNYCHvL+hIIFLAIuCVF5IiIiItK9WB1QZmcNWxLpLsYS2pEjFmYkRST1ku9qFKjpQX4HPNDK8u2Y7vUvh6i+P2Ly0YiIiIiINDcJOBbiMtu63hWRk20C5oSwvKUhLKunUqCmB6kCSlpZ7gEq21gnEJ01tlc63xjg1x1Q7migLzCulXWuw+TqEBERka5tL/D9EJb3GBAdwvJEeoo6QncPWF9eqDyGuUfoTE8A73dync0pUCMiAUsBLgD+RmgPxDVAOXDYx7J44CuYMbQiIiLS9RUD74SwvPtDWJaIRIYzMfcB6zupvmswuWnDTYEaEQlKLfAtQttzqrWM+pmYQI2IiIiIiPQc/6Hz8t1c0En1tMUR7gaIiIiIiIiIiIihQI2IiIiIiIiISIToEkOffgFkhKHeVcBfw1CvSKSJBf7AiSk0+2IOHs8Q2hw1w4BEIL3Z+6swXR5FREQAhgDPhaCcWuAW77/SfZ0N3OTHeh11ffMQsDuE5YlI6HwfmNjodS/gZkI7C1ZLhrSyrEsEai4FbGBXJ9Y5zVunAjUi4AauBz4ESoEk7/u9MLOFhYoLkyw4tdF79ftidwjUJAMXh7C8VYR+WlMRka7ARdNzRTBSgTOA29rfHIlwwzF57tqaMjjU1zcO4ELgzyhQIxKpvgxMwExRDma/jaf955i2pAK9MUnVfekSgRowT00eC3JbF+aPHYh/YG5Ok72vHZheBcktbtGycvSkRrqHW4HtwAxgGXAFHZ9MeFEIyw+3ocCLISorGZMFf22IypOOEcz5p7n68088pldbCoHdQFQDFe1sQ3fjJLjzeXsdx8xuJ+23C7iknWXUn8ukZzhK29+ZUF/fuAntlMci0jHeB270/j8H+B3wQgfXOQNz79OSLhOoaY8LgDcD3KZ+iMdXG71+CjP8I1ALgMVBbCci3csmYGoIyokGikJQjnS8YM4/zdWff+oVBrj98/jX5b8nmYS5aets3yB0wVoRERHpvnpEoMYCyjgxhMIff8QMKfiR9/UK4DfAGwHU6wDWBbC+iHR//h6DWhPK4WbSsYI5/zRXf/4pBh4NsKw/cuLBg5yQBVzZyXWuQJ+FiIiI+KdHBGrA3NjswP+L23LMRfEO7+sa4HCj1/6wCG0isq7uLkxSpkzg151cdwnw806uU0QEAj//NFd//jkaRFnlQdbZ3VUS2Pk8FDTkSURERPzVYwI1En7fAMZggibTO7HedCCOrh2oSQDOCnGZOcD+EJcpIiIiIhJKY8aMITW1o1O7ti5x82YGJCVx1pAh2LbNunXrqKvTI3npOArUSKfbAszsxPpuwEyL2JWNBN4LYXluTK+me9taUUREREQkjH71q19x0UUXhbcRM2YwbtYsrn30UTweD6mpqRQXtzRfj0j7KVAj0gV8ipnhJ1Q+DGFZIiIiIiIiEjqtBmpG0/qUUcGK7YAyJTTuB74VorKGYb5g9d+hwSEqtyeyCe002KFIaCsiIiIi0lkKCgpYsGBBWOr+y5YtjJ81Kyx1d6ZrgdtCUM44YBDgz19sFXB3COrsbloN1JRiZinoCO8CVR1UtgTnKSAphOUlAjGc+A6NxMyC9WoI6xCRjjUOuDAM9dYCv0UzXImIiIhRXV3N2rVrw1J3WVhq7Xz9gRHAX9tZziBMPsy2YgnzgVHtrKu7ajVQc5Duk8PCAZyK/xf9CZgZGsZ4X0dhZisa0+IWvut0Yr6omcChALYNpRjMB92b1tv/up/l7QMq/FgvHUjlxHdoAfAOZrpYEekapmByPH3RiXXGYo6Z/4/Q9iQTERERkdbl0/4YwCxMkKatcurvF+VkPSZHTSKwOYD1Le+/lzR6/RCBJ6W1MD1VzgBuDHDbUBnr/XciZual9rAwuVLCE8uWSOPgxL4SKpaPMi1vXfhYJh0vnxPHkc4wA1jWifV1BY33CweB9TTytU+J/ywrNH89y7axLCskn4VtawArtP977Wh7FREJoaD3WY+n4SdU57OgjqK2faIdtt0p59aedv5u6fftidcyPSZQU4LJkeLvTvkKUMiJMXrbgZ8BLwdQpwPT+yQ5gG06wiZM76A1wAPtKMeN6cImAub7kEdo893EA5NpmicpzlvHFZgD9Dbv8kD2RZGu7inMfhALHAlgu/r957IA64un44Y+dzVPPfUUX/3qV9tdTuLo0Tz1wAP8NgRljR49msOHD7e7nK5sEnCsnWU4Q9AOEfHfU0AwR8C4r34VXC4yPJ527/cALxFkHpbf/Q7+/GcsYH9JSafkfIyiZ/UuXodJl9GYr/uDUIoDFnZQ2e3RYwI1NiZY4+8OVYsJbpR4X3uAykav/WEFUF9HqvP+VBFY+5tzN3s9kNaHMU3EHFzebrT+jcCcdrTBlxrgKsxnJp3HwuQ0ugMTyAyFx4As4LlG7z0AFAN/A17E9I5r/l0U6c6uwwxhnQb8hOCC/8HsM1cC04PYrruJiYkhObmdj1xsGxwOYmNiiG1nWbZt43CoL8he4Pvt2P5GzAMA5UsU6TwxmGvGBwPc7oE772Tq1KkcPXKE66+/vl1teIDgJra5A/jfiy/mG9/4BrZtc/NXvkL58ePtaou/aoiMe8rOkAD8C3it0Xu+7g9CqT0dGTpSjwnUSOglYJKMLsR3pHckJsBV/8yvFpOgOpTPANOBeaj7ckeZiXniGIsJzFwB5GJm9KqXCvQLUX1uYDcml1G9W4AC4P0Q1SHS1Xzg/fcgJslfIM7FPGRY2Y76v6BnPc1rSVVVFS+99FLQw46uOX6cT1au5Iu6uqC2HzlyJDNnzgxq2+6omKbnikDNCVE7RCQweQS+795yxhkwfz5VBw+2ua3D4SA+Pp7LL7+cnTt3snfv3iY9EG/xsf6kSZPo3bs3hYWFbNy40We5HwEzhw2D+fPB42FJVBTFAf4e4p/tNP2O3M/J9weh1Pw7ESkUqJF2sYHvgs8D1XOYm/ibva/PxMz49FgI65+BuRGR0MsFzvf+1I8J/Q4m4Nb46fz/EtqbOJ30RHz7jBPHU38twgQ6A91OTlZRUcHNN98cdKDmAuDZZ5/lhWefDWr7G264QYEakU7Sn9Bc20RjrqH60nKgPVQPu8IlitYfItQR2oe0bYmKiqJfv3786U9/4u9//zv//Oc/ycvLa/HYHR8fz9VXX82UKVP46KOPWgzUiHQ2BWpExKczGv0/GTgKnIOJcs/A5DwCk3BbyaVFRESkO6jv3RsqFvBGCMuLNBOA7FaWHwIGdFJbwARq+vbtC8C8efPIyclh7dq1FBf7fhR49tlnM2XKFAYOHMiaNWt8riMSDgrUiIiEiQVsxeTdCXQ7N/AWwT/xewj4U4DbfIIZbtjR3N6fvZwYk/0FJlAoIiLSUTYQ2qCCG9iBGTreVj+NrpoefDMmFYIvVwN3dmJbAKKjoxkyZIjf648ePZpevXp1XINEgqRAjYi024WYhM4dYSIm2eNvG713Mya/QAUma39XTrDWDzNM8LMAtnFhEnn/ieCe+v0Ck2MqUOmY8cGrg9g2EMOBezCJc2sx369xHVxnVzUcuK+V5fX7T0ck4PsBUNQB5YqIhEsNpgdIqLgx1yiFIS43krT2NyvqxHbUS05O5pxzzsHhcLBhwwa2bt3K8VaS/qalpREdHd2JLYxMQzDXCuOATNp/3TCMtmePfAI4GzNEsL6+DOArwNhmZSUS2oeFh2j9+ikSKFATgEmY4R/+sjDjNsHMeHRxqBskEiHGYZ4WLeuAsusvchrPkxIPnIpJWB3U9IoR5n1gcQDru4E/YIImwQw7uyeIbeqtBl5ox/b+mAHchZnpqxpzYlagxrc+wP8A/8Ekb2+ufv9JDWGdMZjcYPehQE2gHA4HvVNTGT16NCkpKRw6dIgDBw5QWFgY7qaJiHR5cXFxDBgwgGnTpuFwONi7dy+HDh2ipqbG5/qWZZGcnEx0dDSxsbEMGTIEy7KCzkXWlbkw1wpxmIlEAr1uONW7zSrv67XAtja2uQqT3+hwo/qc3jY0rt+FufYI1bXMcO+/XTpQ4yS4aUBDpX5K7UhQAtzg/QlEnPffWcCUELYnGDG07/OsjzUneMtJ8r5O8r26plDuYbIx+WpCrT4Z6v2YWW8AfocJmr7YAfWJdDW1mECpr2Fw9fvPjSGsLxPICWF5PUlMTAxTpkzhwQcf5PTTT+f111/n2WefZcmSJdTW1oa7eSKdLtT3Gs4QliVdT0ZGBhMnTmTs2LF4PB4qKytbDNIAuFwuUlNTiYmJoV+/fsybN68TWxtZdmGu4+8Grifwa/pfAWcFsR3Ax42224rpXdN48pk1mN459wZRti/1v2OkazVQM4nAepCEWhHQO4z1Nza27VVO4gCOYE5AfyewC+WOuFz7HmaGpvawgP82e72vlXUXtbM+ERGR7mL06NH0P/tsJk+eDJhEl8XFxRw/fpzly5eHt3EiYTCJ0N5rWHRM717pGjIzMxk2bBgej+ljevToUUpLS32u63Q6SU9PZ/bs2aSnp1NZWUl+fn5nNlekVa0Gaizgq8CmTmlKU+cAvwxDvS0JpgOcp9F2dpBlhMo1QGw7y3Bjkqxdh/lODMMEYqYBvg+BUNbOOkVERLqL8ePHUzl5MpZlAeB2uzl+/DhHjhwJc8t6rihMMtSuPNBhFqbnXFeUBVwZ4jLD+ZBZwuvYsWMcPnwiLfP8+fMpLCxk165dVFVVNVnXtm0qKyspKCggOTmZ/Px8lixZ0iOHPXUFNxO6nvupmI4U9UOzBmCmkZ/hfd03RPW0V5s5avZjspV3tpFhqLM7OxCCMupzHdR/Jxze978AfE94Jz3BdzEHuHTg1x1Q/hhMgrOfdEDZIiKdKSYmBiv2xGMTh8NBZWUlZWV6rBEuHuBZzEV6ewwGbsHkPAi0rBjg55hzaCDP85OAH3MiH2JXVEl47jOkezp06BDr1q3jjTfe4OKLL2bkyJGcd955lJSU8Oqrr560vsPhICoqCofDQW1tbYu9byS8fo8Zdh0qc4AzgGe8r28EyoFXGq2zPoT1BUvJhEW6gATMuM9QScQEV/wp8ygnZ/MvxYwXnY4ZFpiM6bE1PYRtrNcL8/uPxSQoO6MD6og0Z9B6jqcoTKB0AsGNx4/FBL9a+vxtYB3tv3ERkaZiY2NxxsU1vC4pKaGoqIjy8vIwtqpnq8PMKugrx1MgZgDfDLKsZOBBTF6G7QFsl4kJ1IiIcfToUT777DP++c9/MmvWLHr16sX06dNxOp1kZ2eTk5NDTGEhVFYCJkdNfHw8LpduiSPZy81eT5s2rV2f2Sk5OYzNz2etdxjyZVu3UhQVxdoRIxrW6YX/917Hjx/n008/Dbo9LdG3UqQLGAm8F8Ly3MBoTALS1kRhZiO6rtn7WzDDE0uAb2O6XV8PzAxhG+s1T4baE5KYvo7pdtlaoMSFecLga6aftrgxNxQ3+VhWP1tdKuopJ+IPfyd1tYCBGRl4MjIabhK2fPIJ2V98QVVJSZvlRNXVNWzntm2iMft/y2kyJdI48N37pf6zd+P/9ynQ7araWC7SXRQVFbF06VI2btzI6aefTkZGBgsWLODcc8/lN7/5Df0XLiRnzx5cLhcul4vo6GicTieWZTUMS5XIZVkW7777LikpKcEX8uij8MILrFrlnaPqoougTx/mPxfcpORbt25l7NhgMtq2ToEakS7gU+DMEJb3IaZ3SlvT0j1DeGd+68luwUxP7Us0Jnh1LqbnS6A+w0yx/Rsfy8YAG4MoU6QncmP2RX9mOXQD/OhHcP/94L0ZONPjYZrHw188bYdcnS+9BK+8ggXsqK7GxgTS2wq4S+T4OvCXFpZZmK72gWTHsLw/O9rYrgroQ/t7DYl0BR6Ph2PHjnHddddx6aWXMnfuXKZPn05mZib/93//R9THH1MWE8Mtc+eyxxuwAZOzRvlpJJIoUCPSBdiE9gLLxiQKb6tL3zDMQWIJ5mLwZaDCu6y+58VfMOPk+2CGQ7WlDDifrp24MVijOflvlICZgrBx0KwPZgpCXz1e4MTfvobgvhc2ZmY5X9vqQl4kMG7gO5iehj6XR0UxadIkHty0iaS778ZuNP3rTTfdxO7du6nxI1Azf9487r/vPmzgigULuKWwsEdfxPk6ngZicKgaEgAHpifqgmbvJ2Bm1LyelmfS9KUP8CYmWFfYwjrjMb0vRXqao0eP8sYbb7B27VqGDRvG3LlzOe200xhTUsLoMWP43ve+R1VVFUlJSQAUFhZq9r0u5u233+bhhx8OeLuvHzrEhQUFfG2GSR/82PbtHIuK4uEZM9rYsqm//OUvjB8/PuD6/dVlzvFzOJG8NlBjME+g7wpgm2FAP8w86205CLwURLtE2uP2228nOjqQTtInZC5cSP8BA+g/Z07rKy5eDJWVTJ0/H558kkk+Vml8ePLn8FZB4PtiV53NorlSYEWj1xYwGTPrxa5G74/DPCFt6cYPTK+o5rmDpPuLBr6H+e6Audl0Anfge6hcIOcyf3yO6ZElTW0B1rawLC4qilPGjsWzZQv2sGEwYwYej4e8vDw+cbvZhgmctmV0nz4wYwbYNhvdbvIxQxR7qubH00Cdg8nx4nQ6ueP736fOGUzGrxMGHzyI8x//aLWs0z//HPfKlcy69dYm78dUVmI9+SSn33QTg9LSOHjwIC+91PaVZX1yzY20fD7QQA7pqWpra8nPz6eoqIjc3FyOHTvGpk2b+E52NoPT0wEYOdJMX/PFF1+wfv16duxQauuupKCggLVrWzr7tmwW5n6kfttjmHuNtQWB3XF09EQAXSJQsxs4BRjayjqjMAETX3+uBMwTr+sDqNMG4vzYpi+wEwVq/JXety9j0tJCVt7Qigqs/fsZM2oU1S2MK80sLsaVn8+YRgmiGtu9ezfV1V2rH4FlWfz0pz8NfnzmihUwa5YZo9mavDw4ehR+/nN4/31o/NTXtmHHDhg8GI4fpyo/n91tVJsA9CfwfbG7BCQOYnrK1LMwuWKewwxhqPd14FXM8CTp+hwOB6eOGhVUPqHmkjweHtmxA8ewYeB2Q0UF7N/PL0aNahhO00R2NjidTMlsOl9CaVkZ2dnZAdU9EPgXCtQEyuVyMW7cOKJeew0w3evr6urYtGkT5eXl6mofpObH00D9CpgKuJxOfvHwwxDkg48Ga9bAq6+2XtZzz8Fnn/Fo83NvURH8/vfceccdMGYMa9as8StQI4FxYR7ediY3CpaFW3V1NQUFBSxfvpzVq1dzSW0tCUOH8vnnnzNkyBBs2+bTTz/lww8/JDc3N9zNlTBJIfDjQ+zevZCcjHv37qCOLa3FNqCLBGoubmO5hZmZ5laa3uw03v5FzFPqUF8O/YrQzsbT3d15553ceVcg/SnasGYNnHMOmzZtav3C6P772bp1q8/FY8aMYfv2QOZZiCzBjKm1zIbYbXS3t2zbrJeQAJs3N11YVYWVkoL197/DihXsvvde2kqj1ZH7okikSkpMZMuWLeAItl9oI0VFkJoKixbBmDENx0BaOgZ6E+TRLEHesrfe4pJLLgmo6kXBt7pHi4mJ4Utf+hLRjW7OKysref755ykoKKCuTvOrRYp25ajweLDAnFdbOrfa9ol1fGyrG/qOY2OGirXWU1W6v5qaGmzMMKdNmzZx8cUXY1kWX3zxBVu3bsXjxzBU6Z4uBeqviiz8u0+xvv51wPReDvbY0lo9XSJQIyIte+mll7jtttsC2mZpeTlrNm7kgT//udX1/lhRQS/b5ppevU5a5rZtcqqrA5qhQkSkJxk1ahSXXnopw4YNw+kdDlNWVsbWrVt56623ulxvzu7ud7/7HQ888EBQ206rq2NRdTUD0tNb7OF7bXU191dVMabZOTXZttnnDeJIx/g7ZkbFcCkPY93iQ6OgrHo1CpjRMfV3U9uBn3HytODNLV2yhKlnnMGOHTuYNm1aUPUux4wc8qXHBGrigbcJ/VP80Zgx4m/7WGZ565WT1dTUcOWVV1Jb68/I/JaNOXaMX9bWcuWll1LbwtPquTk5/M+RI1w/f37De9OmTeMnP/lJu+ruTMNp+h2zbJv4r3wFXC5m5+Tw95KSgMobASyrq6OkjZuEakzC2hIf5fszy4mINPXXv/6V17xDYIIRX1PDP2ybW7/9bbITEto8Bj6wYQPFbje/9R7/XnzxRdJCOPxUWhYXF8e4ceO44ooriI2NxbIs6jwesrOzee211xSkiUBVVVU+z3f+qB96X1Ja2mJS9grMdWjzOhSg6Xg1aCp7OZmCNN1LcnIy/fv3Z/z48Vx77bVNplu3LAvbttmyZQuTliyBZcuabFuDSfYO4AEqG71uSV18PCQn40lIaHPdFstoZVnYAzVzgUE+3p+IuRG8kbaDK5Z33XOB9BbKigIu9KM9NrAQ/2c+GYTJu3G4heUvetslTXk8Ht599912X6gew+xM7777boufWV/M1JTvvPNOw3vOdiYN7Gw1NP2OWYDdty+43VSUlLT4/WutPBHpfNu3b29yLApUsvffD1esYDttHwNvwSTIeycnBzBDbqRznHHGGcyePZtRo0bh9AbRjh8/zr59+1jW7AJRRDrfQMJ7jb4UCCxTmHSEo0ePUlhYGHSQViLH5MmTOfPMMznttNOYN29ek0CNw+HA4/FwyimnkHTwIBWrVnHKwIHk5ubiqKpqechqGIU9UHM7JvP+sWbvx3h/fupnOTHAtZgpCn0t89B2QlIX5qb+u0Cxn/XW56i5uZV1NK6/keJiyMmBqir623a7pwLu6/23Py0H104etNP17Kfpd8yyLC7//e9xp6Sw+rnnuHnVqoDKa8+UphJ5XC4X6em+wtQtrJ+XR3J8PP0TEk5a1q+2FvLyyMzMJMGy6HX8OI7iYvpnZODMzaVXcjL94+LarKOsrIziYn+PpN2cbZvjnsNBcmkp/dtRVH2gph9m1pu2joEx7airO4sCvz+H+GPHICcHq7iYAdAkKXQ0JnDe10d5X5s1i/NOO42EoiKTW8jjoTw7m4KiIg5/8knA34Ne5eWQmwv9+gW4pYj4Mgl4msAnLIgjNDOu3QC8F+A2B0NQr5xgWRbZ2dns27eP/Pz8cDdH2mn27NlcffXVpKenNwm8WZaF2+3G7XYzcuRIrMxM8hMSuPDCC1m+fDlx+/ZBB8/gFIywB2oA/onpOdNYfdLRQfjXo+Yo5oDXWjLhtsoag5KMdbiHH4aHHyYa2pwhyF+WH2XpxCbd2YgRI1pMlu3TmDHcf+ON3O8rsffWrTBuHFuzsiAlpSEZd052NgwYwBMPP8wTN9zQZhWPPvoo997bnjlZug+7pARr8GAA7vP+tIcFfNDsdWvHwOfbWV93NAH/n2RbP/wh/PCHJGOC5s2TDFrAG742/PnPzU8j/X79a27AXK8E7NVXYfVqE/QTkZAowb97jcZuwMzW2J5BM5a3jEBUA0n43+tfWmFZDb0tSkpKqKioUGL3buDTTz/1+TnGx8czatQoLrzQjK+xgH79+vGb3/yGrVu3MuS732XvypWd3Nq2RUSgRnqG2cDP7r+fW779baqqqxk1ahQ1NcEPwlmwYAFXDR3KzCef5JuzZ7N248YWuy3q0Oufq4HfNHrdC3AAvm4L6occsmABVFUxooX1GovBXGTk0PQCpxoYhYZkSfezBDg1KYmsrCwclsVDDz3En/70p6DLSwKyMD1RdwFTMA87Wtt/jgddW/e1Gf+GQwM88cQTXH3VVRSXlDB27FiW2Tb/AP6EOQbuwPTm3dhom+9+97tcddVVDB16YvLNHTt28Mgjj/Dee4E+QzeuBp4IakvpKLGxsUydOpXRo0cztbYW99/+xp7t28Ht5vjx4+Tm5rJs2TK2bNnChg0biM7NhaqqcDc7It1666386Ec/6tQ6Y5YuJen228nJysL2kQB6wYIFbNy40ceWkIs5/gbDArZievD7ezSoP9ZLaMTExDTka3v99dfZu3dvmFsUWfy5pm8uGXNODGS7UPcP/e9//8v7779/0vsul4uUlBSOHDnClClTGFhc3DDiYvTo0bgSEoiPiyMjOTmipmiPiEDNzLPP5rmbbmry3sBNm4j7y1947g9/8HnwbMyybeJuu40ffOtbXDlx4knL/S0r5dAhuK+9zzoj0/2YqcPaw4n5wvwYyMfMN28BT3FydP8HQFGz9wqAsqQk6N8fqqrItaygnwq4XC4SRo5k0PjxWA4Hp82fz7K9ezmk8aV+mz5tGj85/3yGDh1KYWEhSUlJnPrxx8Q/+yw/TkigrKyMm2ybeOB3PrZ3AX8Eor79bdixg/xXX22zp8Ak4JuYHgX1gZrhwF0omWKoPPHEE3z22WetrvOL3FxWvvIK/8nKOmlZZlERD9k2t916KxVuN2fv2sUVRUX84Bvf4Iljx/jXM8+wevlyn+W6XC7++Mc/EhUVFYpfpVuoAnItCzIzweGgJCEh4G72jdXPHHLO1Vdz7vjxTCwvh0ceIRc9ZQ1EDf4PdziekgL9+2PHx3MIqMU8hT+EuSi1gULva8uyiI2NZfrll5M6bpxJMujxUFRUxAtLl7IuOzvoz78oyO0kdBITExk1ahSnnHIKmZmZ9OrVi8mTJ5Oenk7vnTvhb38jo18/iI6mtraWPn36kJaWxjnnnMPGjRsZ+N57sNhX3++2ZQIPN3svHnPufIKWA7J9MefrZwj+odWPMIGJjjAE07Nk3Icf0r88hHMjZWbCL37R+jqpqWBZZHqPz8253S1Pm+BPSoWWWN7tjwRQRkGQdYlvffv04fTTT8e2bc455xz2799PUVERR44cCXfTwm4xkBfEdldjAjzNj1OteQL4F/CPIOrzpaU8fJZlUV5ezl/+8hc++ugjLtmxg4u877tcLhwOB6eeeir/e+GFPP744ziOHzfD1sMsIgI1w4cPZ3jzrvRvvQXPP8/111/v8+DZhMcDP/gBX/7yl+Gii05e7m9ZW7fi6YaBmmRM/p7RISovEXOhmuh93YsTT3JjMInZ7qNjLypPOeUUhg8fTmZmJpZlMXTIEGJilI2hLZZlkZaWRlRREcNPPZWMa69l+PDh7Nu3j+TkZBI8HkoXLmTH9OmsWLGCOVVV9AJe8FGWG/gDwIUXQkICxa++6nO9xuqHKL7AiUDNDEygRkLj/fffZ3EbNwL3ABs++YQXPvnkpGVjgIeAl19+mWLMBeVlwNEXXsADlK9ezdHVq32WG+Vyme+Dy8Xwbdu4OMjfIVTHqu7sggsuoO/s2QzMycHzyCPhbo5gjq/x8fGcdtppTJgwgcREc5asq6sjLy+P9957j4MHNRC3K8vMzGTOnDlMnz6dgQMHkpSUxJAhQ3C73ViNJkcoLS2ltrYWy7I45ZRTGD16NP379yequDjoQE0ycD3wLk2vucA8OGvpCijJ+28vmuZX8kcUcAHwKB0XqHFh8r3EVVbC0aPYts27775LTSuzgta3q9UHPAMHwvTprVf+8cdQWwuLFoGPB7lnHztGHx+bTcL8vevPcZ8SeO8DCS93dDQpKSmA6VHRu3fvLjfRSEfJ8v4EaizmWNTWvUBjDwOraTqcuyPYtk1NTQ0ff/wxOTk5jKioYG5KCts2b2b8+PE4gEGDBnHxxRezdu1a3B9+GBG9HyMiUANmFqDGw1Zc5eXE2zbFRUV+BWqSbZvy8nJqi4pOWuxPWdHR0cQG3/yIVt/xutL70153AOs5kdPnq5xIvpxJ55ys5s+fz8SJE4mtq8MD5BcUtGsYVU/gdDqJi4vjrLPOInHlSnr16kXCwIGUlZVRVFREcXExafn59E1I4Ic//CFZWVk48/JAY3Z7tGpMb7oXMRf836SV/Bq1tbhuNBnH5ldV8eV21FvUjm17gkmTJmH174/j4MGAb76kY7jdbgYOHMj3vvc94uPjcXivN+rq6ti/fz979+6lLAKTFYr/Ro4cyfnnn8+sWbMaPl+Px2NmsKyqIhooLi5mx759lJeXEx0dzciRI0lNTTXD4AYObFfAw8b3Ndc3aLlnxgxgGWZ4XqC97pIxD1g60i7gEuDuK67g0UcfxfZ4+GpqaqvJ6OvbVYrv4FM04M7OpvSSS1qt24XplVR86aUN7zmdThK9ifZ/UlbmsxeSGxOoqT8v3khgN6fdjZMTie6bi8ME1OqXuzGBtpbWb4mrvByKirCKi0nG9GpsT3+HY8eOkZWVRXp6OgUFBeTm5qo3TQ9x6NAhjgJHPR5+97vf8dRTT+HC9JgcO3Ysd999NwmffKJATWO5ubkMHDiw4fXFts0LQFrv3m3uiA7giG3z9WuuYbGPiLg/Zd111108ev31QbY+sm3CPH1ZCYQitWf4O4LBmWeeyZAhQ2D3bjweD++//z5FPoJ0YsTFxTFq1Ci+9a1vceONN+KaNYvNWVn89/e/5/XXX2fDhg04nU6ut23+1Ls35513HldddRVD3n6bo7tDlfZZuqK/Ay97/5+D6QL/YgvrRrvdFB0+TLTbzZO//nW7kwlHwrEmUkVHR5N37BgVBw6QGe7GCGASE86YMYPLLrusyfv1gRolquz6duzYwYoVK3C5XIwebfr+bd++nW3btuFYu5YbqqsZNnw4x2trSU9P5+yzz+b5559vMkWshM50YLuP9+/C9D4a18b2F2MCLGmcON+cOXUqq7wzaZ5/1lmsXbv2pO1uAH6Omd5bPWlMD6OWAnr13/yjLbz2l3XNNWBZZHi3HUD7enmtWLGCG1esaNg37QgY5iKdq7i4mIULF/LjH/+YgdXVRGEC75Ew5KlexARqoOlOYjd6r60/l8e7vo3vHS2Qsrqz+r9Rd9D4osf2eNi0aRPHjytlpi8pKSmcffbZzJs3jyuvvJL333+faUVFrPz8cx7buLGhi3ZtbS01QE1NDVs2beKSSy5h2LZtCtTIiWNos3+b84DpPu5wdKvjTaQ6cOAABVu3KlATIcaMGcO8efOanJ9ycnJYuXIljz/+uHrThNhYYFs7tvc1pKUte/fu5amnnuL5559vyF9SXV1NdXU1p1VUcANw/PhxEtPSGD9+fJOeN/v27cOVk4MGV4ROW+eZts5Bjc9p9f/3QEPv+5bK97VdT5YFXNnCssuB2zETigD8EtOb5tYA6/jjH//IOXPmYOXlYc2ZwwpMGoZgDMDka5oBAd+Upz7zDGzZYobLSYe5C7ippYW2TeLUqeBwcHlxsfkcA5SK+R5meTwMOv98XIcPw1VX4XQ66T9gQMNxO9wiKlATCk6nk4SEBMaPH8/IkSNJT09nUnY2MQsX8mtaPqBOXbYM8vOxMFFyfzs7zcZ0Pf11K+uMQUnAQsnpdOJ0Oht2IhvIO3zYdD2WkwwfPpzZs2czbdo09uzZw9NPP83QvDzKy8spaJa4zwbKy8t58skniYqK4hv+ZMH//e8hJ4d0Wt8PwCS0jvGuV78v9sd0m30E/xIdBtpdVlrmdDpJTk5mypQpTJ06leTkZNLy8rAef5yfP/ggVTEx5OTksH79ej799FOsiopwN7lHczgcRLtc4D3WZWdnszsri7lhbpcY0dHRJCUlNbyura1l586d/Otf/yI7O1s9akKsEJMgtz0GY2be8VdNTQ1HjhzxOUSiGPMQadTIkcydP5+pU6cyfvx4AA4fPsxrr71Gnw8+0P4q3U4lZvY7X3IxAZX65cWYa76W1m9J+YABMHo0eHN/DQf4yldgSrO5t95+Gz780K8yg7qePHoU1q7Fuusufl5R0XC/WIK5f5T2+xWml1tLLMviweuuIzYmhp0bNvDKK68EXMcc4AzMOeS05GRmL1hAxty5OBwO+vbtGzH5irpVoCbK5SIzI4Px48dz3nnnMWXKFDIyMmD1ajYsXMi0VrYdcOiQSSiG+eD8jdJmYJLqtpauLBf4ws/ypHWWZZGYmEhiYmKTmWXKysuVp8EHp9PJtGnTmDJlCklJSSxcuJDFixdzVytBrYqKCv75z3/idDqZU1lJEuYG0eNp+he2gTXA6VlZJBUXE0vr+wGYCLbLu159oCYJM3xxGv4lOtyEyUaveYXaJz4ujoF9+zJx4kQWLFjA/Pnz6dOnD2zbBo8/zm233QbJyezYsYOMjAxycnJwHjyonEVhFB0dTZ/UVPAmpM3Pz+fAgQNhbpW0ZP/+/Xz88cd8+OGHepDQAfKAx9tZxgwCC9S0xsKcK+fOnctXv/pVRowYQVxcHOXl5axcuZI333yTiRs3KlAjEioXXADNJ6PJy4ONG2HChJa327oVoqJgxIjA68zJgfx8WLuWM+rqqAXSMbl4FKgJjefbWG4B933nO8SmpLDlued4PIhAjRM4BXMOuXjAAAZdfjmpp5+O2+EgKTExYoardqtATe/evbn44ou55557GDBgAHV1dezdu5c3Dx/m7ja2vfvaa3n0+uuxx47lAk4kamvLr4CzgJntarn4y+FwMGzYMIYOHdqQrV18syyLpKQkbrjhBvr06cPq1av51a9+RVVVVetdhW2biooKoqKi8Hg8REVFkZKYyNGjTUcU1wBfAtb86U/MWLGCfffe2+Z+cDEmv8lMms76tAw4h8ASHQY7o1AkcmOSH9azMAfnaN+rnyTatsE7JaHb42lzOwsYPmIEX7ngAr73ve+Rnp6OZVlm6KjHY6YO9XiwbJuRI0dy4403snLlSlxKLh1WvXr1YurUqfD663g8HqqqqjTkM4LUJ5Wtnx702Wef5c033+TYsWMtbuOmjZlrGmkITldV4bZtnJggt7/HiXo1BD77j7TO6XTisiycDgd33303SX364HA4KCsrY+vWrdxzzz0cPnyY4QrYiQR13HJUV5vrnMYJXmtrG659GtTVwdix8N57LRd2+eXU9e5NzV/+EmArwPX44zgWLqRy6VLOz8igpKSEGzCzZUrX43Q62b59O3v37mXo0KH069fP5wxw4dKtAjX33XcfzksuYcCAAXg8HpYuXcrSpUtZ3cJUstL1WJZFcnJyxHRJi2RRUVFceeWVZGZmsn79et58882GGwh/zJkzh8xt25gxahTLn3iCqVOnmiBPBCXZ6i5eoemNkxv4I97pz/1g7dgByaYT7ys1NW3ehEUBfefN40vf/S7p6ekN7+fl5XFk2zbGAWvWrCFzzBj69OmD2+1m5syZuFesiIgs+D1Venq6yYHyxhts/PRT1q9fz4Ev1F8zUixevJj//Oc/TWZ7at4TsbmP8X86eifAoUNYycnsqK7GiQny+Ptgqd5lwH8C3EZalpKSwvTp07k4LQ1efZXeffqwct06Vq1axYoVK1i+fLlmpRRp5EICP25FfeUrJndQ42vQW26B225rumJtrVknuZWBTTU1LLQsvhlET4w76ur4H4+HSb16qadkF5ecnMy1l1zC1KlTmTlzphmFE2G6fKDG5XKR3qcPjsOH6devH3WpqRw5coTXX3+dP//5zxw6dKjJtN/StblcLubMmUNcXFy4m9IlOBwOLMti+/btfPTRR35tExUVxZSJE/m///s/JvzmN1QcPcqBAweoq6tTkCbEbOA8OCm55FvAn4B3/Cxn6ODB/P3vfwfg/+68k9Vr1vhcz+FwMGTIEO695x4GzZxJWloatm2zc+dOFi1axIYNG7CzsvinbXPXXXeRMXo0F1xwAZdffjlpaWkRk1ytJ4qLi+OUQ4f40iOPgG0z+mc/46dFRVR5PEQBH+JfUss+CxaA283Zx47h+1vS1F7g2na1vOfweDxtBmaacwNPAy/5se584P7evbHfeosrFizglsJCEqHNHsP1HMB/vf9K6PzgBz9g9uzZjDp6FF59lfy8PF5++WVWr15Ndna2gjTS7Y2GFs8nzZN2PwT8OZhKvOkp+mCukQB+Xlt70nXSd4DxwLfaCKIU2HZQgZY6zLlWQZquyel0cs455zAhP5/e5eX86Ec/IikpieTk5JPucWJjY4mzrLD2XO7ygZq4uDhmz56N69//xu12s33/ft555x3effddtmzZQpWe/na42zGJxMDkGwGTzd1XeGzq8uXg8eCsq+OOujq/ksc2FlVby9yNG4n/wx8gNhb27w+y1T1L//79mTx5Munp6ezcuRNnWRnRLhepCQn079+f+Ph4BgwYwNm5ucRt2cL111/PxIkTSU5JoeKzzxj48svcUVfns6dG5sKFsHcvabR9wzAG0931rkbvDcYEKu7Av2TCjcvqDtb7eK8a2A2cPCmob8WxsTDD5L3f3quXz+3cbjd9+vThvOuuI37uXNwZGVRXV7Nr1y5eeeUVVqxYwZ49e+hbWAiYaWjLXS727t3LwYMHiY+Px3I4uBAzHtsXZ10dzscfB6eTaR9+6PcNZFtmt71Ktzd48GBGuVykff45AOvq6tjj8RCLmYK2rfxQDTZuBEy+KH9mSjiFE/v1MKAfgc/WIa3bj3/7+mgAtxumT2ej200+Jp+ev8cJi+BnSYlE/fA/SNWSwSFoR0ZGBoMGDSLd5cK2bY4ePUpBQQGFhYV6UBikm2++Gdu2qauro7S0lOzs7CbL42trYdkyzjrzTCYPGULv3r2JjjaDaXbv3s3ILVuI3r+fCaNG8cUXX1BVVRVwELUtCZjvXyK0el70xcJcC12Bmb3MH6H4rnaUUmBFK8sb7wW7vT/BajzLoa/rpMuBoT7el7a195g6NVQN6SCWZXHaaadx2WWXMWndOtzr1zNs2LATQ//ref8/Y8YMDiUksGzZMkpLS8PS5i4fqElISOD888/H9eablJSU8NFHH/Hkk0+edFCX0KsGtgNfafSeC3MCugrfF4R9N2yAvXtx2Db/4/EEPK2hw+Nh1PbtsGMHHo+HiooK9jqd2MqbcRLbtikrK6OsrIwJEyYQExPD7t27Wbp0KfHr19MvJYUZ48dz2mmn0bt3byZPnsywlSuJ27OHG264gbicHKzSUhKys5nw0ku0mJbtqacAc5HyqJ9t87XeL5q3HzMrQGuXVtv9rK+nczqd9O7dm+nTp/Otb32L1NRUampqyMnJ4e233+bpp5+muLiYuro6Uhtt5/F4OHbsGHv27KF3797UnXIKM3buZFwLJyzL48Hxt7+BZTHuyBH6hvB36OmTxI8bN47TYmPBG6j5x4ABvAekHDvG5ADKGTFiBFEuF6VlZW2eJ9OAFOB67+v6KU3jMNMi97R+Ag6Hg1NPPRX3F1+QnpLCmLQ03LaNtWMHQwcPNgHTALl3724oqy2ZxYEOFuje0jjx3QxW4J/YyQ4dOkR+fj59jx8nFqiprWXgwIGkp6dTWFio2b4CZFkW9957L4mJidTU1HD48GHWr2/6SMN9/DjW8uVcdvnlZJxzDsOGDSPROxvQhx9+SO9nnyXx6FEWLFjA4sWL2b17N6WlpSHtFRyH+f7FYnJVjgtwezfwZWh1opPGQvFd7SgHgXvD3Qhpt1AcUz8NRUM6iMPhYPbs2VxwwQUMLi6G9b4elZ4wd+5c3EOHkp+fz9q14Qn9delAjcPhIDY2lgEDB+JwOHjnnXd4q7xcQZogBZo6aQ8nn5j6AwcwSWYP+djmV3fcwZ133klNVRWTU1Ko8nYd9PfUmZqSQu5nn+FyuSgrLWXNmjXccMMN1OTnB9j67q+2tpaVK1eyZs0aZs6cyWWXXQbAnXfeiePssxkzcyZX/fKXTbaxdu7Esizi4+LgS1/Cys0FzOfT0mdkWRZW/Tp+XATVr9vaew7v6+kEPo5ZTpaamsqXv/xlfvnLX5qZnYBPP/2UxYsX8/vf/56ioiKf2+3atYuUlBQmT57MpZdeyv5//pM777yT119/3ef67qgoSjZtIjo6mr8++ij33qtLt1CZPXs256WkwIsvArD+44/JrqtjH/4/kQXIWbaM/v37s+ytt7jkkktaXfduzEVbffmLgALgxgDb3l0kJSWxZcsWGDOGO2+8kTvvusvkbEpKMkMPZ/jTR6mZxmW15bnn4P77wbaxbBsLGn784fCuW/+vvyJ1wGsWZpbO9phBy0M2/PWPf/yD48ePU52aytmWxYQJE/jZyJE8/fTTPPbYY+Tr+iRg9ZNFuFwuhg4dytChQ5uuUFwM//u/zLvgAuzRJstTfY+ZmTNn4li3DuuTT/jpT3/K3LlzefDBB/noo4+oqKgIWRvzMcfGHOA+4IUAtrWAo5jeiYv93KZ+4gWRjhKKY2owAjmPgfccVj/5hfdc6Nd2lsWQQYOIjopqmuuoBf369eO8886jT58+nHvuuWFJ/9ClAzXnnXceN998M2efdRZOp5N/v/Ya7yqHRtC2Ybq9tUf9BeA2fPeEiPnJT+Dhh3HbNnnV1djA74AH/Ci7f//+zJs3ryGR8MaNG1m0aBEFBQXtbHX3ZNs22dnZ3HLLLSQkJJCZmcn06WaAxN1797KrspLXvN2yN2zYwMGDB7msqIg/pKaeVFYJptutr71r6ZIlTD3jDHbs2MG0aa0/G5qHGZvcuKxpmBvAAZheWrcDDwb+60orZs6cyYUXXtiQOPiZZ55h0aJFrFu3rsUgDcDAgQOZM2cOX/va1zqppeLLoEGDSEtLI9p14pRdXV0d8NBR6QZyc7F69WJbaSnRmPNty3NKnSwJeBn/h0CtA84PsIk9zc6dO3nqqaf43OlkMVBeXk5cYiJXXnklaWlp3HTTTeFuYpdi2zY//OEPG4brZmZmEhMTw7hx4ygoKKCuro7ByckMBbJzcti0axfbt2+nuFGPs1lr1zZ8b2fMmMEjjzzCyy+/zBNPPBGW30lEWvYU8NtANrBtkgYPBsvia9XVXObvdrW1xNxzD1H33YdVUwOnnNLmJvHx8YwaNYoLL7yQ5cuXU15eHkhL263LBmpiYmIYOnQop512Gi7vxWt1VVVA0/tKUwnAX4H321FGGuapwq2YpwXN3XjttVxx+eXU1tTwP1ddxSO1tX5P0RcfH98wltCyLI4dO8bhw4eDj3CWlcE11/Cn7GzKgiuhVcM7oMxA1U+1XVVVRXl5OYXe/CP/e+wYu8rLedsb5CoqKqKyspIKb8LgysrKJt1sbUywxtdfui4+HpKT8SQk+MxL1NhxH2XV/+1LMIEaZZUKrTFjxnDeeedx9tlnY9s2O3bs4PXXX2fTpk2tBmkAzjzzTKZNm9aQvPvFF19kx44dndBqqedyubjmmmsYPXo0rn37Gt5v7ag3EDNrmC9p110HMTFMy8vj7TbqHobpJVm/3hTMPtradt/GPGHuTp544gleffXVhtd/ys5mybPP8try5UR5PLxaU8O9d9zB9l69Ai67cVltmZuTw//YNtd7A+w/wOTH8DewbWFmmHsMM9tUW24EIm8OjMjj8XgoLy+n1LLAtom95hocUVH0q6ri/KNHm+wvAzDXSc33oSi8T5S//W1ISGDMsWMnrZPAic+wPtAWE/pfJyIsXbqUiooK3G43MTExOJ1OEhMTqa6uxrZt+kRF8ZZt8+Mf/5gsbx6b2toT4cekoiLOTkzkow8+YPbs2YwYMYLJkyczduxYsrKywvibiUhj1xH4ccyyLF555hni4+P58L33+O1vf+v/xpWVjBs3jgkTJjDxrLMYY1k4HA5s26akpIR9+/axcOFCflxSQpJl4XK5SEtL44EHHiA3N5ft27d3anLhLhuoGT58OCNHjiQtLY1Dhw6RadsR2z23K9mM/zPN+FKf5OsDfA99mjN6NMyfj6eqincdDr/HtLrdbvr27cuECSZTim3blJeXt3mj2araWnjnHQptm3aU0qIaImdMscfj4fjx4w0Hl2qgtLqag2UnQlRpaWn08nioq6vjwIEDDK+r67oHCGkwdOhQhg8fTnp6OlVVVbz77rtkZWW1mTth4MCBnHrqqQwaNKjhJLZ27VoOHz7cia3v2SzLIjExkXPOOYf+/fvjOHDAr+0SMMktF8JJDy/q+vaFuDiqqqtp65Psi7khrF+vCpM43td20ZjZoRL8amHXsnnzZjZv3tzwugzYvn0772zfjhvTe3TNRx8FlbyycVlt6Yv5DOrP0VdhkkL7e862MOel9X5uMwcFavxl2zYFDgdv9erFl1NTiY2NxVNZSWVdXZP9JQmT56n5PtTwwKp3b0hJodqyTlqnPgyYT9P9+lnMQ5DuJDc3t0kPmebqJ15ev369z1x1xzAJbt966y2mTZtGQkICw4cP56yzzlKgRiSCfBDENhZQc955kJJCTn5+wPetu6qr+aKigl0FBZz+n/80XOMeO3aMXbt28eabb3JXZSVJmOswt9vN5MmTmTp1KkVFRezZsyeIVgenzfuwvpgnah0lBqC8HEdubpN6WkurFxUVxezZs5k4cSK2bfPxxx9zsZK1dWvx8fEMHjyYGd4cAB6Ph5KSEo4e9dVvJzD30zFJaddjpgjsClwuFxMmTGBURQXV27fz0Zo1DKquVqCmG8jIyCAhwdw+l5eX88ILLzR0H2/NzJkzGTVqFKmpqQ29rA4cOEBZWUf0PxNfXC4XQ4YMYfLkyaSmpuLxeE6ayr0lNvBdTs7xdMFjjxHfvz+b3nqLm997r9Uy6nPU3Ox9XZ+j5mYf6yYDGiAnPZVlWeQlJvKHyZM5/fHHierTh6P5+Xz04Yfc/OGHDevdgJlJrfk+VL//WA89BGPGsHvNGm5e3DR7yRjgUnzv13Ky0pIS/v3vf3PnnXcSHR3N4MGDOf/88/nrX/+qBM8iPdjOnTvZt28fK1eubDLrU/3Ig8OHD1OLCcDX/wDMmTOHPXv2RE6gxgbe6OAGWACvvkrGq6/SPAWwrxORZVmceuqpXH/99YwfP54DBw7w2GOPcZ6m4e7WYmJiSEpKakgwV1FRwY4dO/jss8/C27BuYuTIkdxzzz3M2buXo7fdxk9+8hPmlZQQF+6GSbvVn2Rqa2spKSlhy5YtbQ4XtCyLu+++m97DhhEXF0dFRQVLliwhLy+P6moNMO0s8fHx3HbbbcTFxVFeXo5dVtbwJFlEIkdcXBxTpkzh2WefJSPD9EM6fvx4cA+TlGsxJGrr6jh8+DDLly9n7ty5pKamMmbMGAYMGEBOTo6CNSI9WHV1NYWFhQ0pIVpj2zaWZTFo0CCSkzv3KqzVQM1mTPfpjvQ8cO7VV5N7991MmTKlyTIPJ4/DdzgczJo1i7S0NPbv38+SJUvYuHGjDrjd3OjRoxk37sQcUwsXLmTjxo1hbFH3YVkW1157LcOGDSPq4EE8Hg+HcnMbZlCQnsHhcDBlyhSuOPVUeOEFM9TGO91wbW0tO3fubJIDQDpWVFQUaWlpzJw5k+joaFauXEnVmjXMC3fDRKSJM888kylTpnDmmWc2JGzfsWMHL7/8Mi+8cPJcQP04OY9Tw0xcs2dDVBRTqqtPWqc+j01LkzW0xB3Aut2Nw+EgKioKy7Io984Km5eXp3sGEWnTSy+9RFFZGV//+tdJSEhg0KBBTJgwgc8//5ztfgxXDoVWAzU1+M4zEkpVAHFxePr1a7Muh8NBfHw88+bNo1evXrz//vv8+9//prqmpiGgk5ycTP/+JwZRlZSUUFxcTGlpaQf9BtLRMjIy+NKXvsTMmTOxbZu6ujo++eQTDh48GO6mdRujRo3C7XY39JbweDzK+dRNWJaZuLA+GeNll13GsWPHKCoqoqysDMs7nWx9MHSCywUvvGBmV7MsamtrKSoqYsmSJVRWVob5t+k54uLiyMzMJDMzE6fTyc6dOynLylKgJkL9GJM7JFAZwFfwb4r14UAK8Jz39dnA1iDqFP9Y3kSSTqeT6OhoTj31VDZs2EBtbS1ut5v4+HgGDBjAjTfeyNixY01w2+Fg//79LFq0iJUrV56U02sl4GsOqFjgD4D1gx9ARgb7d+3i4YcfbrJOJvAQJnl0oJNM1+H/TF/dhcPhICEhgQkTJpCcnMzRo0fJz89Xr1AR8cvmzZv5FPPg7KabbmLJkiWsW7euU3M1dqkUFA6Hg7i4OCZOnEhcXBy5ubl8/vnnDcvHjxtH/KmnMmLEiIb3CgsLOXjwIF988QV79+6lpqYmLPOgS/D69+/P2LFjGTFiBLZtc+jQIXbs2MGRI0fC3bQubThwMeYJ3Yjt24krLoZPPyXG+37jLOwu73u+9pxeq1ZBQQEJ2dlc3EadU0PR8B5iEr5nTvNlYFkZvPUWANPy8nA0WjbxwAF6rVqFMzeXhMpKro6N5WhFBUeioihzubAcDibHxjIuMZGM6mqSmk13X1lZSW5uLp999hk1NTWh+NXED3FxcWRkZBAfHw9ATk4Oxfv3h7lV0pwHWIw5RqYGsb0TiPNz23hMz4v6dbfi3+xN0gaPBxYvBpeL4du2NZzHHIDLsnBaFtGWxUS3m0zLotaycFsWiQ4Hg6KimFtRQb+cHNx5eXg++YS6PXtw//e/jNi+nSQfx0xfx/X4+v9ceimMGUPBmjW80CxQMwYTqHkZ5ajxh2VZREdHM2DAAGJjYyktLWXfvn3qLSytcgEX4O3h5sMYTNL8xte7k6Dh2rm9soFNIShH2q+goIBNmzbhdDpJT0/nrbfe8mvG1FDqUoEay7KIiooiNTWVqKgowBu8iY3Fqqjgluuuw3nhhWRkZjaM8S0tLWXPnj289957PP300xw9epRaH10eYyorwTvlZVIAbfJ3amkJ3qBBg0hLSyM6Oprq6mqWLVvG7t27e1QvKSc0yU3hACzv99V9/HjAeSucwBXeH2wb7r+/YVkM8Faz9eMx0677kvDgg+B00t/j4UU/6tYFZttKMEknb/BzfcfBg3DddQD8oLy86ZPTpUtxfvABNVFRxEdHc7WP7e3sbOpee4262loq6urwuFzEYS5UioqKFBjtZJZlkZyczNChQ7Ftm5qaGvLy8ijJD6bPhnSkWrzH0SBtxfSQecyPdW/A3Khf0o76xIfaWrjxRgDmV1XxZe/bLoeDGLfb9C60beyNG/lhbKO5HGtqYNcu7B/9iJraWo7X1lJTU0Mf4BtBNKMEM926vwnDJTAFBQXKayhtigfexMzI5yukF40ZTtj4eteNuXb25xq4rboXAje2sxwJjVig9sgR1i1ZwrolSxreb3zP5Swrg6IiHCUlQecQbO2Y36UCNc3Vz4jx7W9+k9jbbyfp7rvhnnuarJOEiXROsm3uaKUs68kn4fe/xwL2BdAGC/gowHZLYC699FKGDh0KmOR8t912GxUVgXb87dom0ewpnG1jDRkCwNdsm68GWF5LTwpaUoyZic1Xj5rV//0vM6ZPZ8fWrYwb7988V+rT1jp/hkE0duqoUWzZsgWAry5YwOJms4VMnjiRSy+5hB/88IfExMQ0+fxt2yb38GE+/fRTPtmwgc8++4yjx46xyOkkAdMrsfHUxNLxkpOTmTt3Lj/96U8BeO+999i1axeJPey4J9Ip3G44fBjcbp789a+59957AZh91lk8/ItfMH3aNOrq6sjNzSUjI8MEbhrJOXiQ999/n7fffpvXX3+9XU3Jwjyxl9CLiYmhV69eba8oAkzH94ywd2FmQxzX6L0bgJ8DA9tZZ/OHpBJeT2GGpLbGOv98AEbhfy/4k8rABOp96dKBmgsuuIDx48cz9tRTKZ84kTdfe40PPviAj9evb1gno18/5s6dyx133MHLL7/MM888wxEfWfhvvukm7rzsMuyLLmIaEEhfDV06d6xDhw5x5MgRioqKeOONN6isrOxxw9eygCsbvXZYFuvWrSMxMZHXX3uNH913X8BlXjhvXsPsaW+88QZ/+9vfGLtzJ9/1eJgNrAB+gwnS/BITXPH5V7cscDjA4VAAJkQC/Tt6wHwG+P6ctmRlkXv4MO998AFnnnkmffv2xeVyUVdXR3l5OW+88Qb5+fmUlZWRmJjIt771LVwuc3ooLi5m165d7fyNJBAjR45k+PDhREebPpvLli3j4MGDjG60zmKgpUwLbsyJ/2NOfiLYd/ZscLn4UlkZ29poRyrmyVH9egOAVwP4PUS6DO95rPHxMzcvj3f/+19q6+qYPn066RkZHMzNZdu2beTn51NTU8Py5cvZvXs3ubm5HDlyROfACJaZmcmsWbN4+umnw90U6QJavOZttLz5/7X/dx+z8S9I8o+//51Jkyaxe/duLrrooqDq+hctB/m6VKCmvgt4UVERqamppKenk5KSQlJSEi9t387r27axMTe3IVN+r169OHPePMZdcgm1w4ez/PBhttTW+oxaFaSlgbfXxhdoeEYkWbRoUcOUwlu3bu2R44srgR2NXltA3YgRkJJCcUZGk2X+mtanD9aYMRT27s1rW7eyrriYXrZNrbeuGuAwwUeIJXJUVVWRl5dHaWkphYWFJCQk4HA4sG2b6upqtm3b1hAAjY2NZfjw4Q1PjUtKSti3b194f4EeJjo6GrfbjW3bVFVVsXnz5oahZxWYxLWt6QvciemG3Tz984+/9jWSk5LYv3Urzz333MkbNzIHOAN4ptF7W/z/NUS6tLy8PBYvXkxWVhavv/46DoeDoqKihqC2x+Nh9+7dFBUVUVlZqZmEIlxUVBQJCQnhboaIhNAZZ5zBr3/967DV33fWLMjMpNrjCepeDE6+TmusSwVqPB4PlZWV5OTkkJCQQFxcXEPeknfeeYe1a9eSn5+Pw+EgNTWVKVOmcNFFF3HmmWdSXl7Oli1bqKqqCvevIQFavXo1q1evDnczup1jx46RlZVFVlYWy5cvp7CwsMf1VOpJ6urqKC0tJSsrq8V1LMvC7XbTr18/LMuipqamIbiTnJxMRkYGcXFxDTlT8pUzpUMUFxeze/duVq1aRUVFBTt37mzIyVWF6enW2p46BhOo+X+c/NDhB9/6Fsn9+7P7rbd4vI1AjRM4BXg8yN9DpCsrKipi48aNbNy4MdxNkSBYlkVs45xCLXACZ3HimDqutBRWrQJgQmmpz/wRIzA9F8/C5CwZ4f2/322ji92AiUSocePGMW7cuLZX7KK61HHC4/FQVlbGxo0bG7K419XVNSS7LC8vb5iCdt68edx5550MGjQIj8fDnj17KCgo6NFPPFy0nvzY8mOdtii5ctexaNEiFi1aFO5mSISJioqib9++WJZFcXExZWVluN1uJkyYwC233MLo0aM5fPgwTz/9NAsXLgx3c7ulzZs3s3nzZh5/XCESEZFgREVFMWjQIBz1w4Jt+6R7AA8QBSxt9J4jKwvmzgXg99XVPhPKOr3brfK+vs/7Eyg3/l83u4MoX6S7qqqqorKytb4onaujOoJ0qUANmKfCTz31FBMnTqRXr144HA6ioqJYsGABq1atIj09nSuuuIKLLroIp9PJsmXL+Pe//82zzz7b46eW/T7wnVaWu4E/0nbipNYEmqRWRCJHfY+aYcOGAZCamsqll17KlClTGD9+PE6nE8uyOHDgAGvWrAlza0VERHyLj49n9uzZDfnWdu/ezWuvvdZknXfgpJlapk+bxofLlwMwd/Zs1q1bd1LZ1wGhyHTzCr5nFvLFEcC6It2ZbdsMGjQo3M1ooqNGJHS5QI1t2+zcuZOf//znzJ8/n/POO4+BAwdy0003cdVVV+F2u0lLS8PlcrFlyxZeeeUV/vOf//T4IM03gbZG5r4F/Alz4mqvghCUIf6ZP39+yG6a+yxeTJ+HH2YN0Ae4HzP9bALQ0uCzcd/8JiQkMKSigmBakRhkWyX0PB4PBQUF/Pa3v+XWW28lOjqa1NRU4uLiKCws5D//+Q+bNm3is88+49ChQ+FurkjYfAe4PATlDA6grD4hqE9a9/Wvf51Zs2aFrf7BgweHre7uyLLM48P6HvnNz1s2Jydlr7Es8CZyr7Esn0nbazHXuQsI7bVza8YDv+/gOkS6iurqlqZT6F66XKAGoKKigo8//pjq6mry8vKYNm0aqamp9OrVqyGQs2/fPtauXcuaNWs4fPhwuJscdv4kgKwGdgNrO7gtAE6nkzvuuIPMF15g2oAB3D1nTifUesKYMd1nAsy+ffvSt2/f0BQWFQU1NcwAeOophs+aBTExsHgxM773PTMzRguOFxay4rPPgqq2zvsj4VdSUsLbb7/N+eefT1xcHNXV1RQUFPDee+/x0UcfsWfPHvLz8yOqy2lXMmfOnIau+IEas2sX0YsXc1cb+2LfwkJ49lluv/12KqObdqxPTFRotL3+SuiCJiOBPZgZutoyHpgYonrFt/79+9O/f/9wN6NVt9N68slQiqHr9pSOjo5m7NixOBwO6urqOHbsGHv37g1Z+dWY6+XOunbuqp+DiAQvMgI1RUW4du4koFvnggIKPvyQlZ9/jmfLFk455RQGDRpEXV0d+/bt46OPPuLzzz/HUVHBKD+6I/UtLITdu4P+FSQwLpeLX/ziF7BsGYNnzWLOo492fiOKijq/zhDasWNH6G+64uLg+usBGPHcc+SfdRZ1ycn0X7qUHddd1zAFtC+7d+/m3mefDW17pNNVVFSwfv163n//feLj4yktLWXv3r08//zzlJWVKeF0O82fP5/58+cHt/Fbb8H77/PoI4+0ui+ydSs89xw/++lPISUluLqkRaHMHLQA8yT+MT/WvQEFajqCx7bZsW0btjtysoD4CihUA9uBr3RyW7Zxcq+TriA2NpapU6ficrkoKSkhLy+P/fv3h7tZIiJ+C3+gxrLgzTfp++abgU/7WV0Nhw/Dv/990qKbAi3r2WfhuedMe3Qj0mFs224yvbZl3sQOx5TbHk+XfUJh2zbTp0/v0DpygPvuu4+jmGl+x40f3+pMM9I92LZNeXk53//+98PdlG7FE4pjnG1jQdvHS++xzbZtaGFdBdxEjJqaGiaddlrEByN2A2PD3YguxOVykZ6ejmVZbNu2jV27doW7SSIiAQl/oOYf/4Dzzyc3Nzesw1Fuv/12HvzZz8xF7ZAhUFIStrZ0Zz/5yU94+OGHG14vLS9nzcaNPPDnP3d6W5Jtm326WRGRDlZcXExaWlq7y5lXU8OfKyoYnJbWatB0VF0da22bwYMHU9LCEKmenrdNRLo3y7KI9g79XL9+PZ8FOTRbRCRcwh+oiYuD5GTssjLCGRqpio6G5GQTqGll7L+0T1VVVZMpzOqAqro6SsKQFEqfsoh0lpIQBP+PY5JflpSUtBqoKfP+W1paGtbzqohIuOXk5LBhwwZ27twZ7qZ0W8OBtzuprphOqkckEoQ/UCMiIiFz7rnnkp6eHpa6XS4XTqczLHWLiEjX0xu4EZiyeTP89a9g23y9upqKVraJxTxsuwLI9bF8SqP/79q1i/3793P06NHQNVqaqAE6c9qWvwLXdGJ9IuGiQI2ISDdy++23h7sJIiIifskEfgYkrFoFn3+OA/h/qamtb2TbcOgQD7W2Sk0N5ORwcN06ovLySK+t9as9faurISen4f++5gDrBTiA/oAeTcB+4OZOrvMCzOfQ/PNJBNw+3vdXsvfffkBpC8tdzcrvFWRdIm1RoEZEpBtQclgREelqNgNnAHffeiuP+jsDaFERpKa2OvmHtWsXziFDuA64LpAGffIJDBwIwButrGYB2YGUKyH3G++PL+35bCzggzbWaV7+wXbUJ9KSiAnU9OvXjxxvBDscQj7NsYhIJ/niiy8YMGBAuJvRRGmpr2dRItLc/cD3/VgvDkjCzMgXDMu7vUiXl5Rker108AOKBQsWsHHjxpPevxq4mxNDrI51aCvEl9PpmN5MSUAWcA4QyDxhdR3QFpGICdQ4nU769w+2o5qISM9VW1vLoUOHwt0M6QRxwHPQajLhFMxN+VPQrimHx7VjW/HP/2E+L3+cjcnJcV8769TcNxKJFi9eTF5eXrib0cS67GwKfLxfBHgAnXXDp6O+KeXef/PR5yvhF/ZAzbp16/B4POFuRgPbtjVtqUgzLuBiWr85DJUxnVCHSFd0GHiXtsfD1/cP7YVJ8hisQ6hrf0d7M4B1LWAe8EIHtUUknLKyssjKygp3M/wWg7ku6iyjO7EuEYkMYQ/UPPRQa6nARCTcaoBaOu/mIBrThbSYzgkMiXQV64FL/FhvDLAF+CpmPxLpjpycSPwZLA16l2BUY75/L3ZyvUWdXJ+IhFfYAzUiEtneBdqYfyGk7gKuxwy7UKBGRER8mQSEYsJl9aGWQP0deDlMdeu6SKTnUKBGmvglcG8n1jcYWNGJ9UlgOvv7ACYoVIguRkREmusLbOukuvoAOzqprmBkAVeGoByb9uVykp5J1ygi0tEUqJEGPwcSwlDvx2Gos7G7gSMdUG5kzcETuHB9H6BjPg+RnsTC7MNVnVRfjLdO6TjrMcmHO1MkTzlbSWQHkkRERNpDgRpp8OdwN6CT1QGrgRHen1CL74AyO1NP+z6IdBcVmGPbaZ1c72pv3dIxsrw/IiIi0v0pUCM9VhkwqwPLXw+M78DyRUR82QfMDHcjRERERCRoVlFRUYvDLJOSkrBtjcIMB8uyqK4Ozahpt9utzzEM9Bl2D/ocuz59ht2DPseuT59h96DPsevTZ9g96HPs+izLoqSkxPey1gI1iYmJ1NQoH76IiIiIiIiISKg4nU7Ky8t9LnN0cltERERERERERKQFCtSIiIiIiIiIiEQIBWpERERERERERCKEAjUiIiIiIiIiIhFCgRoRERERERERkQihQI2IiIiIiIiISIRQoEZEREREREREJEIoUCMiIiIiIiIiEiEUqBERERERERERiRAK1IiIiIiIiIiIRAgFakREREREREREIoQCNSIiIiIiIiIiEUKBGhERERERERGRCKFAjYiIiIiIiIhIhFCgRkREREREREQkQihQIyIiIiIiIiISIRSoERERERERERGJEArUiIiIiIiIiIhECAVqREREREREREQihAI1IiIiIiIiIiIRQoEaEREREREREZEIoUCNiIiIiIiIiEiEUKBGRERERERERCRCKFAjIiIiIiIiIhIhFKgREREREREREYkQCtSIiIiIiIiIiEQIBWpERERERERERCKEAjUiIiIiIiIiIhFCgRoRERERERERkQihQI2IiIiIiIiISIRQoEZEREREREREJEIoUCMiIiIiIiIiEiEUqBERERERERERiRAK1IiIiIiIiIiIRAgFakREREREREREIoQCNSIiIiIiIiIiEUKBGhERERERERGRCKFAjYiIiIiIiIhIhFCgRkREREREREQkQihQIyIiIiIiIiISIRSoERERERERERGJEArUiIiIiIiIiIhECAVqREREREREREQihAI1IiIiIiIiIiIRQoEaEREREREREZEIoUCNiIiIiIiIiEiEUKBGRERERERERCRCKFAjIiIiIiIiIhIhFKgREREREREREYkQCtSIiIiIiIiIiEQIBWpERERERERERCKEAjUiIiIiIiIiIhFCgRoRERERERERkQihQI2IiIiIiIiISIRQoEZEREREREREJEIoUCMiIiIiIiIiEiEUqBERERERERERiRAK1IiIiIiIiIiIRAgFakREREREREREIoQCNSIiIiIiIiIiEcLV1gpOp7Mz2iEiIiIiIiIi0iM4HC33m2k1UFNaWhryxoiIiIiIiIiIiG8a+iQiIiIiIiIiEiEUqBERERERERERiRAK1IiIiIiIiIiIRAgFakREREREREREIoQCNSIiIiIiIiIiEeL/A6V6uNaSEa+GAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 1440x288 with 11 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display_digits_with_boxes(img, label_preds, labels, test, bbox_true, \"Un titre\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMH-Cp1MPt3F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPybzykcPt3F"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "YOLO_MNIST_Localization.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "bfc2e043f4a41bbce066f7921047bdcfe8769546c7198e4df6482fd90de274f1"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05564490bce74bedaf40cbc368909fd4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08133968c3a24449b92b80f7eb61e4d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fc8eb581ed9457aacb56dc01a9dc9e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57f9abfdefd74ab6b4806bc500e7f8c1",
            "placeholder": "​",
            "style": "IPY_MODEL_54ecc4bec948431d8202cafaedffe8f6",
            "value": " 1648877/1648877 [00:00&lt;00:00, 15007992.45it/s]"
          }
        },
        "1082b52862584f47ac371b7c37e45f32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_536017ec05ad45b28cacb6abf83795cc",
              "IPY_MODEL_2debc45cf0ee4ecdbb84ade1ab39615b",
              "IPY_MODEL_1c871c8171144624a9c857c8e4354a7a"
            ],
            "layout": "IPY_MODEL_db27cee5aa524f5c89ee9ef3c50a0e60"
          }
        },
        "15636d229dbf4a54b9f11faf54658dba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "183da87c5a514626bff112af3cbbcffa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05564490bce74bedaf40cbc368909fd4",
            "placeholder": "​",
            "style": "IPY_MODEL_889032b33cd548cabbce84d692a18a9a",
            "value": " 28881/28881 [00:00&lt;00:00, 731005.75it/s]"
          }
        },
        "1c871c8171144624a9c857c8e4354a7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dfc26754d20455ba0da75e33218a36e",
            "placeholder": "​",
            "style": "IPY_MODEL_393d0d522b4743e9a04a4e3d708f529f",
            "value": " 9912422/9912422 [00:00&lt;00:00, 9161291.15it/s]"
          }
        },
        "26b940b2d66a4cb59306b2a3abb8b6b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85c6e81be5de4a2a92f92fa28606274e",
            "placeholder": "​",
            "style": "IPY_MODEL_8b77517829854981acb8f1bf7eb9307c",
            "value": " 4542/4542 [00:00&lt;00:00, 141960.48it/s]"
          }
        },
        "2debc45cf0ee4ecdbb84ade1ab39615b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_628dc217877c4f539974877fa3d87696",
            "max": 9912422,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8635c9299fef4220aab7ee1765e2709c",
            "value": 9912422
          }
        },
        "2dfc26754d20455ba0da75e33218a36e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f398dda6e744f01904c796703da58cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3664bfa18d95441bbf8a0875cc2e8f56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15636d229dbf4a54b9f11faf54658dba",
            "placeholder": "​",
            "style": "IPY_MODEL_498e99190ffc4810b5150c07da2e1b3d",
            "value": "100%"
          }
        },
        "393d0d522b4743e9a04a4e3d708f529f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4104399bac454257be472b40203ef926": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3664bfa18d95441bbf8a0875cc2e8f56",
              "IPY_MODEL_b7243342e8e9473e8695e687276e9eac",
              "IPY_MODEL_183da87c5a514626bff112af3cbbcffa"
            ],
            "layout": "IPY_MODEL_d5481277c0294251a3b141983ec2950c"
          }
        },
        "498e99190ffc4810b5150c07da2e1b3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "536017ec05ad45b28cacb6abf83795cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f398dda6e744f01904c796703da58cd",
            "placeholder": "​",
            "style": "IPY_MODEL_dee265ee475f44329263cd7b22ee1637",
            "value": "100%"
          }
        },
        "54ecc4bec948431d8202cafaedffe8f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57f9abfdefd74ab6b4806bc500e7f8c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b0a6f743129427d8be2e358ac90c848": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60149f89d4684950b8de058c6479e2ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "628dc217877c4f539974877fa3d87696": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65d9876405d94ac09981657509957cc0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "699a5bbfd9ed494ca22f5067871ed5b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b5cf599b6f14f59abbd59cdfeb2757c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ce32bcdc7714b84ac70f7a48835cc49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b183fa1ae8ec4e9ebb00943f16fd4752",
            "max": 4542,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_adaba7d4206e4a68a02229712d9de299",
            "value": 4542
          }
        },
        "858bd330763a4c71b0c9287f2e5ad46e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f78eed65f4724831a6dfcf5272b25993",
              "IPY_MODEL_6ce32bcdc7714b84ac70f7a48835cc49",
              "IPY_MODEL_26b940b2d66a4cb59306b2a3abb8b6b5"
            ],
            "layout": "IPY_MODEL_cd8c25d30c8e46239415fb04da6f9bb5"
          }
        },
        "85c6e81be5de4a2a92f92fa28606274e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8635c9299fef4220aab7ee1765e2709c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "889032b33cd548cabbce84d692a18a9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88f47dba11ff48009ad3440061de6ee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b77517829854981acb8f1bf7eb9307c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa921fc6bdef41049de138c4678ef731": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adaba7d4206e4a68a02229712d9de299": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b183fa1ae8ec4e9ebb00943f16fd4752": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7243342e8e9473e8695e687276e9eac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08133968c3a24449b92b80f7eb61e4d9",
            "max": 28881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88f47dba11ff48009ad3440061de6ee2",
            "value": 28881
          }
        },
        "cd8c25d30c8e46239415fb04da6f9bb5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1b608ab95554c9aaf779b61ca7c5cbf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5481277c0294251a3b141983ec2950c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db27cee5aa524f5c89ee9ef3c50a0e60": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "deb656adb8114376b067f10b37f6232b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65d9876405d94ac09981657509957cc0",
            "placeholder": "​",
            "style": "IPY_MODEL_6b5cf599b6f14f59abbd59cdfeb2757c",
            "value": "100%"
          }
        },
        "dee265ee475f44329263cd7b22ee1637": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec5a25f7484a4bb0b6c404a183f78343": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_deb656adb8114376b067f10b37f6232b",
              "IPY_MODEL_ed55481101c446ac900f073ed6272749",
              "IPY_MODEL_0fc8eb581ed9457aacb56dc01a9dc9e6"
            ],
            "layout": "IPY_MODEL_aa921fc6bdef41049de138c4678ef731"
          }
        },
        "ed55481101c446ac900f073ed6272749": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b0a6f743129427d8be2e358ac90c848",
            "max": 1648877,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60149f89d4684950b8de058c6479e2ec",
            "value": 1648877
          }
        },
        "f78eed65f4724831a6dfcf5272b25993": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1b608ab95554c9aaf779b61ca7c5cbf",
            "placeholder": "​",
            "style": "IPY_MODEL_699a5bbfd9ed494ca22f5067871ed5b1",
            "value": "100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
