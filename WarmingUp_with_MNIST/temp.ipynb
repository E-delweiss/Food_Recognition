{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ageHLREovu4N"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "class VOCDataset(Dataset):\n",
        "\n",
        "    def __init__(self, is_train, image_dir, label_txt, image_size=448, grid_size=7, num_bboxes=2, num_classes=20):\n",
        "        self.is_train = is_train\n",
        "        self.image_size = image_size\n",
        "\n",
        "        self.S = grid_size\n",
        "        self.B = num_bboxes\n",
        "        self.C = num_classes\n",
        "\n",
        "        # if isinstance(label_txt, list) or isinstance(label_txt, tuple):\n",
        "        #     # cat multiple list files together.\n",
        "        #     # This is useful for VOC2007/VOC2012 combination.\n",
        "        #     tmp_file = '/content/label.txt'\n",
        "        #     os.system('cat %s > %s' % (' '.join(label_txt), tmp_file))\n",
        "        #     label_txt = tmp_file\n",
        "            \n",
        "\n",
        "        self.paths, self.boxes, self.labels = [], [], []\n",
        "\n",
        "        with open('/content/VOC.txt') as f:\n",
        "            lines = f.readlines()\n",
        "            \n",
        "        for line in lines:\n",
        "            splitted = line.strip().split()\n",
        "\n",
        "            fname = splitted[0]\n",
        "            path = os.path.join(image_dir, fname)\n",
        "\n",
        "            self.paths.append(path)\n",
        "\n",
        "            num_boxes = (len(splitted) - 1) // 5\n",
        "            box, label = [], []\n",
        "            for i in range(num_boxes):\n",
        "                x1 = float(splitted[5*i + 1])\n",
        "                y1 = float(splitted[5*i + 2])\n",
        "                x2 = float(splitted[5*i + 3])\n",
        "                y2 = float(splitted[5*i + 4])\n",
        "                c  =   int(splitted[5*i + 5])\n",
        "                box.append([x1, y1, x2, y2])\n",
        "                label.append(c)\n",
        "            self.boxes.append(torch.Tensor(box))\n",
        "            self.labels.append(torch.LongTensor(label))\n",
        "\n",
        "        self.num_samples = len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.paths[idx]\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        boxes = self.boxes[idx].clone() # [n, 4]\n",
        "        labels = self.labels[idx].clone() # [n,]\n",
        "        \n",
        "        h, w = img.size\n",
        "        boxes /= torch.Tensor([[w, h, w, h]]).expand_as(boxes) # normalize (x1, y1, x2, y2) w.r.t. image width/height.\n",
        "        \n",
        "        target = self.encode(boxes, labels) # [S, S, 5 x B + C]\n",
        "\n",
        "        img = transforms.Resize((448,448))(img)\n",
        "        img = transforms.ToTensor()(img) #(img - self.mean) / 255.0 # normalize from -1.0 to 1.0.\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def encode(self, boxes, labels):\n",
        "        \"\"\" Encode box coordinates and class labels as one target tensor.\n",
        "        Args:\n",
        "            boxes: (tensor) [[x1, y1, x2, y2]_obj1, ...], normalized from 0.0 to 1.0 w.r.t. image width/height.\n",
        "            labels: (tensor) [c_obj1, c_obj2, ...]\n",
        "        Returns:\n",
        "            An encoded tensor sized [S, S, 5 x B + C], 5=(x, y, w, h, conf)\n",
        "        \"\"\"\n",
        "\n",
        "        S, B, C = self.S, self.B, self.C\n",
        "        N = 5 * B + C\n",
        "\n",
        "        target = torch.zeros(S, S, N)\n",
        "        cell_size = 1.0 / float(S)\n",
        "        boxes_wh = boxes[:, 2:] - boxes[:, :2] # width and height for each box, [n, 2]\n",
        "        boxes_xy = (boxes[:, 2:] + boxes[:, :2]) / 2.0 # center x & y for each box, [n, 2]\n",
        "\n",
        "        for b in range(boxes.size(0)):\n",
        "            xy, wh, label = boxes_xy[b], boxes_wh[b], int(labels[b])\n",
        "            print(xy, wh, label, sep=\"\\n\")\n",
        "\n",
        "            ij = (xy / cell_size).ceil() - 1.0\n",
        "            print(ij)\n",
        "            i, j = int(ij[0]), int(ij[1]) # y & x index which represents its location on the grid.\n",
        "            x0y0 = ij * cell_size # x & y of the cell left-top corner.\n",
        "            xy_normalized = (xy - x0y0) / cell_size # x & y of the box on the cell, normalized from 0.0 to 1.0.\n",
        "\n",
        "            # TBM, remove redundant dimensions from target tensor.\n",
        "            # To remove these, loss implementation also has to be modified.\n",
        "            for k in range(B):\n",
        "                s = 5 * k\n",
        "                target[j, i, s  :s+2] = xy_normalized\n",
        "                target[j, i, s+2:s+4] = wh\n",
        "                target[j, i, s+4    ] = 1.0\n",
        "            target[j, i, 5*B + label] = 1.0\n",
        "\n",
        "        return target\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAYnunij1bGg",
        "outputId": "a5210953-dfcf-40b7-f078-ab694c9677be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 2.])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(torch.Tensor([0.3050,0.3770])/cell_size).ceil()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9jlkK49yC7e",
        "outputId": "56c0c171-eef9-4218-b200-75a2c7e37fa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.3050, 0.3770])\n",
            "tensor([0.5995, 0.7420])\n",
            "14\n",
            "tensor([0., 1.])\n",
            "tensor([0.7149, 0.4250])\n",
            "tensor([1.2228, 0.6500])\n",
            "11\n",
            "tensor([2., 1.])\n",
            "tensor([0.5851, 0.7440])\n",
            "tensor([0.3149, 0.5282])\n",
            "7\n",
            "tensor([1., 2.])\n",
            "tensor([0.1337, 0.9531])\n",
            "tensor([0.2515, 0.4316])\n",
            "7\n",
            "tensor([0., 2.])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "image_dir = '/content/'\n",
        "label_txt = ['/content/VOC.txt']\n",
        "\n",
        "dataset = VOCDataset(True, image_dir, label_txt, grid_size=3,  num_bboxes=1, num_classes=15)\n",
        "data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "for k, v in data_loader:\n",
        "    img, target = k, v\n",
        "    # print(img.size(), target.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "l8gnuSzXxIB1",
        "outputId": "8c476e88-cca2-4eea-8708-829b6b74878c"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-5089bc901d84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageDraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgbimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# draw.point((cx_abs,cy_abs), fill=\"red\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PIL' is not defined"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 72x72 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(1, 1))\n",
        "\n",
        "img_arr = img.squeeze(0).permute(1,2,0).numpy() * 255.0\n",
        "img1 = Image.fromarray(img_arr.astype(np.uint8))\n",
        "rgbimg = Image.new(\"RGBA\", img1.size)\n",
        "rgbimg.paste(img1)\n",
        "\n",
        "# test_bbox_img1 = target[0][2,1,:4]\n",
        "test_bbox_img1 = torch.Tensor([[0.0079, 0.7373, 0.2594, 1.1689]])\n",
        "#bbox14_abs = relative2absolute_(test_bbox_img1)\n",
        "\n",
        "bbox14_abs = test_bbox_img1*1\n",
        "xmin, ymin, xmax, ymax = bbox14_abs.permute(1,0)\n",
        "left, right, top, bottom = xmin, xmax, ymin, ymax\n",
        "\n",
        "draw = PIL.ImageDraw.Draw(rgbimg)\n",
        "draw.line([(left, top), (left, bottom), (right, bottom), (right, top), (left, top)], width=2, fill='red')\n",
        "# draw.point((cx_abs,cy_abs), fill=\"red\")\n",
        "\n",
        "gleft = 0\n",
        "gright = 0\n",
        "for it in range(3):\n",
        "    g1 = 149.33*it\n",
        "    g2 = 149.33*it\n",
        "    \n",
        "    grid_x = [(0,g1), (448,g2)]\n",
        "    grid_y = [(g1,0), (g2,448)]\n",
        "    draw.line(grid_x)\n",
        "    draw.line(grid_y)\n",
        "\n",
        "fig.set_size_inches(5, 5)\n",
        "plt.imshow(rgbimg)\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iaLdPtSz_Mo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "bfc2e043f4a41bbce066f7921047bdcfe8769546c7198e4df6482fd90de274f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
