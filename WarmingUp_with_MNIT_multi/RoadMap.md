# THIS ROADMAP IS IN PROCESS

# Introduction
This notebook aims to help the reader to walk through the YoloV1 code for MNIST localization. It will not explain all the code in detail but will focus on the most important parts.

I won't go into YoloV1 details so I give you some articles that help me to understand the all process :
* YoloV1 [paper](https://arxiv.org/pdf/1506.02640.pdf)
* GitHub [repo](https://github.com/zzzheng/pytorch-yolo-v1) of @zzzheng
* Medium [article](https://medium.com/mlearning-ai/object-detection-explained-yolo-v1-fb4bcd3d87a1)
* @Hackernoon [article](https://hackernoon.com/understanding-yolo-f5a74bbc7967)
* Excellent [web page](https://pjreddie.com/darknet/yolo/) about Yolo, Darknet models, and other stuffs from @Joseph Chet Redmon

### Terminology :
* B : number of bounding boxes generated by the model (here 1)
* C : number of classes (here 10)
* S : number of grid cells in one direction (here 6)
* \*r or \*r_img : indicate a value *relatives to the image size*
* \*cr_img : indicate a **center** value *relatives to the image size*
* \*r_cell : indicate a value *relatives to a cell*
* \*cr_cell : indicate a **center** value *relatives to a cell*

# Dataset
I use the MNIST dataset from http://yann.lecun.com/exdb/mnist/ available in the `torchvision` framework. This dataset is composed of 60k training and 10k validation 28x28 greyscale images. 

To create my dataset I proceed through those steps (see [MNIST_dataset.py](https://github.com/ThOpaque/Food_Recognition/blob/main/WarmingUp_with_MNIST/MNIST_dataset.py) for the full code) :
* Randomly paste the 28x28 digits into a 75x75 black background
* Retrieve the xmin, ymin (top left coordinates), the width and height
* Encode those coordinates *relative to the cell* for xmin, ymin and *relative to the image* for the width and height
* Include those coordinates in a (N,S,S,B+1) tensor
* One-hot encoded the digit labels and include them in a (N,S,S,C) tensor

At the end, a SxS grid is obtained for each image. See below, for `S=6` and the digit `0` : its bounding box size is 28x28 (0.3733x0.3733 relatively to the image) and its center is in the cell `(2,1)`, preciselly, at the coordinates `(0.60, 0.32)` *respectively to this cell*.  

![alt text](https://github.com/ThOpaque/Food_Recognition/blob/main/WarmingUp_with_MNIST/img_utils/example_SxSgrid_digit0.png)


```python
def _encode(self, box, label):    
    ...
    ### Object grid location
    i = (xcr_img / self.cell_size).ceil() - 1.0
    j = (ycr_img / self.cell_size).ceil() - 1.0
    i, j = int(i), int(j)

    ### x & y of the cell left-top corner
    x0 = i * self.cell_size
    y0 = j * self.cell_size
    
    ### x & y of the box on the cell, normalized from 0.0 to 1.0.
    xcr_cell = (xcr - x0) / self.cell_size
    ycr_cell = (ycr - y0) / self.cell_size

    ### 4 coords + 1 conf + 10 classes
    one_hot_label = F.one_hot(torch.as_tensor(label, dtype=torch.int64), self.C)

    box_target = torch.zeros(self.S, self.S, self.B+4)
    box_target[j, i, :5] = torch.Tensor([xcr_cell, ycr_cell, wr, hr, 1.])

    return box_target, one_hot_label 
```

# Darknet-like model
I first started to create the same model from the Yolo paper. 

![alt text](https://github.com/ThOpaque/Food_Recognition/blob/main/WarmingUp_with_MNIST/img_utils/yolo_architecture.png)


Of course, to achieve my purpuse on MNIST, I do not need a such complex model. So, I created a [Darknet-like](https://github.com/ThOpaque/Food_Recognition/blob/main/WarmingUp_with_MNIST/Darknet_like.py) one, by keeping the same global topology :
- Using CNN Blocks which contains a convolutional layer, a batch normalization layer and a LeakyReLU activation
- CNN Blocks are followed by a MaxPool layer except the last ones before fully connected layers
- LeakyReLU activation between the two fully connected layers
- The last fully connected layer outputs a tensor of size (N, S, S, C+B+4+1)

To make the code cleaner, I choose to dissociate the "classification output" (used to classify the digits) and the "regression output" (used to predict the bounding box coordinates) in the forward. 

### Darknet-like model
```python
class YoloMNIST(torch.nn.Module):
    def __init__(self, sizeHW, S, C, B):
        ...
        self.seq = torch.nn.Sequential()        
        self.seq.add_module(f"conv_1", CNNBlock(1, 32, stride=2, kernel_size=7, padding=2))
        self.seq.add_module(f"maxpool_1", torch.nn.MaxPool2d(2))
        self.seq.add_module(f"conv_2", CNNBlock(32, 128, stride=1, kernel_size=3, padding=0))
        self.seq.add_module(f"maxpool_2", torch.nn.MaxPool2d(2))
        self.seq.add_module(f"conv_3", CNNBlock(128, 64, stride=1, kernel_size=1, padding=0))
        self.seq.add_module(f"conv_4", CNNBlock(64, 128, stride=1, kernel_size=3, padding=0))
        self.seq.add_module(f"conv_5", CNNBlock(128, 128, stride=1, kernel_size=3, padding=1))
        self.fcs = self._create_fcs()

    def _create_fcs(self):
        output = torch.nn.Sequential(
            torch.nn.Flatten(),
            torch.nn.Linear(128 * self.S * self.S, 4096),
            torch.nn.LeakyReLU(0.1),
            torch.nn.Linear(4096, self.S * self.S * (self.C + self.B*5)))
        return output

    def forward(self, input:torch.Tensor)->tuple:
        x = self.seq(input)
        x = self.fcs(x)
        x = x.view(x.size(0), self.S, self.S, self.B * 5 + self.C)
        box_coord = x[:,:,:,0:5]
        classifier = x[:,:,:,5:]
        return box_coord, classifier
```

# Yolo Loss
From the Yolo paper, I recall the global formula of the Yolo loss. Again, I recommand you to look at the references I listed above.

![alt text](https://github.com/ThOpaque/Food_Recognition/blob/main/WarmingUp_with_MNIST/img_utils/yolo_loss.png)

What should this class do more than returning the loss ? So that loss fit well in my pipeline, I needed few things :
* Treat tensor with shape (N,S,S,5)
* Treat the N images as matrix operations. That way, I do not need to loop over the batch which improves the overall performances
* Treat the presence or absence of an object as a mask
* Keep track of each loss (xy coordinates, wh sizes, confidence *with* object, confidence *without* object and classes)

At the end, this leads to the following snippet of code :

```python
def forward(self, pred_box:torch.Tensor, true_box:torch.Tensor, pred_class:torch.Tensor, true_class:torch.Tensor):
    ...
    ### Compute the losses for all images in the batch
    for i in range(self.S):
        for j in range(self.S):
            ...
            ### objects to detect
            pred_c = pred_box[:,i,j,4]
            true_c = true_box[:,i,j,4]
            isObject = true_c.eq(0)

            ### sum the losses over the grid
            losses['loss_xy'] += self._coordloss(xy_hat, xy, isObject)
            losses['loss_wh'] += self._sizeloss(wh_hat, wh, isObject)
            losses['loss_conf_obj'] += self._confidenceloss(pred_c, true_c, isObject)
            losses['loss_conf_noobj'] += self._confidenceloss(pred_c, true_c, torch.logical_not(isObject))
            losses['loss_class'] += self._classloss(pred_class[:,i,j], true_class, isObject)

    ### Yolo_v1 loss over the batch, shape : (BATCH_SIZE)
    for key, value in losses.items():
        losses[key] = torch.sum(value)/BATCH_SIZE

    loss = self.LAMBD_COORD * losses['loss_xy'] \
            + self.LAMBD_COORD * losses['loss_wh'] \
            + losses['loss_conf_obj'] \
            + self.LAMBD_NOOBJ * losses['loss_conf_noobj'] \
            + losses['loss_class']

    loss = torch.sum(loss) / BATCH_SIZE
    return losses, loss
```

# Training
The [training module](https://github.com/ThOpaque/Food_Recognition/blob/main/WarmingUp_with_MNIST/train.py) executes a classic training loop with an inner [validation loop](https://github.com/ThOpaque/Food_Recognition/blob/main/WarmingUp_with_MNIST/validation.py) which computes accuracies and MSEs each 100 batch. It prints a [log](https://github.com/ThOpaque/Food_Recognition/blob/main/WarmingUp_with_MNIST/results/logging_10epochs_relativeCoords_29092022_19h41.log) and then save the model and the losses.

* `Adam` optimizer is used
* It has been train with `lr = 1e-3` for the first 7 epochs then `lr = 1e-4` for the last 3 
* `BATCH_SIZE = 64`

```
...
29/09/2022 19:36:23 ::INFO:: ***** Validation class acc : 93.33%
29/09/2022 19:41:56 ::INFO:: Epoch 10/10
29/09/2022 19:41:56 ::INFO:: ***** Training loss : 0.00437
29/09/2022 19:41:56 ::INFO:: ***** MSE validation box loss : 0.02212
29/09/2022 19:41:56 ::INFO:: ***** MSE validation confidence score : 0.14191
29/09/2022 19:41:56 ::INFO:: ***** Validation class acc : 97.23%
29/09/2022 19:41:56 ::INFO:: End training.
```

# Draw bounding boxes
Drawing the boudning boxes is the final step of the Yolo algorithm. It's not the easiest one since it requires to get rid of all the predictions that are not relevent. One recalls the model retrieves one bounding box per grid cell, which leads to a total of `S*S=36` bounding boxes. So, one needs to take into account the best bounding box that may surround the digit (I say the *best bounding box* since there is only **one** object per image. With multiple objects per image, like in [Compagny Meal Trays Detection](https://github.com/ThOpaque/Food_Recognition/tree/main/YoloV1_Compagny_MealTrays) it's a bit more complicated).

So, to draw bounding boxes I needed to : 
* Convert relative bounding box infos into absolute coordinates
* Compute the IoU
* Apply a NMS-like that retrieves the best bounding box candidate (I say *NMS-like* since a "real NMS" is applied on image with more than one object which is a bit more complex)
* Draw bounding boxes


### Relative to absolute
In my pipeline I define [two modules](https://github.com/ThOpaque/Food_Recognition/blob/main/WarmingUp_with_MNIST/IoU.py) to turns relative infos into absolute coordinates. The first one handle the predicted bounding boxes and the second the groundtruth bounding boxes.

Since the position of the object is not known during prediction, the `relative2absolute_pred` module needs the current cell `(i,j)` position. It is called in the [`NMS` module](https://github.com/ThOpaque/Food_Recognition/blob/main/WarmingUp_with_MNIST/NMS.py) which produce the `6*6=36` bounding boxes.

For groundtruth coordinates, it is easier. I just need to retrieve the non-zero positions (since only the cell containing an object has informations in the `(N,S,S,5)` tensor) and use them to turn relatives infos into absolute coordinates.

### Intersection over Union
This function is needed for two purpuses : 

* For validation by computing the gap between groundtruth and predicted boxes
* For Non-Max-Suppression also, but in this case since there is only one object to detect, it is not usefull

### Non-Max-Suppression
The non-max-suppression algorithm should get rid of boxes that have spotted an object but that have not a decent IoU to get chosen regarding the "best one" (which is the box with the highest confidence number). If done correctly, it should let only the boxes with the highest confidence number for each object in the image.

Note also that this confidence number is function of the class prediction probability `pc = pc * P(Ci)` to get the confidence number of a specific object.

Again, since it's a one object detection, one just need to get the predicted box with the maximum confidence number. 

### Drawing bounding boxes
Here, I'll explain my drawing pipeline. It starts from the [`draw_boxes` module](https://github.com/ThOpaque/Food_Recognition/blob/main/WarmingUp_with_MNIST/draw_boxes.py).

This module select `n` random validation images and plot them with groundtruth and predicted bounding boxes. It indicates for each plot the class prediction and the IoU. It calls the [draw_boxes_utils module](https://github.com/ThOpaque/Food_Recognition/blob/main/WarmingUp_with_MNIST/draw_boxes_utils.py) to :
* Create a new `PIL` image and paste the validation image on it
* Draw the groundtruth and predicted bounding boxes with corresponding coordinates and colors


```python
def draw_bounding_boxes_on_image_array(
    image:np.ndarray, box_true, box_pred, color:list=[], thickness:int=1, display_str_list:list=[]
    ):
    ...
    image_pil = PIL.Image.fromarray(image)
    rgbimg = PIL.Image.new("RGBA", image_pil.size)
    rgbimg.paste(image_pil)
    draw_bounding_boxes_on_image(rgbimg, box_true, box_pred, color, thickness, display_str_list)
    plt.imshow(np.array(rgbimg))
```

```python
def draw_ONE_bounding_box_on_image(
    image, xmin:float, ymin:float, xmax:float, ymax:float, color:list=[], thickness:int=1, display_str:list=[]
    ):
    ...
    draw = PIL.ImageDraw.Draw(image)
    left, right, top, bottom = xmin, xmax, ymin, ymax
    draw.line([
        (left, top), (left, bottom), 
        (right, bottom), (right, top), 
        (left, top)], width=thickness, fill=color)
```

![alt text](https://github.com/ThOpaque/Food_Recognition/blob/main/WarmingUp_with_MNIST/results/MNIST_localization_10exemples.png)